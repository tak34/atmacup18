{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from timm.utils.model_ema import ModelEmaV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set dataset path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'atmacup_18'  # comp名\n",
    "\n",
    "    comp_dataset_path = '../raw/atmacup_18_dataset/'\n",
    "\n",
    "    exp_name = 'atmacup_18_cnn_effnetb0'\n",
    "\n",
    "    is_debug = False\n",
    "    use_gray_scale = False\n",
    "\n",
    "    model_in_chans = 9  # モデルの入力チャンネル数\n",
    "\n",
    "    # ============== file path =============\n",
    "    train_fold_dir = \"../proc/baseline/folds\"\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = \"tf_efficientnet_b0_ns\"\n",
    "\n",
    "    num_frames = 3  # model_in_chansの倍数\n",
    "    norm_in_chans = 1 if use_gray_scale else 3\n",
    "\n",
    "    use_torch_compile = False\n",
    "    use_ema = True\n",
    "    ema_decay = 0.995\n",
    "    # ============== training cfg =============\n",
    "    size = 224  # 224\n",
    "\n",
    "    batch_size = 64  # 32\n",
    "\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 20\n",
    "    if is_debug:\n",
    "        epochs = 2\n",
    "\n",
    "    # adamW warmupあり\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-3\n",
    "    if scheduler == 'GradualWarmupSchedulerV2':\n",
    "        lr /= warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    n_fold = 5\n",
    "    use_holdout = False\n",
    "    use_alldata = False\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    skf_col = 'class'\n",
    "    group_col = 'scene'\n",
    "    fold_type = 'gkf'\n",
    "\n",
    "    objective_cv = 'regression'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'minimize'  # 'maximize', 'minimize'\n",
    "    metrics = 'calc_mae_atmacup'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 18\n",
    "    target_col = ['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1', 'x_2', 'y_2',\n",
    "                  'z_2', 'x_3', 'y_3', 'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5']\n",
    "\n",
    "\n",
    "    # ============== ほぼ固定 =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'last'  # 'best'\n",
    "\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== set dataset path =============\n",
    "    if exp_name is not None:\n",
    "        print('set dataset path')\n",
    "\n",
    "        outputs_path = f'../proc/baseline/outputs/{exp_name}/'\n",
    "\n",
    "        submission_dir = outputs_path + 'submissions/'\n",
    "        submission_path = submission_dir + f'submission_{exp_name}.csv'\n",
    "\n",
    "        model_dir = outputs_path + \\\n",
    "            f'{comp_name}-models/'\n",
    "\n",
    "        figures_dir = outputs_path + 'figures/'\n",
    "\n",
    "        log_dir = outputs_path + 'logs/'\n",
    "        log_path = log_dir + f'{exp_name}.txt'\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.ShiftScaleRotate(p=0.5),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        # A.CoarseDropout(max_holes=1, max_height=int(\n",
    "        #     size * 0.3), max_width=int(size * 0.3), p=0.5),\n",
    "\n",
    "        A.Normalize(\n",
    "            mean=[0] * norm_in_chans*num_frames,\n",
    "            std=[1] * norm_in_chans*num_frames, \n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * norm_in_chans*num_frames,\n",
    "            std=[1] * norm_in_chans*num_frames,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA が利用可能か: True\n",
      "利用可能な CUDA デバイス数: 1\n",
      "現在の CUDA デバイス: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA が利用可能か:\", torch.cuda.is_available())\n",
    "print(\"利用可能な CUDA デバイス数:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"現在の CUDA デバイス:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "def get_fold(train, cfg):\n",
    "    if cfg.fold_type == 'kf':\n",
    "        Fold = KFold(n_splits=cfg.n_fold,\n",
    "                     shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.target_col])\n",
    "    elif cfg.fold_type == 'skf':\n",
    "        Fold = StratifiedKFold(n_splits=cfg.n_fold,\n",
    "                               shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.skf_col])\n",
    "    elif cfg.fold_type == 'gkf':\n",
    "        Fold = GroupKFold(n_splits=cfg.n_fold)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.group_col], groups)\n",
    "    elif cfg.fold_type == 'sgkf':\n",
    "        Fold = StratifiedGroupKFold(n_splits=cfg.n_fold,\n",
    "                                    shuffle=True, random_state=cfg.seed)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.skf_col], groups)\n",
    "    # elif fold_type == 'mskf':\n",
    "    #     Fold = MultilabelStratifiedKFold(\n",
    "    #         n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    #     kf = Fold.split(train, train[cfg.skf_col])\n",
    "\n",
    "    for n, (train_index, val_index) in enumerate(kf):\n",
    "        train.loc[val_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "    print(train.groupby('fold').size())\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_folds():\n",
    "    train_df = pd.read_csv(CFG.comp_dataset_path + 'train_features.csv')\n",
    "\n",
    "    train_df['scene'] = train_df['ID'].str.split('_').str[0]\n",
    "\n",
    "    print('group', CFG.group_col)\n",
    "    print(f'train len: {len(train_df)}')\n",
    "\n",
    "    train_df = get_fold(train_df, CFG)\n",
    "\n",
    "    # print(train_df.groupby(['fold', CFG.target_col]).size())\n",
    "    print(train_df['fold'].value_counts())\n",
    "\n",
    "    os.makedirs(CFG.train_fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(CFG.train_fold_dir +\n",
    "                    'train_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group scene\n",
      "train len: 43371\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "dtype: int64\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "make_train_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数固定\n",
    "def set_seed(seed=None, cudnn_deterministic=True):\n",
    "    if seed is None:\n",
    "        seed = 42\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def make_dirs(cfg):\n",
    "    for dir in [cfg.model_dir, cfg.figures_dir, cfg.submission_dir, cfg.log_dir]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def cfg_init(cfg, mode='train'):\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if mode == 'train':\n",
    "        make_dirs(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_init(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common_utils.logger import init_logger, wandb_init, AverageMeter, timeSince\n",
    "# from common_utils.settings import cfg_init\n",
    "\n",
    "def init_logger(log_file):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- exp_info -----------------\n",
      "2024年11月21日 11:14:51\n"
     ]
    }
   ],
   "source": [
    "Logger = init_logger(log_file=CFG.log_path)\n",
    "\n",
    "Logger.info('\\n\\n-------- exp_info -----------------')\n",
    "Logger.info(datetime.datetime.now().strftime('%Y年%m月%d日 %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    # return roc_auc_score(y_true, y_pred)\n",
    "    eval_func = eval(CFG.metrics)\n",
    "    return eval_func(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calc_mae_atmacup(y_true, y_pred):\n",
    "    abs_diff = np.abs(y_true - y_pred)  # 各予測の差分の絶対値を計算して\n",
    "    mae = np.mean(abs_diff.reshape(-1, ))  # 予測の差分の絶対値の平均を計算\n",
    "\n",
    "    return mae\n",
    "\n",
    "def get_result(result_df):\n",
    "\n",
    "    # preds = result_df['preds'].values\n",
    "\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "    preds = result_df[pred_cols].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    Logger.info(f'score: {score:<.4f}')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_traffic_light(image, id):\n",
    "    path = f'./datasets/atmacup_18/traffic_lights/{id}.json'\n",
    "    traffic_lights = json.load(open(path))\n",
    "\n",
    "    traffic_class = ['green',\n",
    "                     'straight', 'left', 'right', 'empty', 'other', 'yellow', 'red']\n",
    "    class_to_idx = {\n",
    "        cls: idx for idx, cls in enumerate(traffic_class)\n",
    "    }\n",
    "\n",
    "    for traffic_light in traffic_lights:\n",
    "        bbox = traffic_light['bbox']\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # int\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        point1 = (x1, y1)\n",
    "        point2 = (x2, y2)\n",
    "\n",
    "        idx = class_to_idx[traffic_light['class']]\n",
    "        color = 255 - int(255*(idx/len(traffic_class)))\n",
    "\n",
    "        cv2.rectangle(image, point1, point2, color=color, thickness=1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_image_for_cache(path):\n",
    "    if CFG.use_gray_scale:\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # image = cv2.resize(image, (CFG.size, CFG.size))\n",
    "\n",
    "    # 効かない\n",
    "    # image = draw_traffic_light(image, path.split('/')[-2])\n",
    "    return (path, image)\n",
    "\n",
    "\n",
    "def make_video_cache(paths):\n",
    "    debug = []\n",
    "    for idx in range(9):\n",
    "        color = 255 - int(255*(idx/9))\n",
    "        debug.append(color)\n",
    "    print(debug)\n",
    "\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        res = pool.imap_unordered(read_image_for_cache, paths)\n",
    "        res = tqdm(res)\n",
    "        res = list(res)\n",
    "\n",
    "    return dict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import ReplayCompose\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        # aug = A.Compose(cfg.train_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        # aug = A.Compose(cfg.valid_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, cfg, labels=None, transform=None):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.base_paths = df['base_path'].values\n",
    "        # self.labels = df[self.cfg.target_col].values\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def read_image_multiframe(self, idx):\n",
    "        base_path = self.base_paths[idx]\n",
    "\n",
    "        images = []\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "\n",
    "            image = self.cfg.video_cache[path]\n",
    "\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.read_image_multiframe(idx)\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image=image)['image']\n",
    "            replay = None\n",
    "            images = []\n",
    "            for img in image:\n",
    "                if replay is None:\n",
    "                    sample = self.transform(image=img)\n",
    "                    replay = sample['replay']\n",
    "                else:\n",
    "                    sample = ReplayCompose.replay(replay, image=img)\n",
    "                images.append(sample['image'])\n",
    "\n",
    "            image = torch.concat(images, dim=0)\n",
    "\n",
    "        if self.labels is None:\n",
    "            return image\n",
    "\n",
    "        if self.cfg.objective_cv == 'multiclass':\n",
    "            label = torch.tensor(self.labels[idx]).long()\n",
    "        else:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aug_video(train, cfg, plot_count=1):\n",
    "    transform = CFG.train_aug_list\n",
    "    transform = A.ReplayCompose(transform)\n",
    "\n",
    "    dataset = CustomDataset(\n",
    "        train, CFG, transform=transform)\n",
    "\n",
    "    for i in range(plot_count):\n",
    "        image = dataset.read_image_multiframe(i)\n",
    "\n",
    "        if cfg.use_gray_scale:\n",
    "            image = np.stack(image, axis=2)\n",
    "        else:\n",
    "            image = np.concatenate(image, axis=2)\n",
    "\n",
    "        aug_image = dataset[i]\n",
    "        # torch to numpy\n",
    "        aug_image = aug_image.permute(1, 2, 0).numpy()*255\n",
    "\n",
    "        for frame in range(image.shape[-1]):\n",
    "            if frame % 3 != 0:\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            if cfg.use_gray_scale:\n",
    "                axes[0].imshow(image[..., frame], cmap=\"gray\")\n",
    "                axes[1].imshow(aug_image[..., frame], cmap=\"gray\")\n",
    "            else:\n",
    "                axes[0].imshow(image[..., frame:frame+3].astype(int))\n",
    "                axes[1].imshow(aug_image[..., frame:frame+3].astype(int))\n",
    "            plt.savefig(cfg.figures_dir +\n",
    "                        f'aug_{i}_frame{frame}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "        # self.cfg = cfg\n",
    "\n",
    "        if model_name is None:\n",
    "            model_name = cfg.model_name\n",
    "\n",
    "        print(f'pretrained: {pretrained}')\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=0,\n",
    "            in_chans=cfg.model_in_chans)\n",
    "\n",
    "        self.n_features = self.model.num_features\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "\n",
    "        # nn.Dropout(0.5),\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.target_size)\n",
    "        )\n",
    "\n",
    "    def feature(self, image):\n",
    "\n",
    "        feature = self.model(image)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.feature(image)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(\n",
    "            optimizer, multiplier, total_epoch, after_scheduler)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [\n",
    "                        base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "def get_scheduler(cfg, optimizer):\n",
    "    if cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=cfg.factor, patience=cfg.patience, verbose=True, eps=cfg.eps)\n",
    "    elif cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer, T_max=cfg.epochs, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=cfg.T_0, T_mult=1, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'GradualWarmupSchedulerV2':\n",
    "        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, cfg.epochs, eta_min=1e-7)\n",
    "        scheduler = GradualWarmupSchedulerV2(\n",
    "            optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def scheduler_step(scheduler, avg_val_loss, epoch):\n",
    "    if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        scheduler.step(avg_val_loss)\n",
    "    elif isinstance(scheduler, CosineAnnealingLR):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, GradualWarmupSchedulerV2):\n",
    "        scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device,\n",
    "             model_ema=None):\n",
    "    \"\"\" 1epoch毎のtrain \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.use_amp)\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    preds_labels = []\n",
    "    start = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with autocast(CFG.use_amp):\n",
    "            y_preds = model(images)\n",
    "\n",
    "            if y_preds.size(1) == 1:\n",
    "                y_preds = y_preds.view(-1)\n",
    "\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.detach().to('cpu').numpy())\n",
    "\n",
    "        preds_labels.append(labels.detach().to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(preds_labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        if y_preds.size(1) == 1:\n",
    "            y_preds = y_preds.view(-1)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # binary\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(folds, fold):\n",
    "\n",
    "    Logger.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    if CFG.use_alldata:\n",
    "        train_folds = folds.copy().reset_index(drop=True)\n",
    "    else:\n",
    "        train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # train_folds = train_downsampling(train_folds)\n",
    "\n",
    "    train_labels = train_folds[CFG.target_col].values\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        train_folds, CFG, labels=train_labels, transform=get_transforms(data='train', cfg=CFG))\n",
    "    valid_dataset = CustomDataset(\n",
    "        valid_folds, CFG, labels=valid_labels, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                              )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "\n",
    "    model = CustomModel(CFG, pretrained=CFG.pretrained)\n",
    "    model.to(device)\n",
    "\n",
    "    if CFG.use_ema:\n",
    "        model_ema = ModelEmaV2(model, decay=CFG.ema_decay)\n",
    "    else:\n",
    "        model_ema = None\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr)\n",
    "    scheduler = get_scheduler(CFG, optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    if CFG.objective_cv == 'binary':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif CFG.objective_cv == 'multiclass':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif CFG.objective_cv == 'regression':\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "    if CFG.metric_direction == 'minimize':\n",
    "        best_score = np.inf\n",
    "    elif CFG.metric_direction == 'maximize':\n",
    "        best_score = -1\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    df_score = pd.DataFrame(columns=[\"train_loss\", 'train_score', 'val_loss', 'val_score'])\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss, train_preds, train_labels_epoch = train_fn(fold, train_loader, model,\n",
    "                                                             criterion, optimizer, epoch, scheduler, device, model_ema)\n",
    "        train_score = get_score(train_labels_epoch, train_preds)\n",
    "\n",
    "        # eval\n",
    "        if model_ema is not None:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model_ema.module, criterion, device)\n",
    "        else:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model, criterion, device)\n",
    "\n",
    "        scheduler_step(scheduler, avg_val_loss, epoch)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, valid_preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        # Logger.info(f'Epoch {epoch+1} - avgScore: {avg_score:.4f}')\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_Score: {train_score:.4f} avgScore: {score:.4f}')\n",
    "        \n",
    "        df_score.loc[epoch] = [avg_loss, train_score, avg_val_loss, score]\n",
    "\n",
    "        if CFG.metric_direction == 'minimize':\n",
    "            update_best = score < best_score\n",
    "        elif CFG.metric_direction == 'maximize':\n",
    "            update_best = score > best_score\n",
    "\n",
    "        if update_best:\n",
    "            best_loss = avg_val_loss\n",
    "            best_score = score\n",
    "\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "\n",
    "            if model_ema is not None:\n",
    "                torch.save({'model': model_ema.module.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "            else:\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save({'model': model.state_dict(),\n",
    "                'preds': valid_preds},\n",
    "               CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    \"\"\"\n",
    "    if model_ema is not None:\n",
    "        torch.save({'model': model_ema.module.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    else:\n",
    "        torch.save({'model': model.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "\n",
    "    check_point = torch.load(\n",
    "        CFG.model_dir + f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth', map_location=torch.device('cpu'))\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "\n",
    "    check_point_pred = check_point['preds']\n",
    "\n",
    "    # Columns must be same length as key 対策\n",
    "    if check_point_pred.ndim == 1:\n",
    "        check_point_pred = check_point_pred.reshape(-1, CFG.target_size)\n",
    "\n",
    "    print('check_point_pred shape', check_point_pred.shape)\n",
    "    valid_folds[pred_cols] = check_point_pred\n",
    "    return valid_folds, df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = pd.read_csv(CFG.train_fold_dir + 'train_folds.csv')\n",
    "    train['ori_idx'] = train.index\n",
    "\n",
    "    train['scene'] = train['ID'].str.split('_').str[0]\n",
    "\n",
    "    \"\"\"\n",
    "    if CFG.is_debug:\n",
    "        use_ids = train['scene'].unique()[:100]\n",
    "        train = train[train['scene'].isin(use_ids)].reset_index(drop=True)\n",
    "    \"\"\"\n",
    "\n",
    "    train['base_path'] = CFG.comp_dataset_path + 'images/' + train['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in train['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    # plot_aug_video(train, CFG, plot_count=10)\n",
    "\n",
    "    # train\n",
    "    oof_df = pd.DataFrame()\n",
    "    list_df_score = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold not in CFG.train_folds:\n",
    "            print(f'fold {fold} is skipped')\n",
    "            continue\n",
    "\n",
    "        _oof_df, _df_score = train_fold(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        list_df_score.append(_df_score)\n",
    "        Logger.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "\n",
    "        if CFG.use_holdout or CFG.use_alldata:\n",
    "            break\n",
    "\n",
    "    oof_df = oof_df.sort_values('ori_idx').reset_index(drop=True)\n",
    "\n",
    "    # CV result\n",
    "    Logger.info(\"========== CV ==========\")\n",
    "    score = get_result(oof_df)\n",
    "\n",
    "    # 学習曲線を可視化する\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    for df_score in list_df_score:\n",
    "        ax1.plot(df_score['val_loss'])\n",
    "        ax2.plot(df_score['val_score'])\n",
    "    ax1.set_title('Validation Loss')\n",
    "    ax2.set_title('Validation Score') \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2.set_ylabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CFG.figures_dir + f'learning_curve_{CFG.exp_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # save result\n",
    "    oof_df.to_csv(CFG.submission_dir + 'oof_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-0.5.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08722d8bef543c799a5e40088cecf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd72e4c439c45d9aeccba2322b376ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 2s (remain 19m 55s) Loss: 4.6399(4.6399) Grad: 29306.2031  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.3732(2.7057) Grad: 100556.5703  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.2716(2.5998) Grad: 106140.7500  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 51s) Loss: 1.7475(1.7475) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.5998  avg_val_loss: 2.0110  time: 64s\n",
      "Epoch 1 - avg_train_Score: 2.5998 avgScore: 2.0110\n",
      "Epoch 1 - Save Best Score: 2.0110 Model\n",
      "Epoch 1 - Save Best Loss: 2.0110 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.8583(2.0110) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 8s) Loss: 1.0689(1.0689) Grad: 178395.8438  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.2622(1.1544) Grad: 214691.3594  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.0813(1.1545) Grad: 209819.8125  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 1.0956(1.0956) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1545  avg_val_loss: 1.0971  time: 63s\n",
      "Epoch 2 - avg_train_Score: 1.1545 avgScore: 1.0971\n",
      "Epoch 2 - Save Best Score: 1.0971 Model\n",
      "Epoch 2 - Save Best Loss: 1.0971 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0680(1.0971) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 9m 25s) Loss: 0.9475(0.9475) Grad: 195542.7812  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0868(1.2798) Grad: 45548.4492  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.1638(1.2719) Grad: 58855.3320  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 10s) Loss: 0.9969(0.9969) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2719  avg_val_loss: 1.0175  time: 64s\n",
      "Epoch 3 - avg_train_Score: 1.2719 avgScore: 1.0175\n",
      "Epoch 3 - Save Best Score: 1.0175 Model\n",
      "Epoch 3 - Save Best Loss: 1.0175 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.9992(1.0175) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 9m 5s) Loss: 1.0931(1.0931) Grad: 124031.8203  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9717(0.9682) Grad: 100090.4766  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9297(0.9649) Grad: 104027.8516  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.9324(0.9324) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 0.9649  avg_val_loss: 0.8864  time: 63s\n",
      "Epoch 4 - avg_train_Score: 0.9649 avgScore: 0.8864\n",
      "Epoch 4 - Save Best Score: 0.8864 Model\n",
      "Epoch 4 - Save Best Loss: 0.8864 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8164(0.8864) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 9m 16s) Loss: 0.8878(0.8878) Grad: 98121.9375  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8183(0.8654) Grad: 124915.5781  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7543(0.8650) Grad: 95079.0938  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 11s) Loss: 0.8522(0.8522) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.8650  avg_val_loss: 0.8378  time: 64s\n",
      "Epoch 5 - avg_train_Score: 0.8650 avgScore: 0.8378\n",
      "Epoch 5 - Save Best Score: 0.8378 Model\n",
      "Epoch 5 - Save Best Loss: 0.8378 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7832(0.8378) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 9m 31s) Loss: 0.7760(0.7760) Grad: 90601.9922  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7596(0.7943) Grad: 94675.4453  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7586(0.7933) Grad: 98547.4297  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.8018(0.8018) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.7933  avg_val_loss: 0.8203  time: 64s\n",
      "Epoch 6 - avg_train_Score: 0.7933 avgScore: 0.8203\n",
      "Epoch 6 - Save Best Score: 0.8203 Model\n",
      "Epoch 6 - Save Best Loss: 0.8203 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7808(0.8203) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 9m 34s) Loss: 0.6452(0.6452) Grad: 100973.4844  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.8833(0.7351) Grad: 117437.4609  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5322(0.7369) Grad: 84304.2500  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.8119(0.8119) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7369  avg_val_loss: 0.8010  time: 63s\n",
      "Epoch 7 - avg_train_Score: 0.7369 avgScore: 0.8010\n",
      "Epoch 7 - Save Best Score: 0.8010 Model\n",
      "Epoch 7 - Save Best Loss: 0.8010 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7565(0.8010) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 9m 36s) Loss: 0.5954(0.5954) Grad: 92377.2734  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6844(0.6751) Grad: 101210.6562  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6061(0.6749) Grad: 96029.6016  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.8282(0.8282) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.6749  avg_val_loss: 0.7920  time: 63s\n",
      "Epoch 8 - avg_train_Score: 0.6749 avgScore: 0.7920\n",
      "Epoch 8 - Save Best Score: 0.7920 Model\n",
      "Epoch 8 - Save Best Loss: 0.7920 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7306(0.7920) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 9m 18s) Loss: 0.5980(0.5980) Grad: 107027.1406  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.7548(0.6286) Grad: 91967.9922  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.7930(0.6292) Grad: 100840.1797  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7641(0.7641) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6292  avg_val_loss: 0.7817  time: 63s\n",
      "Epoch 9 - avg_train_Score: 0.6292 avgScore: 0.7817\n",
      "Epoch 9 - Save Best Score: 0.7817 Model\n",
      "Epoch 9 - Save Best Loss: 0.7817 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7392(0.7817) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 9m 40s) Loss: 0.6242(0.6242) Grad: 94597.9844  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.5643(0.5757) Grad: 107668.7578  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.4743(0.5769) Grad: 79135.3047  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.7745(0.7745) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5769  avg_val_loss: 0.7755  time: 64s\n",
      "Epoch 10 - avg_train_Score: 0.5769 avgScore: 0.7755\n",
      "Epoch 10 - Save Best Score: 0.7755 Model\n",
      "Epoch 10 - Save Best Loss: 0.7755 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7266(0.7755) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 9m 19s) Loss: 0.5115(0.5115) Grad: 91086.6172  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.5971(0.5543) Grad: 51057.1953  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5433(0.5595) Grad: 46504.0781  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7526(0.7526) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5595  avg_val_loss: 0.7752  time: 63s\n",
      "Epoch 11 - avg_train_Score: 0.5595 avgScore: 0.7752\n",
      "Epoch 11 - Save Best Score: 0.7752 Model\n",
      "Epoch 11 - Save Best Loss: 0.7752 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7532(0.7752) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.5917(0.5917) Grad: 104040.2969  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.5671(0.5070) Grad: 81869.7500  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5053(0.5075) Grad: 92531.7500  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.7789(0.7789) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.5075  avg_val_loss: 0.7679  time: 63s\n",
      "Epoch 12 - avg_train_Score: 0.5075 avgScore: 0.7679\n",
      "Epoch 12 - Save Best Score: 0.7679 Model\n",
      "Epoch 12 - Save Best Loss: 0.7679 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7138(0.7679) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 9m 35s) Loss: 0.5281(0.5281) Grad: 108617.0547  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4590(0.4469) Grad: 99462.1016  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4021(0.4477) Grad: 86786.5078  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.7602(0.7602) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4477  avg_val_loss: 0.7622  time: 64s\n",
      "Epoch 13 - avg_train_Score: 0.4477 avgScore: 0.7622\n",
      "Epoch 13 - Save Best Score: 0.7622 Model\n",
      "Epoch 13 - Save Best Loss: 0.7622 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7067(0.7622) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 9m 30s) Loss: 0.4900(0.4900) Grad: 84004.3438  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3632(0.4100) Grad: 99858.0625  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4454(0.4101) Grad: 88989.9453  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.7630(0.7630) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4101  avg_val_loss: 0.7595  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.4101 avgScore: 0.7595\n",
      "Epoch 14 - Save Best Score: 0.7595 Model\n",
      "Epoch 14 - Save Best Loss: 0.7595 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7026(0.7595) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 9m 31s) Loss: 0.4901(0.4901) Grad: 90938.2266  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3485(0.3749) Grad: 73268.4766  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3313(0.3749) Grad: 91304.7656  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7825(0.7825) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3749  avg_val_loss: 0.7636  time: 63s\n",
      "Epoch 15 - avg_train_Score: 0.3749 avgScore: 0.7636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7273(0.7636) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 0s (remain 9m 0s) Loss: 0.3778(0.3778) Grad: 80361.5781  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3776(0.3468) Grad: 77213.3984  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3844(0.3463) Grad: 96424.5312  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7701(0.7701) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3463  avg_val_loss: 0.7587  time: 64s\n",
      "Epoch 16 - avg_train_Score: 0.3463 avgScore: 0.7587\n",
      "Epoch 16 - Save Best Score: 0.7587 Model\n",
      "Epoch 16 - Save Best Loss: 0.7587 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7285(0.7587) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 9m 14s) Loss: 0.2807(0.2807) Grad: 86892.2344  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2988(0.3168) Grad: 74063.0391  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2907(0.3165) Grad: 75763.0547  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7737(0.7737) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.3165  avg_val_loss: 0.7590  time: 63s\n",
      "Epoch 17 - avg_train_Score: 0.3165 avgScore: 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7175(0.7590) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 9m 16s) Loss: 0.2880(0.2880) Grad: 84446.5156  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2946(0.2943) Grad: 94731.7969  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2548(0.2940) Grad: 89295.3438  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7869(0.7869) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2940  avg_val_loss: 0.7603  time: 63s\n",
      "Epoch 18 - avg_train_Score: 0.2940 avgScore: 0.7603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7222(0.7603) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 9m 23s) Loss: 0.2703(0.2703) Grad: 66872.8594  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2637(0.2743) Grad: 69233.7891  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2429(0.2743) Grad: 70470.6484  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7891(0.7891) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2743  avg_val_loss: 0.7605  time: 64s\n",
      "Epoch 19 - avg_train_Score: 0.2743 avgScore: 0.7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7143(0.7605) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 9m 21s) Loss: 0.2216(0.2216) Grad: 67473.7266  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2477(0.2595) Grad: 75389.7891  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2779(0.2595) Grad: 86365.7422  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7951(0.7951) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7167(0.7613) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2595  avg_val_loss: 0.7613  time: 64s\n",
      "Epoch 20 - avg_train_Score: 0.2595 avgScore: 0.7613\n",
      "/tmp/ipykernel_152978/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 0 result ==========\n",
      "score: 0.7613\n",
      "========== fold: 1 training ==========\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8675, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 9m 40s) Loss: 5.8678(5.8678) Grad: 30714.1875  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.0320(2.8664) Grad: 84490.0234  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.3296(2.7541) Grad: 96645.5859  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 2.1886(2.1886) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.7541  avg_val_loss: 2.0788  time: 63s\n",
      "Epoch 1 - avg_train_Score: 2.7541 avgScore: 2.0788\n",
      "Epoch 1 - Save Best Score: 2.0788 Model\n",
      "Epoch 1 - Save Best Loss: 2.0788 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.7371(2.0788) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 9m 6s) Loss: 1.1914(1.1914) Grad: 207709.9531  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0943(1.1855) Grad: 102952.1953  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.0600(1.1833) Grad: 100028.6797  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 1.1820(1.1820) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1833  avg_val_loss: 1.1066  time: 64s\n",
      "Epoch 2 - avg_train_Score: 1.1833 avgScore: 1.1066\n",
      "Epoch 2 - Save Best Score: 1.1066 Model\n",
      "Epoch 2 - Save Best Loss: 1.1066 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9703(1.1066) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 9m 8s) Loss: 1.2556(1.2556) Grad: 273061.9062  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.2065(1.2721) Grad: 52025.1562  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9309(1.2559) Grad: 57095.9258  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 1.0674(1.0674) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2559  avg_val_loss: 0.9576  time: 64s\n",
      "Epoch 3 - avg_train_Score: 1.2559 avgScore: 0.9576\n",
      "Epoch 3 - Save Best Score: 0.9576 Model\n",
      "Epoch 3 - Save Best Loss: 0.9576 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9617(0.9576) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 9m 7s) Loss: 1.2452(1.2452) Grad: 128510.7656  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.0283(0.9646) Grad: 94013.3984  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.9125(0.9606) Grad: 100853.6406  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.9110(0.9110) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 0.9606  avg_val_loss: 0.8501  time: 64s\n",
      "Epoch 4 - avg_train_Score: 0.9606 avgScore: 0.8501\n",
      "Epoch 4 - Save Best Score: 0.8501 Model\n",
      "Epoch 4 - Save Best Loss: 0.8501 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 10s (remain 0m 0s) Loss: 0.9132(0.8501) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 9m 33s) Loss: 0.7424(0.7424) Grad: 97515.5078  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8468(0.8491) Grad: 102561.0469  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8975(0.8501) Grad: 107249.8203  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.8583(0.8583) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.8501  avg_val_loss: 0.8038  time: 64s\n",
      "Epoch 5 - avg_train_Score: 0.8501 avgScore: 0.8038\n",
      "Epoch 5 - Save Best Score: 0.8038 Model\n",
      "Epoch 5 - Save Best Loss: 0.8038 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8110(0.8038) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 9m 10s) Loss: 0.8374(0.8374) Grad: 113277.3047  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.7416(0.7766) Grad: 98354.2109  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.6553(0.7765) Grad: 99448.5469  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.8587(0.8587) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.7765  avg_val_loss: 0.7784  time: 66s\n",
      "Epoch 6 - avg_train_Score: 0.7765 avgScore: 0.7784\n",
      "Epoch 6 - Save Best Score: 0.7784 Model\n",
      "Epoch 6 - Save Best Loss: 0.7784 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 10s (remain 0m 0s) Loss: 0.7543(0.7784) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 10m 18s) Loss: 0.7835(0.7835) Grad: 120491.0547  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8144(0.7220) Grad: 103445.4297  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7270(0.7230) Grad: 115446.3750  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.8459(0.8459) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7230  avg_val_loss: 0.7623  time: 64s\n",
      "Epoch 7 - avg_train_Score: 0.7230 avgScore: 0.7623\n",
      "Epoch 7 - Save Best Score: 0.7623 Model\n",
      "Epoch 7 - Save Best Loss: 0.7623 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7561(0.7623) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.6670(0.6670) Grad: 111139.0234  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6253(0.6646) Grad: 110694.4609  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7096(0.6664) Grad: 112672.2109  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.8488(0.8488) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.6664  avg_val_loss: 0.7435  time: 63s\n",
      "Epoch 8 - avg_train_Score: 0.6664 avgScore: 0.7435\n",
      "Epoch 8 - Save Best Score: 0.7435 Model\n",
      "Epoch 8 - Save Best Loss: 0.7435 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7080(0.7435) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 9m 22s) Loss: 0.7058(0.7058) Grad: 104550.4609  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.5900(0.6153) Grad: 93886.3125  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5511(0.6143) Grad: 87339.0625  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.8513(0.8513) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6143  avg_val_loss: 0.7396  time: 63s\n",
      "Epoch 9 - avg_train_Score: 0.6143 avgScore: 0.7396\n",
      "Epoch 9 - Save Best Score: 0.7396 Model\n",
      "Epoch 9 - Save Best Loss: 0.7396 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7122(0.7396) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 9m 33s) Loss: 0.5489(0.5489) Grad: 96074.6719  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6042(0.5691) Grad: 90013.1875  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6120(0.5699) Grad: 117419.5312  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.8345(0.8345) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5699  avg_val_loss: 0.7324  time: 64s\n",
      "Epoch 10 - avg_train_Score: 0.5699 avgScore: 0.7324\n",
      "Epoch 10 - Save Best Score: 0.7324 Model\n",
      "Epoch 10 - Save Best Loss: 0.7324 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7237(0.7324) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 0s (remain 8m 52s) Loss: 0.5205(0.5205) Grad: 112468.6484  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.5916(0.5289) Grad: 86933.2344  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.4847(0.5289) Grad: 77004.3203  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.8237(0.8237) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5289  avg_val_loss: 0.7224  time: 63s\n",
      "Epoch 11 - avg_train_Score: 0.5289 avgScore: 0.7224\n",
      "Epoch 11 - Save Best Score: 0.7224 Model\n",
      "Epoch 11 - Save Best Loss: 0.7224 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7222(0.7224) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 9m 25s) Loss: 0.5151(0.5151) Grad: 79575.7266  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4786(0.4794) Grad: 76794.0000  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.4743(0.4795) Grad: 88367.8125  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.8436(0.8436) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4795  avg_val_loss: 0.7200  time: 63s\n",
      "Epoch 12 - avg_train_Score: 0.4795 avgScore: 0.7200\n",
      "Epoch 12 - Save Best Score: 0.7200 Model\n",
      "Epoch 12 - Save Best Loss: 0.7200 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7033(0.7200) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 9m 14s) Loss: 0.4466(0.4466) Grad: 81235.6641  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4078(0.4438) Grad: 79879.5625  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3818(0.4436) Grad: 94888.1094  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.8321(0.8321) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4436  avg_val_loss: 0.7151  time: 63s\n",
      "Epoch 13 - avg_train_Score: 0.4436 avgScore: 0.7151\n",
      "Epoch 13 - Save Best Score: 0.7151 Model\n",
      "Epoch 13 - Save Best Loss: 0.7151 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7039(0.7151) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 9m 12s) Loss: 0.3858(0.3858) Grad: 76906.6406  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3761(0.4046) Grad: 78040.6250  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3493(0.4048) Grad: 76347.1094  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.8275(0.8275) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4048  avg_val_loss: 0.7159  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.4048 avgScore: 0.7159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7225(0.7159) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 25s) Loss: 0.4146(0.4146) Grad: 103435.5859  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3463(0.3734) Grad: 87418.9453  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3808(0.3737) Grad: 94991.9688  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.8264(0.8264) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3737  avg_val_loss: 0.7127  time: 63s\n",
      "Epoch 15 - avg_train_Score: 0.3737 avgScore: 0.7127\n",
      "Epoch 15 - Save Best Score: 0.7127 Model\n",
      "Epoch 15 - Save Best Loss: 0.7127 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6831(0.7127) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 9m 40s) Loss: 0.3380(0.3380) Grad: 85927.1484  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3124(0.3401) Grad: 63464.5664  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3189(0.3401) Grad: 65048.7188  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 16s) Loss: 0.8252(0.8252) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3401  avg_val_loss: 0.7149  time: 65s\n",
      "Epoch 16 - avg_train_Score: 0.3401 avgScore: 0.7149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6868(0.7149) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 9m 40s) Loss: 0.2849(0.2849) Grad: 75754.5469  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3072(0.3118) Grad: 83428.8906  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2485(0.3120) Grad: 78811.1797  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.8059(0.8059) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.3120  avg_val_loss: 0.7127  time: 63s\n",
      "Epoch 17 - avg_train_Score: 0.3120 avgScore: 0.7127\n",
      "Epoch 17 - Save Best Score: 0.7127 Model\n",
      "Epoch 17 - Save Best Loss: 0.7127 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6959(0.7127) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 9m 22s) Loss: 0.2682(0.2682) Grad: 81302.7656  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3142(0.2871) Grad: 91094.6719  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2598(0.2870) Grad: 99298.6641  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.8137(0.8137) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2870  avg_val_loss: 0.7149  time: 64s\n",
      "Epoch 18 - avg_train_Score: 0.2870 avgScore: 0.7149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 10s (remain 0m 0s) Loss: 0.6918(0.7149) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 9m 35s) Loss: 0.2128(0.2128) Grad: 74282.9922  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2354(0.2687) Grad: 69248.6484  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2503(0.2683) Grad: 62074.0156  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.8076(0.8076) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2683  avg_val_loss: 0.7152  time: 64s\n",
      "Epoch 19 - avg_train_Score: 0.2683 avgScore: 0.7152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7002(0.7152) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 9m 25s) Loss: 0.2431(0.2431) Grad: 78976.1875  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2855(0.2520) Grad: 77895.2500  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2465(0.2521) Grad: 63420.7422  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 16s) Loss: 0.8098(0.8098) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2521  avg_val_loss: 0.7157  time: 64s\n",
      "Epoch 20 - avg_train_Score: 0.2521 avgScore: 0.7157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6873(0.7157) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 1 result ==========\n",
      "score: 0.7157\n",
      "========== fold: 2 training ==========\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 9m 38s) Loss: 5.2983(5.2983) Grad: 30793.5488  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.1566(2.8898) Grad: 170898.3281  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.2286(2.7766) Grad: 179815.8750  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 2.3015(2.3015) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.7766  avg_val_loss: 2.3988  time: 63s\n",
      "Epoch 1 - avg_train_Score: 2.7766 avgScore: 2.3988\n",
      "Epoch 1 - Save Best Score: 2.3988 Model\n",
      "Epoch 1 - Save Best Loss: 2.3988 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 2.3804(2.3988) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 9m 5s) Loss: 1.6087(1.6087) Grad: 205673.8125  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.2579(1.2049) Grad: 97020.7422  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8369(1.1986) Grad: 103766.8594  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.9092(0.9092) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1986  avg_val_loss: 1.0666  time: 64s\n",
      "Epoch 2 - avg_train_Score: 1.1986 avgScore: 1.0666\n",
      "Epoch 2 - Save Best Score: 1.0666 Model\n",
      "Epoch 2 - Save Best Loss: 1.0666 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.1284(1.0666) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 9m 17s) Loss: 0.9476(0.9476) Grad: 193351.3438  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.3467(1.2010) Grad: 122160.9219  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.2591(1.1960) Grad: 191560.4531  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.8914(0.8914) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.1960  avg_val_loss: 0.9719  time: 64s\n",
      "Epoch 3 - avg_train_Score: 1.1960 avgScore: 0.9719\n",
      "Epoch 3 - Save Best Score: 0.9719 Model\n",
      "Epoch 3 - Save Best Loss: 0.9719 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 1.0259(0.9719) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 9m 16s) Loss: 0.8197(0.8197) Grad: 127321.6094  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.1791(1.0316) Grad: 113512.0938  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.9828(1.0307) Grad: 143100.1875  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.7748(0.7748) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0307  avg_val_loss: 0.8675  time: 63s\n",
      "Epoch 4 - avg_train_Score: 1.0307 avgScore: 0.8675\n",
      "Epoch 4 - Save Best Score: 0.8675 Model\n",
      "Epoch 4 - Save Best Loss: 0.8675 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8776(0.8675) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 9m 24s) Loss: 0.8199(0.8199) Grad: 133216.6094  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.8692(0.9244) Grad: 105703.2031  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.8461(0.9228) Grad: 120805.6406  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.7197(0.7197) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9228  avg_val_loss: 0.8402  time: 63s\n",
      "Epoch 5 - avg_train_Score: 0.9228 avgScore: 0.8402\n",
      "Epoch 5 - Save Best Score: 0.8402 Model\n",
      "Epoch 5 - Save Best Loss: 0.8402 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8243(0.8402) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 9m 44s) Loss: 0.7435(0.7435) Grad: 127203.3906  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7941(0.8414) Grad: 100253.7031  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7970(0.8390) Grad: 114941.8203  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.7483(0.7483) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8390  avg_val_loss: 0.8184  time: 64s\n",
      "Epoch 6 - avg_train_Score: 0.8390 avgScore: 0.8184\n",
      "Epoch 6 - Save Best Score: 0.8184 Model\n",
      "Epoch 6 - Save Best Loss: 0.8184 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8408(0.8184) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 9m 13s) Loss: 0.6260(0.6260) Grad: 98732.9375  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7945(0.7644) Grad: 101701.7422  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8217(0.7663) Grad: 103581.9375  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7004(0.7004) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7663  avg_val_loss: 0.7811  time: 64s\n",
      "Epoch 7 - avg_train_Score: 0.7663 avgScore: 0.7811\n",
      "Epoch 7 - Save Best Score: 0.7811 Model\n",
      "Epoch 7 - Save Best Loss: 0.7811 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8251(0.7811) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 9m 53s) Loss: 0.8463(0.8463) Grad: 155692.2656  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.7072(0.7412) Grad: 51922.1758  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.7867(0.7463) Grad: 64059.0273  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7403(0.7403) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7463  avg_val_loss: 0.7947  time: 63s\n",
      "Epoch 8 - avg_train_Score: 0.7463 avgScore: 0.7947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7537(0.7947) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 9m 1s) Loss: 0.6238(0.6238) Grad: 80954.1250  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6559(0.6552) Grad: 101996.7734  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5880(0.6534) Grad: 116118.8438  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.6688(0.6688) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6534  avg_val_loss: 0.7500  time: 63s\n",
      "Epoch 9 - avg_train_Score: 0.6534 avgScore: 0.7500\n",
      "Epoch 9 - Save Best Score: 0.7500 Model\n",
      "Epoch 9 - Save Best Loss: 0.7500 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7260(0.7500) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 9m 22s) Loss: 0.5407(0.5407) Grad: 103721.9922  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6052(0.5765) Grad: 92740.8750  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6554(0.5773) Grad: 88902.1250  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.6638(0.6638) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5773  avg_val_loss: 0.7419  time: 63s\n",
      "Epoch 10 - avg_train_Score: 0.5773 avgScore: 0.7419\n",
      "Epoch 10 - Save Best Score: 0.7419 Model\n",
      "Epoch 10 - Save Best Loss: 0.7419 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7704(0.7419) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 9m 15s) Loss: 0.4628(0.4628) Grad: 92924.6172  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5824(0.5329) Grad: 88833.5078  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6651(0.5346) Grad: 127260.9922  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6720(0.6720) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5346  avg_val_loss: 0.7349  time: 64s\n",
      "Epoch 11 - avg_train_Score: 0.5346 avgScore: 0.7349\n",
      "Epoch 11 - Save Best Score: 0.7349 Model\n",
      "Epoch 11 - Save Best Loss: 0.7349 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7661(0.7349) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 9m 52s) Loss: 0.5060(0.5060) Grad: 96220.6016  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4886(0.4907) Grad: 89499.9844  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5528(0.4909) Grad: 79653.1875  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6610(0.6610) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4909  avg_val_loss: 0.7330  time: 63s\n",
      "Epoch 12 - avg_train_Score: 0.4909 avgScore: 0.7330\n",
      "Epoch 12 - Save Best Score: 0.7330 Model\n",
      "Epoch 12 - Save Best Loss: 0.7330 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7499(0.7330) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 9m 42s) Loss: 0.4730(0.4730) Grad: 85766.1016  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4924(0.4560) Grad: 88937.4922  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4042(0.4564) Grad: 84279.8672  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.6949(0.6949) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4564  avg_val_loss: 0.7269  time: 64s\n",
      "Epoch 13 - avg_train_Score: 0.4564 avgScore: 0.7269\n",
      "Epoch 13 - Save Best Score: 0.7269 Model\n",
      "Epoch 13 - Save Best Loss: 0.7269 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7217(0.7269) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 10m 28s) Loss: 0.4070(0.4070) Grad: 81208.7500  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.3896(0.4170) Grad: 99368.8359  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.4616(0.4172) Grad: 91745.1172  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.6796(0.6796) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4172  avg_val_loss: 0.7274  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.4172 avgScore: 0.7274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7382(0.7274) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 9m 49s) Loss: 0.3639(0.3639) Grad: 74263.3906  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4034(0.3810) Grad: 90468.8906  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4305(0.3804) Grad: 79472.5234  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6922(0.6922) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3804  avg_val_loss: 0.7299  time: 64s\n",
      "Epoch 15 - avg_train_Score: 0.3804 avgScore: 0.7299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7609(0.7299) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 9m 3s) Loss: 0.3838(0.3838) Grad: 94917.4688  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3316(0.3481) Grad: 77406.4844  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3391(0.3481) Grad: 74224.2188  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.6965(0.6965) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3481  avg_val_loss: 0.7274  time: 64s\n",
      "Epoch 16 - avg_train_Score: 0.3481 avgScore: 0.7274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7679(0.7274) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 9m 53s) Loss: 0.3399(0.3399) Grad: 111410.9297  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2968(0.3209) Grad: 86509.9453  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3820(0.3213) Grad: 116297.8594  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.6798(0.6798) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.3213  avg_val_loss: 0.7275  time: 63s\n",
      "Epoch 17 - avg_train_Score: 0.3213 avgScore: 0.7275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7592(0.7275) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 9m 24s) Loss: 0.2887(0.2887) Grad: 77191.4453  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2888(0.2963) Grad: 78908.2344  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2543(0.2960) Grad: 70526.2188  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6749(0.6749) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2960  avg_val_loss: 0.7287  time: 63s\n",
      "Epoch 18 - avg_train_Score: 0.2960 avgScore: 0.7287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7715(0.7287) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 9m 7s) Loss: 0.2382(0.2382) Grad: 70486.3359  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2900(0.2735) Grad: 85885.2422  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2937(0.2743) Grad: 90464.5625  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6713(0.6713) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2743  avg_val_loss: 0.7281  time: 65s\n",
      "Epoch 19 - avg_train_Score: 0.2743 avgScore: 0.7281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7667(0.7281) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 9m 8s) Loss: 0.2633(0.2633) Grad: 91695.5625  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2033(0.2579) Grad: 85830.2109  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3041(0.2579) Grad: 84586.2656  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.6718(0.6718) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2579  avg_val_loss: 0.7284  time: 64s\n",
      "Epoch 20 - avg_train_Score: 0.2579 avgScore: 0.7284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7729(0.7284) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 2 result ==========\n",
      "score: 0.7284\n",
      "========== fold: 3 training ==========\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 9m 24s) Loss: 5.0891(5.0891) Grad: 34165.1172  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 1.2934(2.8262) Grad: 104506.5234  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 1.7071(2.7141) Grad: 76438.1094  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 2.2657(2.2657) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.7141  avg_val_loss: 2.2711  time: 65s\n",
      "Epoch 1 - avg_train_Score: 2.7141 avgScore: 2.2711\n",
      "Epoch 1 - Save Best Score: 2.2711 Model\n",
      "Epoch 1 - Save Best Loss: 2.2711 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 2.0720(2.2711) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 9m 29s) Loss: 1.1357(1.1357) Grad: 177003.6875  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 1.3018(1.1683) Grad: 93520.7109  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 1.1513(1.1680) Grad: 83062.3672  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 1.0172(1.0172) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1680  avg_val_loss: 1.1561  time: 65s\n",
      "Epoch 2 - avg_train_Score: 1.1680 avgScore: 1.1561\n",
      "Epoch 2 - Save Best Score: 1.1561 Model\n",
      "Epoch 2 - Save Best Loss: 1.1561 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0672(1.1561) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 25s) Loss: 0.9425(0.9425) Grad: 208234.2188  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 1.0309(1.1948) Grad: 149771.2656  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 1.2563(1.1906) Grad: 63210.0078  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.8458(0.8458) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.1906  avg_val_loss: 0.9956  time: 65s\n",
      "Epoch 3 - avg_train_Score: 1.1906 avgScore: 0.9956\n",
      "Epoch 3 - Save Best Score: 0.9956 Model\n",
      "Epoch 3 - Save Best Loss: 0.9956 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9911(0.9956) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 9m 44s) Loss: 1.1461(1.1461) Grad: 116032.4766  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.1616(1.0266) Grad: 57918.1797  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.0354(1.0270) Grad: 54692.1680  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.8074(0.8074) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0270  avg_val_loss: 0.9524  time: 63s\n",
      "Epoch 4 - avg_train_Score: 1.0270 avgScore: 0.9524\n",
      "Epoch 4 - Save Best Score: 0.9524 Model\n",
      "Epoch 4 - Save Best Loss: 0.9524 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0036(0.9524) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 9m 56s) Loss: 0.9704(0.9704) Grad: 109502.7969  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9455(0.9923) Grad: 54208.4375  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.0779(0.9855) Grad: 79494.1016  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.7950(0.7950) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9855  avg_val_loss: 0.8718  time: 63s\n",
      "Epoch 5 - avg_train_Score: 0.9855 avgScore: 0.8718\n",
      "Epoch 5 - Save Best Score: 0.8718 Model\n",
      "Epoch 5 - Save Best Loss: 0.8718 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.9298(0.8718) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 9m 27s) Loss: 0.7520(0.7520) Grad: 100531.5234  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7331(0.7882) Grad: 97907.8047  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7711(0.7867) Grad: 101167.0156  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7350(0.7350) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.7867  avg_val_loss: 0.8149  time: 64s\n",
      "Epoch 6 - avg_train_Score: 0.7867 avgScore: 0.8149\n",
      "Epoch 6 - Save Best Score: 0.8149 Model\n",
      "Epoch 6 - Save Best Loss: 0.8149 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8984(0.8149) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 9m 35s) Loss: 0.7500(0.7500) Grad: 95071.3125  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6033(0.7051) Grad: 81308.2344  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6662(0.7072) Grad: 83274.7188  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.6650(0.6650) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7072  avg_val_loss: 0.7940  time: 63s\n",
      "Epoch 7 - avg_train_Score: 0.7072 avgScore: 0.7940\n",
      "Epoch 7 - Save Best Score: 0.7940 Model\n",
      "Epoch 7 - Save Best Loss: 0.7940 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8845(0.7940) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 9m 34s) Loss: 0.7288(0.7288) Grad: 114362.6719  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7436(0.6592) Grad: 94270.3750  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.7109(0.6603) Grad: 81680.2578  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6459(0.6459) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.6603  avg_val_loss: 0.7801  time: 65s\n",
      "Epoch 8 - avg_train_Score: 0.6603 avgScore: 0.7801\n",
      "Epoch 8 - Save Best Score: 0.7801 Model\n",
      "Epoch 8 - Save Best Loss: 0.7801 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8589(0.7801) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.6974(0.6974) Grad: 113259.3516  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5484(0.6124) Grad: 90672.6016  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5660(0.6143) Grad: 117861.4219  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.6488(0.6488) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6143  avg_val_loss: 0.7718  time: 64s\n",
      "Epoch 9 - avg_train_Score: 0.6143 avgScore: 0.7718\n",
      "Epoch 9 - Save Best Score: 0.7718 Model\n",
      "Epoch 9 - Save Best Loss: 0.7718 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8747(0.7718) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 10m 16s) Loss: 0.6390(0.6390) Grad: 104914.7422  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5288(0.5710) Grad: 114415.9844  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5860(0.5707) Grad: 87024.3203  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6538(0.6538) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5707  avg_val_loss: 0.7607  time: 64s\n",
      "Epoch 10 - avg_train_Score: 0.5707 avgScore: 0.7607\n",
      "Epoch 10 - Save Best Score: 0.7607 Model\n",
      "Epoch 10 - Save Best Loss: 0.7607 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8718(0.7607) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 9m 24s) Loss: 0.4622(0.4622) Grad: 92092.6719  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5906(0.5244) Grad: 104310.5547  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5335(0.5248) Grad: 101092.5000  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6500(0.6500) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5248  avg_val_loss: 0.7534  time: 64s\n",
      "Epoch 11 - avg_train_Score: 0.5248 avgScore: 0.7534\n",
      "Epoch 11 - Save Best Score: 0.7534 Model\n",
      "Epoch 11 - Save Best Loss: 0.7534 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8546(0.7534) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 9m 19s) Loss: 0.4287(0.4287) Grad: 89690.0078  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4436(0.4816) Grad: 85570.1094  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4907(0.4826) Grad: 89018.5156  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6591(0.6591) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4826  avg_val_loss: 0.7518  time: 64s\n",
      "Epoch 12 - avg_train_Score: 0.4826 avgScore: 0.7518\n",
      "Epoch 12 - Save Best Score: 0.7518 Model\n",
      "Epoch 12 - Save Best Loss: 0.7518 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8320(0.7518) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 9m 23s) Loss: 0.4682(0.4682) Grad: 107392.9141  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5144(0.4432) Grad: 90283.8516  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4618(0.4448) Grad: 92747.5156  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.6392(0.6392) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4448  avg_val_loss: 0.7490  time: 64s\n",
      "Epoch 13 - avg_train_Score: 0.4448 avgScore: 0.7490\n",
      "Epoch 13 - Save Best Score: 0.7490 Model\n",
      "Epoch 13 - Save Best Loss: 0.7490 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7920(0.7490) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 9m 28s) Loss: 0.4024(0.4024) Grad: 93755.2188  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5086(0.4094) Grad: 91625.9922  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3719(0.4089) Grad: 87127.3984  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6597(0.6597) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4089  avg_val_loss: 0.7474  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.4089 avgScore: 0.7474\n",
      "Epoch 14 - Save Best Score: 0.7474 Model\n",
      "Epoch 14 - Save Best Loss: 0.7474 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8062(0.7474) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 9m 26s) Loss: 0.3195(0.3195) Grad: 104841.5859  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3355(0.3746) Grad: 75370.1875  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3666(0.3736) Grad: 80068.9062  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6513(0.6513) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3736  avg_val_loss: 0.7442  time: 64s\n",
      "Epoch 15 - avg_train_Score: 0.3736 avgScore: 0.7442\n",
      "Epoch 15 - Save Best Score: 0.7442 Model\n",
      "Epoch 15 - Save Best Loss: 0.7442 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8192(0.7442) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 9m 52s) Loss: 0.3894(0.3894) Grad: 90932.9375  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2935(0.3448) Grad: 70186.0781  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3477(0.3441) Grad: 78664.2578  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6602(0.6602) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3441  avg_val_loss: 0.7440  time: 63s\n",
      "Epoch 16 - avg_train_Score: 0.3441 avgScore: 0.7440\n",
      "Epoch 16 - Save Best Score: 0.7440 Model\n",
      "Epoch 16 - Save Best Loss: 0.7440 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8286(0.7440) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 6s) Loss: 0.2995(0.2995) Grad: 82264.3516  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3483(0.3144) Grad: 79761.3828  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3678(0.3148) Grad: 75109.3516  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.6495(0.6495) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.3148  avg_val_loss: 0.7417  time: 64s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8435(0.7417) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 - avg_train_Score: 0.3148 avgScore: 0.7417\n",
      "Epoch 17 - Save Best Score: 0.7417 Model\n",
      "Epoch 17 - Save Best Loss: 0.7417 Model\n",
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 9m 29s) Loss: 0.2889(0.2889) Grad: 70475.2266  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3694(0.2875) Grad: 81964.4219  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2710(0.2872) Grad: 66047.8047  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.6454(0.6454) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2872  avg_val_loss: 0.7423  time: 65s\n",
      "Epoch 18 - avg_train_Score: 0.2872 avgScore: 0.7423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8471(0.7423) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 9m 59s) Loss: 0.2795(0.2795) Grad: 86557.4531  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3163(0.2711) Grad: 90517.5859  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2492(0.2709) Grad: 71586.8984  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6406(0.6406) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2709  avg_val_loss: 0.7416  time: 64s\n",
      "Epoch 19 - avg_train_Score: 0.2709 avgScore: 0.7416\n",
      "Epoch 19 - Save Best Score: 0.7416 Model\n",
      "Epoch 19 - Save Best Loss: 0.7416 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8490(0.7416) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 9m 47s) Loss: 0.2607(0.2607) Grad: 85352.1719  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2555(0.2553) Grad: 73384.0234  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2691(0.2545) Grad: 76414.6797  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.6471(0.6471) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2545  avg_val_loss: 0.7421  time: 64s\n",
      "Epoch 20 - avg_train_Score: 0.2545 avgScore: 0.7421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8507(0.7421) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 3 result ==========\n",
      "score: 0.7421\n",
      "========== fold: 4 training ==========\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 21s) Loss: 5.0624(5.0624) Grad: 28954.4492  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.3764(2.8734) Grad: 185321.5312  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.6641(2.7590) Grad: 178262.9531  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 2.3769(2.3769) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.7590  avg_val_loss: 2.5073  time: 64s\n",
      "Epoch 1 - avg_train_Score: 2.7590 avgScore: 2.5073\n",
      "Epoch 1 - Save Best Score: 2.5073 Model\n",
      "Epoch 1 - Save Best Loss: 2.5073 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 2.5756(2.5073) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 9m 44s) Loss: 1.6679(1.6679) Grad: 184844.3438  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.1863(1.2068) Grad: 109610.7031  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.4125(1.2036) Grad: 87529.3125  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 1.2249(1.2249) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.2036  avg_val_loss: 1.1005  time: 63s\n",
      "Epoch 2 - avg_train_Score: 1.2036 avgScore: 1.1005\n",
      "Epoch 2 - Save Best Score: 1.1005 Model\n",
      "Epoch 2 - Save Best Loss: 1.1005 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.1772(1.1005) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 1.0396(1.0396) Grad: 201586.3281  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0255(1.2138) Grad: 137886.2656  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.2194(1.2054) Grad: 144667.9219  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 1.0712(1.0712) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2054  avg_val_loss: 0.9751  time: 65s\n",
      "Epoch 3 - avg_train_Score: 1.2054 avgScore: 0.9751\n",
      "Epoch 3 - Save Best Score: 0.9751 Model\n",
      "Epoch 3 - Save Best Loss: 0.9751 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 10s (remain 0m 0s) Loss: 1.1693(0.9751) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 9m 39s) Loss: 1.0567(1.0567) Grad: 133233.4062  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.7965(1.0119) Grad: 119374.3281  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.8755(1.0092) Grad: 91440.8672  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.9639(0.9639) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0092  avg_val_loss: 0.8785  time: 65s\n",
      "Epoch 4 - avg_train_Score: 1.0092 avgScore: 0.8785\n",
      "Epoch 4 - Save Best Score: 0.8785 Model\n",
      "Epoch 4 - Save Best Loss: 0.8785 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9218(0.8785) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 9m 13s) Loss: 0.8279(0.8279) Grad: 125135.6797  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9292(0.9047) Grad: 101210.6797  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9620(0.9024) Grad: 119098.2812  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.9255(0.9255) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9024  avg_val_loss: 0.8403  time: 64s\n",
      "Epoch 5 - avg_train_Score: 0.9024 avgScore: 0.8403\n",
      "Epoch 5 - Save Best Score: 0.8403 Model\n",
      "Epoch 5 - Save Best Loss: 0.8403 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8920(0.8403) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 9m 24s) Loss: 0.5950(0.5950) Grad: 110179.2734  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.7463(0.8240) Grad: 102030.1250  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.8289(0.8229) Grad: 120011.1953  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.8450(0.8450) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8229  avg_val_loss: 0.8135  time: 65s\n",
      "Epoch 6 - avg_train_Score: 0.8229 avgScore: 0.8135\n",
      "Epoch 6 - Save Best Score: 0.8135 Model\n",
      "Epoch 6 - Save Best Loss: 0.8135 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8789(0.8135) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 9m 51s) Loss: 0.8420(0.8420) Grad: 123472.8984  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8871(0.7567) Grad: 116675.9609  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7799(0.7552) Grad: 115075.2500  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.8503(0.8503) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7552  avg_val_loss: 0.7875  time: 65s\n",
      "Epoch 7 - avg_train_Score: 0.7552 avgScore: 0.7875\n",
      "Epoch 7 - Save Best Score: 0.7875 Model\n",
      "Epoch 7 - Save Best Loss: 0.7875 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7783(0.7875) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 12m 38s) Loss: 0.6590(0.6590) Grad: 79721.0000  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7081(0.6833) Grad: 82558.9609  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7339(0.6837) Grad: 113172.1953  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.8194(0.8194) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.6837  avg_val_loss: 0.7692  time: 64s\n",
      "Epoch 8 - avg_train_Score: 0.6837 avgScore: 0.7692\n",
      "Epoch 8 - Save Best Score: 0.7692 Model\n",
      "Epoch 8 - Save Best Loss: 0.7692 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7513(0.7692) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 9m 20s) Loss: 0.6018(0.6018) Grad: 92643.2734  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6121(0.6379) Grad: 105616.7578  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6389(0.6384) Grad: 95246.0547  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.7718(0.7718) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6384  avg_val_loss: 0.7634  time: 63s\n",
      "Epoch 9 - avg_train_Score: 0.6384 avgScore: 0.7634\n",
      "Epoch 9 - Save Best Score: 0.7634 Model\n",
      "Epoch 9 - Save Best Loss: 0.7634 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7988(0.7634) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 10m 1s) Loss: 0.5613(0.5613) Grad: 82547.6484  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6166(0.5820) Grad: 86292.9688  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4960(0.5829) Grad: 86456.0000  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7650(0.7650) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5829  avg_val_loss: 0.7511  time: 64s\n",
      "Epoch 10 - avg_train_Score: 0.5829 avgScore: 0.7511\n",
      "Epoch 10 - Save Best Score: 0.7511 Model\n",
      "Epoch 10 - Save Best Loss: 0.7511 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7624(0.7511) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 9m 20s) Loss: 0.7073(0.7073) Grad: 125890.2344  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5121(0.5342) Grad: 91492.7422  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6304(0.5359) Grad: 83773.9375  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.7847(0.7847) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5359  avg_val_loss: 0.7472  time: 63s\n",
      "Epoch 11 - avg_train_Score: 0.5359 avgScore: 0.7472\n",
      "Epoch 11 - Save Best Score: 0.7472 Model\n",
      "Epoch 11 - Save Best Loss: 0.7472 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7587(0.7472) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 9m 34s) Loss: 0.5326(0.5326) Grad: 99357.7969  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4968(0.4985) Grad: 98257.2891  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5514(0.4987) Grad: 86126.4297  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.7594(0.7594) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4987  avg_val_loss: 0.7423  time: 63s\n",
      "Epoch 12 - avg_train_Score: 0.4987 avgScore: 0.7423\n",
      "Epoch 12 - Save Best Score: 0.7423 Model\n",
      "Epoch 12 - Save Best Loss: 0.7423 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7512(0.7423) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 9m 33s) Loss: 0.4228(0.4228) Grad: 82022.8828  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4998(0.4498) Grad: 96525.5234  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3985(0.4508) Grad: 99771.2969  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.7792(0.7792) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4508  avg_val_loss: 0.7373  time: 63s\n",
      "Epoch 13 - avg_train_Score: 0.4508 avgScore: 0.7373\n",
      "Epoch 13 - Save Best Score: 0.7373 Model\n",
      "Epoch 13 - Save Best Loss: 0.7373 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7386(0.7373) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 9m 16s) Loss: 0.4447(0.4447) Grad: 83955.9375  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4726(0.4135) Grad: 83981.7188  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4270(0.4138) Grad: 82838.3047  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.7772(0.7772) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4138  avg_val_loss: 0.7343  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.4138 avgScore: 0.7343\n",
      "Epoch 14 - Save Best Score: 0.7343 Model\n",
      "Epoch 14 - Save Best Loss: 0.7343 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7291(0.7343) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 9m 13s) Loss: 0.3787(0.3787) Grad: 82983.6328  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4362(0.3788) Grad: 84044.0391  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3638(0.3786) Grad: 76916.4453  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.7656(0.7656) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3786  avg_val_loss: 0.7365  time: 63s\n",
      "Epoch 15 - avg_train_Score: 0.3786 avgScore: 0.7365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7568(0.7365) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 9m 48s) Loss: 0.3416(0.3416) Grad: 104396.8828  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3538(0.3457) Grad: 96419.0547  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3672(0.3451) Grad: 96035.9297  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.7612(0.7612) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3451  avg_val_loss: 0.7338  time: 64s\n",
      "Epoch 16 - avg_train_Score: 0.3451 avgScore: 0.7338\n",
      "Epoch 16 - Save Best Score: 0.7338 Model\n",
      "Epoch 16 - Save Best Loss: 0.7338 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7521(0.7338) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 9m 24s) Loss: 0.3080(0.3080) Grad: 72592.8047  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3314(0.3152) Grad: 76530.4844  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2710(0.3152) Grad: 69972.8594  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.7612(0.7612) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.3152  avg_val_loss: 0.7333  time: 63s\n",
      "Epoch 17 - avg_train_Score: 0.3152 avgScore: 0.7333\n",
      "Epoch 17 - Save Best Score: 0.7333 Model\n",
      "Epoch 17 - Save Best Loss: 0.7333 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7403(0.7333) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.2797(0.2797) Grad: 74765.2812  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2558(0.2877) Grad: 68544.0781  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2518(0.2874) Grad: 82423.7656  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.7571(0.7571) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2874  avg_val_loss: 0.7323  time: 64s\n",
      "Epoch 18 - avg_train_Score: 0.2874 avgScore: 0.7323\n",
      "Epoch 18 - Save Best Score: 0.7323 Model\n",
      "Epoch 18 - Save Best Loss: 0.7323 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7390(0.7323) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 9m 56s) Loss: 0.2567(0.2567) Grad: 76033.5312  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2749(0.2673) Grad: 82791.6719  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2583(0.2671) Grad: 72708.4922  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 16s) Loss: 0.7580(0.7580) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2671  avg_val_loss: 0.7329  time: 66s\n",
      "Epoch 19 - avg_train_Score: 0.2671 avgScore: 0.7329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7370(0.7329) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_152978/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 9m 26s) Loss: 0.2641(0.2641) Grad: 64233.0781  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2840(0.2524) Grad: 86608.0234  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2971(0.2525) Grad: 90082.5156  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7574(0.7574) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2525  avg_val_loss: 0.7339  time: 63s\n",
      "Epoch 20 - avg_train_Score: 0.2525 avgScore: 0.7339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7393(0.7339) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152978/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 4 result ==========\n",
      "score: 0.7339\n",
      "========== CV ==========\n",
      "score: 0.7363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "            if CFG.objective_cv == 'binary':\n",
    "                outputs.append(torch.sigmoid(model(x)).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'multiclass':\n",
    "                outputs.append(torch.softmax(\n",
    "                    model(x), axis=1).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'regression':\n",
    "                outputs.append(model(x).to('cpu').numpy())\n",
    "\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "\n",
    "def test_fn(valid_loader, model, device):\n",
    "    preds = []\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        preds.append(y_preds)\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def inference():\n",
    "    test = pd.read_csv(CFG.comp_dataset_path +\n",
    "                       'test_features.csv')\n",
    "\n",
    "    test['base_path'] = CFG.comp_dataset_path + 'images/' + test['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in test['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    print(test.head(5))\n",
    "\n",
    "    valid_dataset = CustomDataset(\n",
    "        test, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = EnsembleModel()\n",
    "    folds = [0] if CFG.use_holdout else list(range(CFG.n_fold))\n",
    "    for fold in folds:\n",
    "        _model = CustomModel(CFG, pretrained=False)\n",
    "        _model.to(device)\n",
    "\n",
    "        model_path = CFG.model_dir + \\\n",
    "            f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth'\n",
    "        print('load', model_path)\n",
    "        state = torch.load(model_path)['model']\n",
    "        _model.load_state_dict(state)\n",
    "        _model.eval()\n",
    "\n",
    "        # _model = tta.ClassificationTTAWrapper(\n",
    "        #     _model, tta.aliases.five_crop_transform(256, 256))\n",
    "\n",
    "        model.add_model(_model)\n",
    "\n",
    "    preds = test_fn(valid_loader, model, device)\n",
    "\n",
    "    test[CFG.target_col] = preds\n",
    "    test.to_csv(CFG.submission_dir +\n",
    "                'submission_oof.csv', index=False)\n",
    "    test[CFG.target_col].to_csv(\n",
    "        CFG.submission_dir + f'submission_{CFG.exp_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-0.5.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28af896b249d4f7e9e0290391bf5aa2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     ID      vEgo      aEgo  steeringAngleDeg  \\\n",
      "0  012baccc145d400c896cb82065a93d42_120  3.374273 -0.019360        -34.008415   \n",
      "1  012baccc145d400c896cb82065a93d42_220  2.441048 -0.022754        307.860077   \n",
      "2  012baccc145d400c896cb82065a93d42_320  3.604152 -0.286239         10.774388   \n",
      "3  012baccc145d400c896cb82065a93d42_420  2.048902 -0.537628         61.045235   \n",
      "4  01d738e799d260a10f6324f78023b38f_120  2.201528 -1.898600          5.740093   \n",
      "\n",
      "   steeringTorque  brake  brakePressed  gas  gasPressed gearShifter  \\\n",
      "0            17.0    0.0         False  0.0       False       drive   \n",
      "1           295.0    0.0          True  0.0       False       drive   \n",
      "2          -110.0    0.0          True  0.0       False       drive   \n",
      "3           189.0    0.0          True  0.0       False       drive   \n",
      "4           -41.0    0.0          True  0.0       False       drive   \n",
      "\n",
      "   leftBlinker  rightBlinker  \\\n",
      "0        False         False   \n",
      "1        False         False   \n",
      "2        False         False   \n",
      "3         True         False   \n",
      "4        False         False   \n",
      "\n",
      "                                           base_path  \n",
      "0  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "1  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "2  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "3  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "4  ../raw/atmacup_18_dataset/images/01d738e799d26...  \n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_effnetb0/atmacup_18-models/tf_efficientnet_b0_ns_fold0_last.pth\n",
      "pretrained: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n",
      "/tmp/ipykernel_152978/610043316.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path)['model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../proc/baseline/outputs/atmacup_18_cnn_effnetb0/atmacup_18-models/tf_efficientnet_b0_ns_fold1_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_effnetb0/atmacup_18-models/tf_efficientnet_b0_ns_fold2_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_effnetb0/atmacup_18-models/tf_efficientnet_b0_ns_fold3_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_effnetb0/atmacup_18-models/tf_efficientnet_b0_ns_fold4_last.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88f7f5201a247fcbdbf48f1856bfb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
