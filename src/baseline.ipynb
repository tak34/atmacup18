{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from timm.utils.model_ema import ModelEmaV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set dataset path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'atmacup_18'  # comp名\n",
    "\n",
    "    comp_dataset_path = '../raw/atmacup_18_dataset/'\n",
    "\n",
    "    exp_name = 'atmacup_18_cnn_exp001'\n",
    "\n",
    "    is_debug = False\n",
    "    use_gray_scale = False\n",
    "\n",
    "    model_in_chans = 9  # モデルの入力チャンネル数\n",
    "\n",
    "    # ============== file path =============\n",
    "    train_fold_dir = \"../proc/baseline/folds\"\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = 'resnet34d'\n",
    "\n",
    "    num_frames = 3  # model_in_chansの倍数\n",
    "    norm_in_chans = 1 if use_gray_scale else 3\n",
    "\n",
    "    use_torch_compile = False\n",
    "    use_ema = True\n",
    "    ema_decay = 0.995\n",
    "    # ============== training cfg =============\n",
    "    size = 224  # 224\n",
    "\n",
    "    batch_size = 64  # 32\n",
    "\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 20\n",
    "    if is_debug:\n",
    "        epochs = 2\n",
    "\n",
    "    # adamW warmupあり\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-3\n",
    "    if scheduler == 'GradualWarmupSchedulerV2':\n",
    "        lr /= warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    n_fold = 5\n",
    "    use_holdout = False\n",
    "    use_alldata = False\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    skf_col = 'class'\n",
    "    group_col = 'scene'\n",
    "    fold_type = 'gkf'\n",
    "\n",
    "    objective_cv = 'regression'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'minimize'  # 'maximize', 'minimize'\n",
    "    metrics = 'calc_mae_atmacup'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 18\n",
    "    target_col = ['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1', 'x_2', 'y_2',\n",
    "                  'z_2', 'x_3', 'y_3', 'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5']\n",
    "\n",
    "\n",
    "    # ============== ほぼ固定 =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'last'  # 'best'\n",
    "\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== set dataset path =============\n",
    "    if exp_name is not None:\n",
    "        print('set dataset path')\n",
    "\n",
    "        outputs_path = f'../proc/baseline/outputs/{exp_name}/'\n",
    "\n",
    "        submission_dir = outputs_path + 'submissions/'\n",
    "        submission_path = submission_dir + f'submission_{exp_name}.csv'\n",
    "\n",
    "        model_dir = outputs_path + \\\n",
    "            f'{comp_name}-models/'\n",
    "\n",
    "        figures_dir = outputs_path + 'figures/'\n",
    "\n",
    "        log_dir = outputs_path + 'logs/'\n",
    "        log_path = log_dir + f'{exp_name}.txt'\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.ShiftScaleRotate(p=0.5),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        # A.CoarseDropout(max_holes=1, max_height=int(\n",
    "        #     size * 0.3), max_width=int(size * 0.3), p=0.5),\n",
    "\n",
    "        A.Normalize(\n",
    "            mean=[0] * norm_in_chans*num_frames,\n",
    "            std=[1] * norm_in_chans*num_frames, \n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * norm_in_chans*num_frames,\n",
    "            std=[1] * norm_in_chans*num_frames,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA が利用可能か: True\n",
      "利用可能な CUDA デバイス数: 1\n",
      "現在の CUDA デバイス: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA が利用可能か:\", torch.cuda.is_available())\n",
    "print(\"利用可能な CUDA デバイス数:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"現在の CUDA デバイス:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "def get_fold(train, cfg):\n",
    "    if cfg.fold_type == 'kf':\n",
    "        Fold = KFold(n_splits=cfg.n_fold,\n",
    "                     shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.target_col])\n",
    "    elif cfg.fold_type == 'skf':\n",
    "        Fold = StratifiedKFold(n_splits=cfg.n_fold,\n",
    "                               shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.skf_col])\n",
    "    elif cfg.fold_type == 'gkf':\n",
    "        Fold = GroupKFold(n_splits=cfg.n_fold)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.group_col], groups)\n",
    "    elif cfg.fold_type == 'sgkf':\n",
    "        Fold = StratifiedGroupKFold(n_splits=cfg.n_fold,\n",
    "                                    shuffle=True, random_state=cfg.seed)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.skf_col], groups)\n",
    "    # elif fold_type == 'mskf':\n",
    "    #     Fold = MultilabelStratifiedKFold(\n",
    "    #         n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    #     kf = Fold.split(train, train[cfg.skf_col])\n",
    "\n",
    "    for n, (train_index, val_index) in enumerate(kf):\n",
    "        train.loc[val_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "    print(train.groupby('fold').size())\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_folds():\n",
    "    train_df = pd.read_csv(CFG.comp_dataset_path + 'train_features.csv')\n",
    "\n",
    "    train_df['scene'] = train_df['ID'].str.split('_').str[0]\n",
    "\n",
    "    print('group', CFG.group_col)\n",
    "    print(f'train len: {len(train_df)}')\n",
    "\n",
    "    train_df = get_fold(train_df, CFG)\n",
    "\n",
    "    # print(train_df.groupby(['fold', CFG.target_col]).size())\n",
    "    print(train_df['fold'].value_counts())\n",
    "\n",
    "    os.makedirs(CFG.train_fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(CFG.train_fold_dir +\n",
    "                    'train_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group scene\n",
      "train len: 43371\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "dtype: int64\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "make_train_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数固定\n",
    "def set_seed(seed=None, cudnn_deterministic=True):\n",
    "    if seed is None:\n",
    "        seed = 42\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def make_dirs(cfg):\n",
    "    for dir in [cfg.model_dir, cfg.figures_dir, cfg.submission_dir, cfg.log_dir]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def cfg_init(cfg, mode='train'):\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if mode == 'train':\n",
    "        make_dirs(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_init(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common_utils.logger import init_logger, wandb_init, AverageMeter, timeSince\n",
    "# from common_utils.settings import cfg_init\n",
    "\n",
    "def init_logger(log_file):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- exp_info -----------------\n",
      "2024年11月20日 10:47:41\n"
     ]
    }
   ],
   "source": [
    "Logger = init_logger(log_file=CFG.log_path)\n",
    "\n",
    "Logger.info('\\n\\n-------- exp_info -----------------')\n",
    "Logger.info(datetime.datetime.now().strftime('%Y年%m月%d日 %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    # return roc_auc_score(y_true, y_pred)\n",
    "    eval_func = eval(CFG.metrics)\n",
    "    return eval_func(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calc_mae_atmacup(y_true, y_pred):\n",
    "    abs_diff = np.abs(y_true - y_pred)  # 各予測の差分の絶対値を計算して\n",
    "    mae = np.mean(abs_diff.reshape(-1, ))  # 予測の差分の絶対値の平均を計算\n",
    "\n",
    "    return mae\n",
    "\n",
    "def get_result(result_df):\n",
    "\n",
    "    # preds = result_df['preds'].values\n",
    "\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "    preds = result_df[pred_cols].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    Logger.info(f'score: {score:<.4f}')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_traffic_light(image, id):\n",
    "    path = f'./datasets/atmacup_18/traffic_lights/{id}.json'\n",
    "    traffic_lights = json.load(open(path))\n",
    "\n",
    "    traffic_class = ['green',\n",
    "                     'straight', 'left', 'right', 'empty', 'other', 'yellow', 'red']\n",
    "    class_to_idx = {\n",
    "        cls: idx for idx, cls in enumerate(traffic_class)\n",
    "    }\n",
    "\n",
    "    for traffic_light in traffic_lights:\n",
    "        bbox = traffic_light['bbox']\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # int\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        point1 = (x1, y1)\n",
    "        point2 = (x2, y2)\n",
    "\n",
    "        idx = class_to_idx[traffic_light['class']]\n",
    "        color = 255 - int(255*(idx/len(traffic_class)))\n",
    "\n",
    "        cv2.rectangle(image, point1, point2, color=color, thickness=1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_image_for_cache(path):\n",
    "    if CFG.use_gray_scale:\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # image = cv2.resize(image, (CFG.size, CFG.size))\n",
    "\n",
    "    # 効かない\n",
    "    # image = draw_traffic_light(image, path.split('/')[-2])\n",
    "    return (path, image)\n",
    "\n",
    "\n",
    "def make_video_cache(paths):\n",
    "    debug = []\n",
    "    for idx in range(9):\n",
    "        color = 255 - int(255*(idx/9))\n",
    "        debug.append(color)\n",
    "    print(debug)\n",
    "\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        res = pool.imap_unordered(read_image_for_cache, paths)\n",
    "        res = tqdm(res)\n",
    "        res = list(res)\n",
    "\n",
    "    return dict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import ReplayCompose\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        # aug = A.Compose(cfg.train_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        # aug = A.Compose(cfg.valid_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, cfg, labels=None, transform=None):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.base_paths = df['base_path'].values\n",
    "        # self.labels = df[self.cfg.target_col].values\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def read_image_multiframe(self, idx):\n",
    "        base_path = self.base_paths[idx]\n",
    "\n",
    "        images = []\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "\n",
    "            image = self.cfg.video_cache[path]\n",
    "\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.read_image_multiframe(idx)\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image=image)['image']\n",
    "            replay = None\n",
    "            images = []\n",
    "            for img in image:\n",
    "                if replay is None:\n",
    "                    sample = self.transform(image=img)\n",
    "                    replay = sample['replay']\n",
    "                else:\n",
    "                    sample = ReplayCompose.replay(replay, image=img)\n",
    "                images.append(sample['image'])\n",
    "\n",
    "            image = torch.concat(images, dim=0)\n",
    "\n",
    "        if self.labels is None:\n",
    "            return image\n",
    "\n",
    "        if self.cfg.objective_cv == 'multiclass':\n",
    "            label = torch.tensor(self.labels[idx]).long()\n",
    "        else:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aug_video(train, cfg, plot_count=1):\n",
    "    transform = CFG.train_aug_list\n",
    "    transform = A.ReplayCompose(transform)\n",
    "\n",
    "    dataset = CustomDataset(\n",
    "        train, CFG, transform=transform)\n",
    "\n",
    "    for i in range(plot_count):\n",
    "        image = dataset.read_image_multiframe(i)\n",
    "\n",
    "        if cfg.use_gray_scale:\n",
    "            image = np.stack(image, axis=2)\n",
    "        else:\n",
    "            image = np.concatenate(image, axis=2)\n",
    "\n",
    "        aug_image = dataset[i]\n",
    "        # torch to numpy\n",
    "        aug_image = aug_image.permute(1, 2, 0).numpy()*255\n",
    "\n",
    "        for frame in range(image.shape[-1]):\n",
    "            if frame % 3 != 0:\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            if cfg.use_gray_scale:\n",
    "                axes[0].imshow(image[..., frame], cmap=\"gray\")\n",
    "                axes[1].imshow(aug_image[..., frame], cmap=\"gray\")\n",
    "            else:\n",
    "                axes[0].imshow(image[..., frame:frame+3].astype(int))\n",
    "                axes[1].imshow(aug_image[..., frame:frame+3].astype(int))\n",
    "            plt.savefig(cfg.figures_dir +\n",
    "                        f'aug_{i}_frame{frame}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "        # self.cfg = cfg\n",
    "\n",
    "        if model_name is None:\n",
    "            model_name = cfg.model_name\n",
    "\n",
    "        print(f'pretrained: {pretrained}')\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=0,\n",
    "            in_chans=cfg.model_in_chans)\n",
    "\n",
    "        self.n_features = self.model.num_features\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "\n",
    "        # nn.Dropout(0.5),\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.target_size)\n",
    "        )\n",
    "\n",
    "    def feature(self, image):\n",
    "\n",
    "        feature = self.model(image)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.feature(image)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(\n",
    "            optimizer, multiplier, total_epoch, after_scheduler)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [\n",
    "                        base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "def get_scheduler(cfg, optimizer):\n",
    "    if cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=cfg.factor, patience=cfg.patience, verbose=True, eps=cfg.eps)\n",
    "    elif cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer, T_max=cfg.epochs, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=cfg.T_0, T_mult=1, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'GradualWarmupSchedulerV2':\n",
    "        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, cfg.epochs, eta_min=1e-7)\n",
    "        scheduler = GradualWarmupSchedulerV2(\n",
    "            optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def scheduler_step(scheduler, avg_val_loss, epoch):\n",
    "    if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        scheduler.step(avg_val_loss)\n",
    "    elif isinstance(scheduler, CosineAnnealingLR):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, GradualWarmupSchedulerV2):\n",
    "        scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device,\n",
    "             model_ema=None):\n",
    "    \"\"\" 1epoch毎のtrain \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.use_amp)\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    preds_labels = []\n",
    "    start = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with autocast(CFG.use_amp):\n",
    "            y_preds = model(images)\n",
    "\n",
    "            if y_preds.size(1) == 1:\n",
    "                y_preds = y_preds.view(-1)\n",
    "\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.detach().to('cpu').numpy())\n",
    "\n",
    "        preds_labels.append(labels.detach().to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(preds_labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        if y_preds.size(1) == 1:\n",
    "            y_preds = y_preds.view(-1)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # binary\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(folds, fold):\n",
    "\n",
    "    Logger.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    if CFG.use_alldata:\n",
    "        train_folds = folds.copy().reset_index(drop=True)\n",
    "    else:\n",
    "        train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # train_folds = train_downsampling(train_folds)\n",
    "\n",
    "    train_labels = train_folds[CFG.target_col].values\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        train_folds, CFG, labels=train_labels, transform=get_transforms(data='train', cfg=CFG))\n",
    "    valid_dataset = CustomDataset(\n",
    "        valid_folds, CFG, labels=valid_labels, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                              )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "\n",
    "    model = CustomModel(CFG, pretrained=CFG.pretrained)\n",
    "    model.to(device)\n",
    "\n",
    "    if CFG.use_ema:\n",
    "        model_ema = ModelEmaV2(model, decay=CFG.ema_decay)\n",
    "    else:\n",
    "        model_ema = None\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr)\n",
    "    scheduler = get_scheduler(CFG, optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    if CFG.objective_cv == 'binary':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif CFG.objective_cv == 'multiclass':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif CFG.objective_cv == 'regression':\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "    if CFG.metric_direction == 'minimize':\n",
    "        best_score = np.inf\n",
    "    elif CFG.metric_direction == 'maximize':\n",
    "        best_score = -1\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    df_score = pd.DataFrame(columns=[\"train_loss\", 'train_score', 'val_loss', 'val_score'])\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss, train_preds, train_labels_epoch = train_fn(fold, train_loader, model,\n",
    "                                                             criterion, optimizer, epoch, scheduler, device, model_ema)\n",
    "        train_score = get_score(train_labels_epoch, train_preds)\n",
    "\n",
    "        # eval\n",
    "        if model_ema is not None:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model_ema.module, criterion, device)\n",
    "        else:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model, criterion, device)\n",
    "\n",
    "        scheduler_step(scheduler, avg_val_loss, epoch)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, valid_preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        # Logger.info(f'Epoch {epoch+1} - avgScore: {avg_score:.4f}')\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_Score: {train_score:.4f} avgScore: {score:.4f}')\n",
    "        \n",
    "        df_score.loc[epoch] = [avg_loss, train_score, avg_val_loss, score]\n",
    "\n",
    "        if CFG.metric_direction == 'minimize':\n",
    "            update_best = score < best_score\n",
    "        elif CFG.metric_direction == 'maximize':\n",
    "            update_best = score > best_score\n",
    "\n",
    "        if update_best:\n",
    "            best_loss = avg_val_loss\n",
    "            best_score = score\n",
    "\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "\n",
    "            if model_ema is not None:\n",
    "                torch.save({'model': model_ema.module.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "            else:\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save({'model': model.state_dict(),\n",
    "                'preds': valid_preds},\n",
    "               CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    \"\"\"\n",
    "    if model_ema is not None:\n",
    "        torch.save({'model': model_ema.module.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    else:\n",
    "        torch.save({'model': model.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "\n",
    "    check_point = torch.load(\n",
    "        CFG.model_dir + f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth', map_location=torch.device('cpu'))\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "\n",
    "    check_point_pred = check_point['preds']\n",
    "\n",
    "    # Columns must be same length as key 対策\n",
    "    if check_point_pred.ndim == 1:\n",
    "        check_point_pred = check_point_pred.reshape(-1, CFG.target_size)\n",
    "\n",
    "    print('check_point_pred shape', check_point_pred.shape)\n",
    "    valid_folds[pred_cols] = check_point_pred\n",
    "    return valid_folds, df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = pd.read_csv(CFG.train_fold_dir + 'train_folds.csv')\n",
    "    train['ori_idx'] = train.index\n",
    "\n",
    "    train['scene'] = train['ID'].str.split('_').str[0]\n",
    "\n",
    "    \"\"\"\n",
    "    if CFG.is_debug:\n",
    "        use_ids = train['scene'].unique()[:100]\n",
    "        train = train[train['scene'].isin(use_ids)].reset_index(drop=True)\n",
    "    \"\"\"\n",
    "\n",
    "    train['base_path'] = CFG.comp_dataset_path + 'images/' + train['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in train['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    # plot_aug_video(train, CFG, plot_count=10)\n",
    "\n",
    "    # train\n",
    "    oof_df = pd.DataFrame()\n",
    "    list_df_score = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold not in CFG.train_folds:\n",
    "            print(f'fold {fold} is skipped')\n",
    "            continue\n",
    "\n",
    "        _oof_df, _df_score = train_fold(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        list_df_score.append(_df_score)\n",
    "        Logger.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "\n",
    "        if CFG.use_holdout or CFG.use_alldata:\n",
    "            break\n",
    "\n",
    "    oof_df = oof_df.sort_values('ori_idx').reset_index(drop=True)\n",
    "\n",
    "    # CV result\n",
    "    Logger.info(\"========== CV ==========\")\n",
    "    score = get_result(oof_df)\n",
    "\n",
    "    # 学習曲線を可視化する\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    for df_score in list_df_score:\n",
    "        ax1.plot(df_score['val_loss'])\n",
    "        ax2.plot(df_score['val_score'])\n",
    "    ax1.set_title('Validation Loss')\n",
    "    ax2.set_title('Validation Score') \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2.set_ylabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CFG.figures_dir + f'learning_curve_{CFG.exp_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # save result\n",
    "    oof_df.to_csv(CFG.submission_dir + 'oof_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-0.5.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e406e3e32cb4784960766bd0a13b491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 2s (remain 19m 28s) Loss: 6.0161(6.0161) Grad: 90772.9375  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.3780(2.2970) Grad: 287529.7500  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.5147(2.2217) Grad: 305098.0625  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 1.9239(1.9239) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.2217  avg_val_loss: 2.0092  time: 64s\n",
      "Epoch 1 - avg_train_Score: 2.2217 avgScore: 2.0092\n",
      "Epoch 1 - Save Best Score: 2.0092 Model\n",
      "Epoch 1 - Save Best Loss: 2.0092 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 2.0313(2.0092) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 0s (remain 8m 14s) Loss: 1.1568(1.1568) Grad: 202497.3438  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.1710(1.1373) Grad: 121837.5625  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.1150(1.1297) Grad: 119889.8125  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 1.2549(1.2549) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1297  avg_val_loss: 1.0486  time: 63s\n",
      "Epoch 2 - avg_train_Score: 1.1297 avgScore: 1.0486\n",
      "Epoch 2 - Save Best Score: 1.0486 Model\n",
      "Epoch 2 - Save Best Loss: 1.0486 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0585(1.0486) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 0s (remain 7m 53s) Loss: 0.8621(0.8621) Grad: 262067.7812  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0856(1.1898) Grad: 170080.3281  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7892(1.1812) Grad: 167275.8594  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 1.0019(1.0019) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.1812  avg_val_loss: 0.9690  time: 64s\n",
      "Epoch 3 - avg_train_Score: 1.1812 avgScore: 0.9690\n",
      "Epoch 3 - Save Best Score: 0.9690 Model\n",
      "Epoch 3 - Save Best Loss: 0.9690 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.9076(0.9690) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 0s (remain 8m 7s) Loss: 1.0897(1.0897) Grad: 191977.9844  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0539(1.0279) Grad: 168259.6094  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8965(1.0266) Grad: 155250.8594  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.9381(0.9381) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0266  avg_val_loss: 0.9124  time: 63s\n",
      "Epoch 4 - avg_train_Score: 1.0266 avgScore: 0.9124\n",
      "Epoch 4 - Save Best Score: 0.9124 Model\n",
      "Epoch 4 - Save Best Loss: 0.9124 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8868(0.9124) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 9m 19s) Loss: 1.0065(1.0065) Grad: 199316.5781  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9546(0.9292) Grad: 154591.5781  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.8873(0.9304) Grad: 137796.4375  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 0s (remain 1m 6s) Loss: 0.9216(0.9216) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9304  avg_val_loss: 0.8726  time: 63s\n",
      "Epoch 5 - avg_train_Score: 0.9304 avgScore: 0.8726\n",
      "Epoch 5 - Save Best Score: 0.8726 Model\n",
      "Epoch 5 - Save Best Loss: 0.8726 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8534(0.8726) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 0s (remain 8m 57s) Loss: 0.8022(0.8022) Grad: 152952.0000  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.8121(0.8500) Grad: 154055.9375  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9564(0.8530) Grad: 123356.1719  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 16s) Loss: 0.8873(0.8873) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8530  avg_val_loss: 0.8259  time: 64s\n",
      "Epoch 6 - avg_train_Score: 0.8530 avgScore: 0.8259\n",
      "Epoch 6 - Save Best Score: 0.8259 Model\n",
      "Epoch 6 - Save Best Loss: 0.8259 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7638(0.8259) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 0s (remain 9m 0s) Loss: 0.8494(0.8494) Grad: 161426.8594  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 0m 52s (remain 0m 4s) Loss: 0.7999(0.7840) Grad: 154799.8906  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 0m 56s (remain 0m 0s) Loss: 0.6900(0.7834) Grad: 113204.9141  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.8194(0.8194) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7834  avg_val_loss: 0.7981  time: 66s\n",
      "Epoch 7 - avg_train_Score: 0.7834 avgScore: 0.7981\n",
      "Epoch 7 - Save Best Score: 0.7981 Model\n",
      "Epoch 7 - Save Best Loss: 0.7981 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7804(0.7981) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 0s (remain 8m 13s) Loss: 0.7881(0.7881) Grad: 170341.1719  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.7022(0.7217) Grad: 97145.4141  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6271(0.7224) Grad: 126874.2812  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 11s) Loss: 0.8032(0.8032) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7224  avg_val_loss: 0.7740  time: 63s\n",
      "Epoch 8 - avg_train_Score: 0.7224 avgScore: 0.7740\n",
      "Epoch 8 - Save Best Score: 0.7740 Model\n",
      "Epoch 8 - Save Best Loss: 0.7740 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7225(0.7740) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 0s (remain 8m 12s) Loss: 0.5716(0.5716) Grad: 89455.4375  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6524(0.6543) Grad: 122123.6562  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6482(0.6555) Grad: 113315.9297  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 11s) Loss: 0.7497(0.7497) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6555  avg_val_loss: 0.7421  time: 63s\n",
      "Epoch 9 - avg_train_Score: 0.6555 avgScore: 0.7421\n",
      "Epoch 9 - Save Best Score: 0.7421 Model\n",
      "Epoch 9 - Save Best Loss: 0.7421 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6719(0.7421) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 0s (remain 8m 18s) Loss: 0.5516(0.5516) Grad: 99624.1641  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.5716(0.5892) Grad: 78483.9531  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6081(0.5913) Grad: 140051.5938  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 8s) Loss: 0.7363(0.7363) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5913  avg_val_loss: 0.7247  time: 63s\n",
      "Epoch 10 - avg_train_Score: 0.5913 avgScore: 0.7247\n",
      "Epoch 10 - Save Best Score: 0.7247 Model\n",
      "Epoch 10 - Save Best Loss: 0.7247 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6170(0.7247) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 0s (remain 7m 54s) Loss: 0.6316(0.6316) Grad: 149523.9844  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5809(0.5367) Grad: 102155.3359  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4018(0.5362) Grad: 91119.5312  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.7251(0.7251) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5362  avg_val_loss: 0.7145  time: 63s\n",
      "Epoch 11 - avg_train_Score: 0.5362 avgScore: 0.7145\n",
      "Epoch 11 - Save Best Score: 0.7145 Model\n",
      "Epoch 11 - Save Best Loss: 0.7145 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6562(0.7145) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 0s (remain 8m 15s) Loss: 0.5264(0.5264) Grad: 82775.2500  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5514(0.4844) Grad: 132973.5938  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4546(0.4859) Grad: 95669.1328  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 0s (remain 1m 6s) Loss: 0.7146(0.7146) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4859  avg_val_loss: 0.7114  time: 63s\n",
      "Epoch 12 - avg_train_Score: 0.4859 avgScore: 0.7114\n",
      "Epoch 12 - Save Best Score: 0.7114 Model\n",
      "Epoch 12 - Save Best Loss: 0.7114 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6424(0.7114) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 0s (remain 8m 28s) Loss: 0.4121(0.4121) Grad: 98504.6250  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3940(0.4367) Grad: 93288.1641  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.4534(0.4376) Grad: 120355.1641  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.7156(0.7156) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4376  avg_val_loss: 0.7072  time: 63s\n",
      "Epoch 13 - avg_train_Score: 0.4376 avgScore: 0.7072\n",
      "Epoch 13 - Save Best Score: 0.7072 Model\n",
      "Epoch 13 - Save Best Loss: 0.7072 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6509(0.7072) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 0s (remain 8m 3s) Loss: 0.4076(0.4076) Grad: 112083.4219  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3938(0.3927) Grad: 121882.8281  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3921(0.3914) Grad: 94764.0078  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.7101(0.7101) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.3914  avg_val_loss: 0.7047  time: 63s\n",
      "Epoch 14 - avg_train_Score: 0.3914 avgScore: 0.7047\n",
      "Epoch 14 - Save Best Score: 0.7047 Model\n",
      "Epoch 14 - Save Best Loss: 0.7047 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6483(0.7047) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 0s (remain 8m 11s) Loss: 0.3570(0.3570) Grad: 93384.8750  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3635(0.3530) Grad: 85204.3125  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2932(0.3522) Grad: 74394.2344  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 7s) Loss: 0.7115(0.7115) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3522  avg_val_loss: 0.7038  time: 63s\n",
      "Epoch 15 - avg_train_Score: 0.3522 avgScore: 0.7038\n",
      "Epoch 15 - Save Best Score: 0.7038 Model\n",
      "Epoch 15 - Save Best Loss: 0.7038 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6752(0.7038) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 0s (remain 8m 55s) Loss: 0.3414(0.3414) Grad: 82356.6797  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3001(0.3170) Grad: 73174.5859  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3151(0.3166) Grad: 77452.3125  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.7015(0.7015) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3166  avg_val_loss: 0.7021  time: 64s\n",
      "Epoch 16 - avg_train_Score: 0.3166 avgScore: 0.7021\n",
      "Epoch 16 - Save Best Score: 0.7021 Model\n",
      "Epoch 16 - Save Best Loss: 0.7021 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6744(0.7021) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 0s (remain 8m 31s) Loss: 0.2993(0.2993) Grad: 136179.1250  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2967(0.2809) Grad: 56872.6367  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2831(0.2801) Grad: 107113.6719  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 8s) Loss: 0.6886(0.6886) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.2801  avg_val_loss: 0.7021  time: 64s\n",
      "Epoch 17 - avg_train_Score: 0.2801 avgScore: 0.7021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6636(0.7021) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 0s (remain 8m 32s) Loss: 0.3153(0.3153) Grad: 88685.2656  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2706(0.2535) Grad: 82293.5000  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2296(0.2529) Grad: 70937.5781  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 0s (remain 1m 6s) Loss: 0.6989(0.6989) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2529  avg_val_loss: 0.7032  time: 62s\n",
      "Epoch 18 - avg_train_Score: 0.2529 avgScore: 0.7032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6581(0.7032) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 0s (remain 8m 17s) Loss: 0.2863(0.2863) Grad: 62585.7305  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2098(0.2292) Grad: 76439.1016  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2634(0.2294) Grad: 101143.3750  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.7006(0.7006) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2294  avg_val_loss: 0.7021  time: 62s\n",
      "Epoch 19 - avg_train_Score: 0.2294 avgScore: 0.7021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6512(0.7021) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 0s (remain 8m 22s) Loss: 0.1966(0.1966) Grad: 52666.6562  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2085(0.2087) Grad: 93049.0156  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2099(0.2089) Grad: 76769.8281  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 10s) Loss: 0.6974(0.6974) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2089  avg_val_loss: 0.7024  time: 64s\n",
      "Epoch 20 - avg_train_Score: 0.2089 avgScore: 0.7024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6493(0.7024) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 0 result ==========\n",
      "score: 0.7024\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8675, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 0s (remain 8m 29s) Loss: 5.5328(5.5328) Grad: 85208.7734  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.3448(2.2981) Grad: 250248.2812  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.1757(2.2216) Grad: 254167.7344  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 2.1033(2.1033) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.2216  avg_val_loss: 1.9290  time: 64s\n",
      "Epoch 1 - avg_train_Score: 2.2216 avgScore: 1.9290\n",
      "Epoch 1 - Save Best Score: 1.9290 Model\n",
      "Epoch 1 - Save Best Loss: 1.9290 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 1.7395(1.9290) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 0s (remain 8m 31s) Loss: 1.3240(1.3240) Grad: inf  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8652(1.1372) Grad: 134001.2500  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.3922(1.1326) Grad: 151618.9844  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 1.1266(1.1266) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1326  avg_val_loss: 1.0070  time: 64s\n",
      "Epoch 2 - avg_train_Score: 1.1326 avgScore: 1.0070\n",
      "Epoch 2 - Save Best Score: 1.0070 Model\n",
      "Epoch 2 - Save Best Loss: 1.0070 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.9755(1.0070) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 0s (remain 8m 28s) Loss: 1.0822(1.0822) Grad: 311079.8125  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0697(1.2026) Grad: 152168.9688  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.1294(1.1977) Grad: 201825.3438  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 1.0645(1.0645) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.1977  avg_val_loss: 0.9706  time: 63s\n",
      "Epoch 3 - avg_train_Score: 1.1977 avgScore: 0.9706\n",
      "Epoch 3 - Save Best Score: 0.9706 Model\n",
      "Epoch 3 - Save Best Loss: 0.9706 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.9843(0.9706) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 0s (remain 8m 23s) Loss: 1.3892(1.3892) Grad: 254762.0781  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9814(1.0244) Grad: 167730.6562  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9789(1.0266) Grad: 157753.9531  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.9840(0.9840) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0266  avg_val_loss: 0.8923  time: 65s\n",
      "Epoch 4 - avg_train_Score: 1.0266 avgScore: 0.8923\n",
      "Epoch 4 - Save Best Score: 0.8923 Model\n",
      "Epoch 4 - Save Best Loss: 0.8923 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8999(0.8923) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 0s (remain 8m 11s) Loss: 0.7891(0.7891) Grad: 158671.5312  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9757(0.9388) Grad: 172552.0781  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8125(0.9378) Grad: 175565.6250  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 10s) Loss: 0.9063(0.9063) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9378  avg_val_loss: 0.8582  time: 64s\n",
      "Epoch 5 - avg_train_Score: 0.9378 avgScore: 0.8582\n",
      "Epoch 5 - Save Best Score: 0.8582 Model\n",
      "Epoch 5 - Save Best Loss: 0.8582 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8578(0.8582) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 0s (remain 8m 32s) Loss: 1.0207(1.0207) Grad: 150554.1250  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7692(0.8668) Grad: 139762.9375  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.0060(0.8694) Grad: 145947.0156  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.9080(0.9080) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8694  avg_val_loss: 0.8212  time: 64s\n",
      "Epoch 6 - avg_train_Score: 0.8694 avgScore: 0.8212\n",
      "Epoch 6 - Save Best Score: 0.8212 Model\n",
      "Epoch 6 - Save Best Loss: 0.8212 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8307(0.8212) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 0s (remain 8m 41s) Loss: 0.6726(0.6726) Grad: 140289.3281  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9535(0.8015) Grad: 145186.7656  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6324(0.8004) Grad: 110990.3438  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.8248(0.8248) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8004  avg_val_loss: 0.7878  time: 64s\n",
      "Epoch 7 - avg_train_Score: 0.8004 avgScore: 0.7878\n",
      "Epoch 7 - Save Best Score: 0.7878 Model\n",
      "Epoch 7 - Save Best Loss: 0.7878 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8214(0.7878) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 0s (remain 8m 15s) Loss: 0.9865(0.9865) Grad: 131584.5469  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.8070(0.7323) Grad: 143128.5781  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.7158(0.7317) Grad: 86136.3984  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.7937(0.7937) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7317  avg_val_loss: 0.7601  time: 63s\n",
      "Epoch 8 - avg_train_Score: 0.7317 avgScore: 0.7601\n",
      "Epoch 8 - Save Best Score: 0.7601 Model\n",
      "Epoch 8 - Save Best Loss: 0.7601 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7710(0.7601) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 0s (remain 8m 14s) Loss: 0.7245(0.7245) Grad: 135072.9688  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7511(0.6601) Grad: 124682.6953  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7205(0.6629) Grad: 91373.8906  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 11s) Loss: 0.8041(0.8041) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6629  avg_val_loss: 0.7359  time: 64s\n",
      "Epoch 9 - avg_train_Score: 0.6629 avgScore: 0.7359\n",
      "Epoch 9 - Save Best Score: 0.7359 Model\n",
      "Epoch 9 - Save Best Loss: 0.7359 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7707(0.7359) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 0s (remain 8m 35s) Loss: 0.5685(0.5685) Grad: 105637.3438  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4789(0.6081) Grad: 83880.4453  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5474(0.6085) Grad: 96076.3750  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7739(0.7739) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6085  avg_val_loss: 0.7220  time: 64s\n",
      "Epoch 10 - avg_train_Score: 0.6085 avgScore: 0.7220\n",
      "Epoch 10 - Save Best Score: 0.7220 Model\n",
      "Epoch 10 - Save Best Loss: 0.7220 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7261(0.7220) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 0s (remain 8m 19s) Loss: 0.5446(0.5446) Grad: 93554.1641  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.5494(0.5419) Grad: 100519.2578  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5856(0.5430) Grad: 107060.0469  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.7814(0.7814) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5430  avg_val_loss: 0.7150  time: 63s\n",
      "Epoch 11 - avg_train_Score: 0.5430 avgScore: 0.7150\n",
      "Epoch 11 - Save Best Score: 0.7150 Model\n",
      "Epoch 11 - Save Best Loss: 0.7150 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7305(0.7150) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 0s (remain 8m 31s) Loss: 0.5177(0.5177) Grad: 104095.2656  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4178(0.4985) Grad: 101212.6094  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5229(0.4994) Grad: 91562.6719  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 10s) Loss: 0.7194(0.7194) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4994  avg_val_loss: 0.7074  time: 63s\n",
      "Epoch 12 - avg_train_Score: 0.4994 avgScore: 0.7074\n",
      "Epoch 12 - Save Best Score: 0.7074 Model\n",
      "Epoch 12 - Save Best Loss: 0.7074 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6884(0.7074) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 0s (remain 8m 37s) Loss: 0.4021(0.4021) Grad: 116400.2422  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4875(0.4481) Grad: 109782.0703  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5302(0.4486) Grad: 141663.3750  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.7293(0.7293) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4486  avg_val_loss: 0.7034  time: 64s\n",
      "Epoch 13 - avg_train_Score: 0.4486 avgScore: 0.7034\n",
      "Epoch 13 - Save Best Score: 0.7034 Model\n",
      "Epoch 13 - Save Best Loss: 0.7034 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7041(0.7034) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 0s (remain 8m 22s) Loss: 0.5044(0.5044) Grad: 116550.2188  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4191(0.4024) Grad: 83224.2734  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3826(0.4025) Grad: 85218.5859  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 11s) Loss: 0.7301(0.7301) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4025  avg_val_loss: 0.6967  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.4025 avgScore: 0.6967\n",
      "Epoch 14 - Save Best Score: 0.6967 Model\n",
      "Epoch 14 - Save Best Loss: 0.6967 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7042(0.6967) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 0s (remain 8m 31s) Loss: 0.3684(0.3684) Grad: 99323.9141  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3332(0.3618) Grad: 66335.7578  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4026(0.3612) Grad: 82430.5312  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.7427(0.7427) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3612  avg_val_loss: 0.6970  time: 64s\n",
      "Epoch 15 - avg_train_Score: 0.3612 avgScore: 0.6970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7128(0.6970) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 0s (remain 8m 21s) Loss: 0.3578(0.3578) Grad: 88281.1094  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3218(0.3221) Grad: 71674.8516  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2675(0.3221) Grad: 80354.0391  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.7404(0.7404) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3221  avg_val_loss: 0.6926  time: 63s\n",
      "Epoch 16 - avg_train_Score: 0.3221 avgScore: 0.6926\n",
      "Epoch 16 - Save Best Score: 0.6926 Model\n",
      "Epoch 16 - Save Best Loss: 0.6926 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7319(0.6926) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 0s (remain 8m 31s) Loss: 0.2811(0.2811) Grad: 49732.4648  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2706(0.2906) Grad: 60689.2070  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2709(0.2907) Grad: 62121.8125  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7432(0.7432) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.2907  avg_val_loss: 0.6908  time: 64s\n",
      "Epoch 17 - avg_train_Score: 0.2907 avgScore: 0.6908\n",
      "Epoch 17 - Save Best Score: 0.6908 Model\n",
      "Epoch 17 - Save Best Loss: 0.6908 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7227(0.6908) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 0s (remain 8m 29s) Loss: 0.2523(0.2523) Grad: 88194.8828  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2491(0.2584) Grad: 73155.7656  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2636(0.2586) Grad: 78051.5156  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 10s) Loss: 0.7390(0.7390) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2586  avg_val_loss: 0.6916  time: 63s\n",
      "Epoch 18 - avg_train_Score: 0.2586 avgScore: 0.6916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7249(0.6916) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 0s (remain 8m 37s) Loss: 0.2481(0.2481) Grad: 67461.6641  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2105(0.2330) Grad: 80246.4297  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2294(0.2333) Grad: 75704.8672  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 8s) Loss: 0.7412(0.7412) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2333  avg_val_loss: 0.6919  time: 63s\n",
      "Epoch 19 - avg_train_Score: 0.2333 avgScore: 0.6919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7101(0.6919) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 0s (remain 8m 31s) Loss: 0.2316(0.2316) Grad: 72623.6406  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2173(0.2164) Grad: 65297.3711  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2210(0.2166) Grad: 80831.3359  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 10s) Loss: 0.7476(0.7476) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2166  avg_val_loss: 0.6917  time: 63s\n",
      "Epoch 20 - avg_train_Score: 0.2166 avgScore: 0.6917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7196(0.6917) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 1 result ==========\n",
      "score: 0.6917\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 0s (remain 8m 27s) Loss: 6.3900(6.3900) Grad: 87051.9219  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.4557(2.2803) Grad: 318714.0938  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.3035(2.2032) Grad: 299538.0312  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 2.0007(2.0007) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.2032  avg_val_loss: 2.1200  time: 63s\n",
      "Epoch 1 - avg_train_Score: 2.2032 avgScore: 2.1200\n",
      "Epoch 1 - Save Best Score: 2.1200 Model\n",
      "Epoch 1 - Save Best Loss: 2.1200 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 2.0169(2.1200) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 0s (remain 8m 47s) Loss: 1.4982(1.4982) Grad: 314279.6250  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.9782(1.1611) Grad: 122488.0312  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.1871(1.1607) Grad: 133210.4531  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.8847(0.8847) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1607  avg_val_loss: 1.0342  time: 63s\n",
      "Epoch 2 - avg_train_Score: 1.1607 avgScore: 1.0342\n",
      "Epoch 2 - Save Best Score: 1.0342 Model\n",
      "Epoch 2 - Save Best Loss: 1.0342 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 1.1405(1.0342) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 0s (remain 8m 49s) Loss: 0.8790(0.8790) Grad: 253709.4062  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0463(1.2482) Grad: 208448.3594  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.2031(1.2406) Grad: 241500.0156  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.8061(0.8061) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2406  avg_val_loss: 0.9782  time: 64s\n",
      "Epoch 3 - avg_train_Score: 1.2406 avgScore: 0.9782\n",
      "Epoch 3 - Save Best Score: 0.9782 Model\n",
      "Epoch 3 - Save Best Loss: 0.9782 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0529(0.9782) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 11m 19s) Loss: 1.0364(1.0364) Grad: 203938.3750  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0290(1.0720) Grad: 172347.5156  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.1457(1.0696) Grad: 179514.4688  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7479(0.7479) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0696  avg_val_loss: 0.9046  time: 63s\n",
      "Epoch 4 - avg_train_Score: 1.0696 avgScore: 0.9046\n",
      "Epoch 4 - Save Best Score: 0.9046 Model\n",
      "Epoch 4 - Save Best Loss: 0.9046 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9521(0.9046) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 0s (remain 8m 33s) Loss: 0.8847(0.8847) Grad: 131847.8125  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.9421(0.9574) Grad: 131040.2188  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.0391(0.9591) Grad: 174545.6562  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7283(0.7283) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9591  avg_val_loss: 0.8586  time: 63s\n",
      "Epoch 5 - avg_train_Score: 0.9591 avgScore: 0.8586\n",
      "Epoch 5 - Save Best Score: 0.8586 Model\n",
      "Epoch 5 - Save Best Loss: 0.8586 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8929(0.8586) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 0s (remain 8m 44s) Loss: 0.8743(0.8743) Grad: 148640.6719  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.1943(0.8889) Grad: 179562.5781  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.9084(0.8872) Grad: 123757.2812  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.7496(0.7496) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8872  avg_val_loss: 0.8239  time: 63s\n",
      "Epoch 6 - avg_train_Score: 0.8872 avgScore: 0.8239\n",
      "Epoch 6 - Save Best Score: 0.8239 Model\n",
      "Epoch 6 - Save Best Loss: 0.8239 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8837(0.8239) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 0s (remain 8m 36s) Loss: 0.8420(0.8420) Grad: 171478.4062  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9410(0.8009) Grad: 131111.8906  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6685(0.8025) Grad: 124395.5781  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6984(0.6984) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8025  avg_val_loss: 0.7935  time: 64s\n",
      "Epoch 7 - avg_train_Score: 0.8025 avgScore: 0.7935\n",
      "Epoch 7 - Save Best Score: 0.7935 Model\n",
      "Epoch 7 - Save Best Loss: 0.7935 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8897(0.7935) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 0s (remain 8m 40s) Loss: 0.6927(0.6927) Grad: 128934.3359  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7658(0.7332) Grad: 123255.5938  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6553(0.7325) Grad: 103357.1328  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.6799(0.6799) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7325  avg_val_loss: 0.7650  time: 63s\n",
      "Epoch 8 - avg_train_Score: 0.7325 avgScore: 0.7650\n",
      "Epoch 8 - Save Best Score: 0.7650 Model\n",
      "Epoch 8 - Save Best Loss: 0.7650 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8022(0.7650) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 9m 1s) Loss: 0.5471(0.5471) Grad: 101861.0391  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6349(0.6590) Grad: 103039.1172  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6966(0.6611) Grad: 116756.3828  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6601(0.6601) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6611  avg_val_loss: 0.7474  time: 63s\n",
      "Epoch 9 - avg_train_Score: 0.6611 avgScore: 0.7474\n",
      "Epoch 9 - Save Best Score: 0.7474 Model\n",
      "Epoch 9 - Save Best Loss: 0.7474 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8046(0.7474) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 0s (remain 8m 30s) Loss: 0.6556(0.6556) Grad: 126490.9141  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.5481(0.6005) Grad: 97300.3906  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5448(0.6008) Grad: 112342.3594  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6298(0.6298) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6008  avg_val_loss: 0.7325  time: 63s\n",
      "Epoch 10 - avg_train_Score: 0.6008 avgScore: 0.7325\n",
      "Epoch 10 - Save Best Score: 0.7325 Model\n",
      "Epoch 10 - Save Best Loss: 0.7325 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7837(0.7325) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 0s (remain 9m 0s) Loss: 0.4783(0.4783) Grad: 98688.5000  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5202(0.5529) Grad: 136144.9531  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4688(0.5523) Grad: 105069.9219  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6194(0.6194) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5523  avg_val_loss: 0.7147  time: 63s\n",
      "Epoch 11 - avg_train_Score: 0.5523 avgScore: 0.7147\n",
      "Epoch 11 - Save Best Score: 0.7147 Model\n",
      "Epoch 11 - Save Best Loss: 0.7147 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7458(0.7147) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 0s (remain 8m 42s) Loss: 0.5242(0.5242) Grad: 105016.3047  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4719(0.4949) Grad: 104274.8438  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5265(0.4947) Grad: 132252.1875  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.6249(0.6249) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4947  avg_val_loss: 0.7065  time: 63s\n",
      "Epoch 12 - avg_train_Score: 0.4947 avgScore: 0.7065\n",
      "Epoch 12 - Save Best Score: 0.7065 Model\n",
      "Epoch 12 - Save Best Loss: 0.7065 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7276(0.7065) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 9m 12s) Loss: 0.5187(0.5187) Grad: 111236.4609  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4957(0.4465) Grad: 83489.4688  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4927(0.4460) Grad: 90904.4062  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.6210(0.6210) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4460  avg_val_loss: 0.7003  time: 64s\n",
      "Epoch 13 - avg_train_Score: 0.4460 avgScore: 0.7003\n",
      "Epoch 13 - Save Best Score: 0.7003 Model\n",
      "Epoch 13 - Save Best Loss: 0.7003 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7277(0.7003) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 0s (remain 8m 59s) Loss: 0.3760(0.3760) Grad: 86488.2188  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4245(0.4018) Grad: 73955.8281  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3714(0.4011) Grad: 64910.2812  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.6052(0.6052) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4011  avg_val_loss: 0.6981  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.4011 avgScore: 0.6981\n",
      "Epoch 14 - Save Best Score: 0.6981 Model\n",
      "Epoch 14 - Save Best Loss: 0.6981 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7027(0.6981) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 9m 3s) Loss: 0.3112(0.3112) Grad: 81480.7109  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3501(0.3625) Grad: 126227.2109  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3460(0.3624) Grad: 78884.7109  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.6158(0.6158) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3624  avg_val_loss: 0.6948  time: 63s\n",
      "Epoch 15 - avg_train_Score: 0.3624 avgScore: 0.6948\n",
      "Epoch 15 - Save Best Score: 0.6948 Model\n",
      "Epoch 15 - Save Best Loss: 0.6948 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7130(0.6948) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 0s (remain 8m 52s) Loss: 0.3891(0.3891) Grad: 70481.1719  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2849(0.3243) Grad: 67136.1953  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3358(0.3240) Grad: 94871.2109  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.6022(0.6022) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3240  avg_val_loss: 0.6928  time: 63s\n",
      "Epoch 16 - avg_train_Score: 0.3240 avgScore: 0.6928\n",
      "Epoch 16 - Save Best Score: 0.6928 Model\n",
      "Epoch 16 - Save Best Loss: 0.6928 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7007(0.6928) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 0s (remain 8m 46s) Loss: 0.3232(0.3232) Grad: 101428.0859  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2871(0.2908) Grad: 60669.8477  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2573(0.2910) Grad: 85345.1953  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.5969(0.5969) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.2910  avg_val_loss: 0.6902  time: 63s\n",
      "Epoch 17 - avg_train_Score: 0.2910 avgScore: 0.6902\n",
      "Epoch 17 - Save Best Score: 0.6902 Model\n",
      "Epoch 17 - Save Best Loss: 0.6902 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7214(0.6902) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 0s (remain 8m 37s) Loss: 0.2551(0.2551) Grad: 66128.9219  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2821(0.2630) Grad: 78366.8750  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2450(0.2625) Grad: 68956.8047  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.5917(0.5917) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2625  avg_val_loss: 0.6900  time: 65s\n",
      "Epoch 18 - avg_train_Score: 0.2625 avgScore: 0.6900\n",
      "Epoch 18 - Save Best Score: 0.6900 Model\n",
      "Epoch 18 - Save Best Loss: 0.6900 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7121(0.6900) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 9m 21s) Loss: 0.2140(0.2140) Grad: 76157.9531  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2211(0.2399) Grad: 63067.5664  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2531(0.2403) Grad: 95381.6328  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.5868(0.5868) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2403  avg_val_loss: 0.6895  time: 64s\n",
      "Epoch 19 - avg_train_Score: 0.2403 avgScore: 0.6895\n",
      "Epoch 19 - Save Best Score: 0.6895 Model\n",
      "Epoch 19 - Save Best Loss: 0.6895 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7068(0.6895) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 9m 1s) Loss: 0.2344(0.2344) Grad: 68724.8281  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2057(0.2217) Grad: 56296.9922  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2600(0.2213) Grad: 54216.8203  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.5883(0.5883) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2213  avg_val_loss: 0.6891  time: 64s\n",
      "Epoch 20 - avg_train_Score: 0.2213 avgScore: 0.6891\n",
      "Epoch 20 - Save Best Score: 0.6891 Model\n",
      "Epoch 20 - Save Best Loss: 0.6891 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7052(0.6891) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 2 result ==========\n",
      "score: 0.6891\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 0s (remain 8m 26s) Loss: 6.6070(6.6070) Grad: 97692.4531  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.4078(2.3051) Grad: 317602.6562  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.2860(2.2282) Grad: 318673.5000  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 1.7732(1.7732) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.5304(1.7710) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.2282  avg_val_loss: 1.7710  time: 64s\n",
      "Epoch 1 - avg_train_Score: 2.2282 avgScore: 1.7710\n",
      "Epoch 1 - Save Best Score: 1.7710 Model\n",
      "Epoch 1 - Save Best Loss: 1.7710 Model\n",
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 9m 24s) Loss: 0.9589(0.9589) Grad: 227443.7969  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.9405(1.1125) Grad: 262491.4688  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.9004(1.1073) Grad: 253630.7188  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.9262(0.9262) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1073  avg_val_loss: 1.0766  time: 63s\n",
      "Epoch 2 - avg_train_Score: 1.1073 avgScore: 1.0766\n",
      "Epoch 2 - Save Best Score: 1.0766 Model\n",
      "Epoch 2 - Save Best Loss: 1.0766 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 1.0785(1.0766) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 0s (remain 8m 57s) Loss: 0.9396(0.9396) Grad: 360468.7188  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.1913(1.2617) Grad: 214219.2031  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.3285(1.2541) Grad: 189885.3281  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.9703(0.9703) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2541  avg_val_loss: 1.0536  time: 63s\n",
      "Epoch 3 - avg_train_Score: 1.2541 avgScore: 1.0536\n",
      "Epoch 3 - Save Best Score: 1.0536 Model\n",
      "Epoch 3 - Save Best Loss: 1.0536 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0555(1.0536) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 0s (remain 8m 48s) Loss: 0.9810(0.9810) Grad: 148472.2969  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.9608(1.0587) Grad: 144149.3906  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.9449(1.0565) Grad: 164747.0781  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.8467(0.8467) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0565  avg_val_loss: 0.9433  time: 63s\n",
      "Epoch 4 - avg_train_Score: 1.0565 avgScore: 0.9433\n",
      "Epoch 4 - Save Best Score: 0.9433 Model\n",
      "Epoch 4 - Save Best Loss: 0.9433 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 1.0140(0.9433) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 0s (remain 8m 52s) Loss: 0.8498(0.8498) Grad: 137069.9844  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.9942(0.9421) Grad: 104146.5469  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.8513(0.9426) Grad: 135332.0312  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.8033(0.8033) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9426  avg_val_loss: 0.8954  time: 63s\n",
      "Epoch 5 - avg_train_Score: 0.9426 avgScore: 0.8954\n",
      "Epoch 5 - Save Best Score: 0.8954 Model\n",
      "Epoch 5 - Save Best Loss: 0.8954 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9462(0.8954) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 9m 15s) Loss: 0.8190(0.8190) Grad: 140649.3438  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9700(0.8569) Grad: 161760.7188  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7576(0.8573) Grad: 112313.8750  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7718(0.7718) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8573  avg_val_loss: 0.8458  time: 64s\n",
      "Epoch 6 - avg_train_Score: 0.8573 avgScore: 0.8458\n",
      "Epoch 6 - Save Best Score: 0.8458 Model\n",
      "Epoch 6 - Save Best Loss: 0.8458 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8828(0.8458) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 9m 9s) Loss: 0.7511(0.7511) Grad: 139596.7500  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8165(0.7747) Grad: 128506.1406  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8118(0.7753) Grad: 116886.0391  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 11s) Loss: 0.7493(0.7493) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7753  avg_val_loss: 0.8031  time: 64s\n",
      "Epoch 7 - avg_train_Score: 0.7753 avgScore: 0.8031\n",
      "Epoch 7 - Save Best Score: 0.8031 Model\n",
      "Epoch 7 - Save Best Loss: 0.8031 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8658(0.8031) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 9m 4s) Loss: 0.8289(0.8289) Grad: 119960.9531  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7723(0.7065) Grad: 178183.6719  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5318(0.7084) Grad: 114609.1406  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.6947(0.6947) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7084  avg_val_loss: 0.7760  time: 63s\n",
      "Epoch 8 - avg_train_Score: 0.7084 avgScore: 0.7760\n",
      "Epoch 8 - Save Best Score: 0.7760 Model\n",
      "Epoch 8 - Save Best Loss: 0.7760 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7970(0.7760) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 9m 6s) Loss: 0.5056(0.5056) Grad: 102284.8750  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7586(0.6430) Grad: 130406.3516  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6196(0.6421) Grad: 145224.9688  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6827(0.6827) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6421  avg_val_loss: 0.7550  time: 64s\n",
      "Epoch 9 - avg_train_Score: 0.6421 avgScore: 0.7550\n",
      "Epoch 9 - Save Best Score: 0.7550 Model\n",
      "Epoch 9 - Save Best Loss: 0.7550 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7889(0.7550) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 9m 15s) Loss: 0.5714(0.5714) Grad: 118592.0625  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6129(0.5757) Grad: 83988.8594  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6643(0.5785) Grad: 103024.9688  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6782(0.6782) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5785  avg_val_loss: 0.7446  time: 64s\n",
      "Epoch 10 - avg_train_Score: 0.5785 avgScore: 0.7446\n",
      "Epoch 10 - Save Best Score: 0.7446 Model\n",
      "Epoch 10 - Save Best Loss: 0.7446 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7662(0.7446) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 0s (remain 9m 0s) Loss: 0.6122(0.6122) Grad: 101758.4688  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5387(0.5230) Grad: 114637.2891  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5330(0.5240) Grad: 103440.6094  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 16s) Loss: 0.6376(0.6376) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5240  avg_val_loss: 0.7283  time: 63s\n",
      "Epoch 11 - avg_train_Score: 0.5240 avgScore: 0.7283\n",
      "Epoch 11 - Save Best Score: 0.7283 Model\n",
      "Epoch 11 - Save Best Loss: 0.7283 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7559(0.7283) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 0s (remain 8m 57s) Loss: 0.4698(0.4698) Grad: 102147.4219  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4397(0.4731) Grad: 103190.1797  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.4429(0.4729) Grad: 134178.0469  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6339(0.6339) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4729  avg_val_loss: 0.7212  time: 63s\n",
      "Epoch 12 - avg_train_Score: 0.4729 avgScore: 0.7212\n",
      "Epoch 12 - Save Best Score: 0.7212 Model\n",
      "Epoch 12 - Save Best Loss: 0.7212 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7554(0.7212) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 0s (remain 8m 40s) Loss: 0.4641(0.4641) Grad: 111418.9844  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4562(0.4259) Grad: 79102.8828  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.4383(0.4255) Grad: 85879.2109  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.6471(0.6471) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4255  avg_val_loss: 0.7182  time: 63s\n",
      "Epoch 13 - avg_train_Score: 0.4255 avgScore: 0.7182\n",
      "Epoch 13 - Save Best Score: 0.7182 Model\n",
      "Epoch 13 - Save Best Loss: 0.7182 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7242(0.7182) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 9m 29s) Loss: 0.4197(0.4197) Grad: 79105.3906  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 0m 52s (remain 0m 4s) Loss: 0.3459(0.3858) Grad: 102830.0391  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 0m 56s (remain 0m 0s) Loss: 0.4110(0.3866) Grad: 77052.4219  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.6496(0.6496) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.3866  avg_val_loss: 0.7144  time: 65s\n",
      "Epoch 14 - avg_train_Score: 0.3866 avgScore: 0.7144\n",
      "Epoch 14 - Save Best Score: 0.7144 Model\n",
      "Epoch 14 - Save Best Loss: 0.7144 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7604(0.7144) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 9m 8s) Loss: 0.3857(0.3857) Grad: 77900.7031  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.3488(0.3486) Grad: 68894.7656  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.3420(0.3482) Grad: 75528.4062  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.6477(0.6477) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3482  avg_val_loss: 0.7126  time: 65s\n",
      "Epoch 15 - avg_train_Score: 0.3482 avgScore: 0.7126\n",
      "Epoch 15 - Save Best Score: 0.7126 Model\n",
      "Epoch 15 - Save Best Loss: 0.7126 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7424(0.7126) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 0s (remain 8m 57s) Loss: 0.3989(0.3989) Grad: 62328.6406  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2710(0.3107) Grad: 61670.7266  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3082(0.3111) Grad: 99588.9219  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 11s) Loss: 0.6451(0.6451) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3111  avg_val_loss: 0.7113  time: 65s\n",
      "Epoch 16 - avg_train_Score: 0.3111 avgScore: 0.7113\n",
      "Epoch 16 - Save Best Score: 0.7113 Model\n",
      "Epoch 16 - Save Best Loss: 0.7113 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7367(0.7113) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 0s (remain 8m 39s) Loss: 0.2809(0.2809) Grad: 90524.4062  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 0m 48s (remain 0m 4s) Loss: 0.3045(0.2815) Grad: 75769.3906  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 0m 52s (remain 0m 0s) Loss: 0.2405(0.2812) Grad: 66220.8594  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 0.6233(0.6233) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.2812  avg_val_loss: 0.7079  time: 62s\n",
      "Epoch 17 - avg_train_Score: 0.2812 avgScore: 0.7079\n",
      "Epoch 17 - Save Best Score: 0.7079 Model\n",
      "Epoch 17 - Save Best Loss: 0.7079 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7360(0.7079) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 9m 11s) Loss: 0.2415(0.2415) Grad: 79750.8125  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2420(0.2517) Grad: 85132.7344  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2348(0.2517) Grad: 76962.5938  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.6210(0.6210) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2517  avg_val_loss: 0.7094  time: 64s\n",
      "Epoch 18 - avg_train_Score: 0.2517 avgScore: 0.7094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7260(0.7094) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 0s (remain 8m 49s) Loss: 0.1925(0.1925) Grad: 64840.3672  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2145(0.2288) Grad: 63180.2695  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.1958(0.2285) Grad: 58450.0664  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6137(0.6137) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2285  avg_val_loss: 0.7088  time: 64s\n",
      "Epoch 19 - avg_train_Score: 0.2285 avgScore: 0.7088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7312(0.7088) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 9m 40s) Loss: 0.1708(0.1708) Grad: 70640.4844  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.1856(0.2103) Grad: 68220.1016  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.1899(0.2106) Grad: 70221.9375  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6153(0.6153) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2106  avg_val_loss: 0.7090  time: 64s\n",
      "Epoch 20 - avg_train_Score: 0.2106 avgScore: 0.7090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7436(0.7090) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 3 result ==========\n",
      "score: 0.7090\n",
      "========== fold: 4 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 0s (remain 8m 17s) Loss: 5.0225(5.0225) Grad: 89598.4062  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.1455(2.2037) Grad: 259454.6094  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.0658(2.1293) Grad: 256876.6562  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 15s) Loss: 1.5972(1.5972) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.1293  avg_val_loss: 1.7219  time: 63s\n",
      "Epoch 1 - avg_train_Score: 2.1293 avgScore: 1.7219\n",
      "Epoch 1 - Save Best Score: 1.7219 Model\n",
      "Epoch 1 - Save Best Loss: 1.7219 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 1.7307(1.7219) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 0s (remain 8m 46s) Loss: 1.1513(1.1513) Grad: 265384.4688  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.1799(1.1092) Grad: 318957.5625  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.0188(1.1054) Grad: 272062.4688  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 1.2328(1.2328) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1054  avg_val_loss: 1.0594  time: 63s\n",
      "Epoch 2 - avg_train_Score: 1.1054 avgScore: 1.0594\n",
      "Epoch 2 - Save Best Score: 1.0594 Model\n",
      "Epoch 2 - Save Best Loss: 1.0594 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 1.1480(1.0594) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 0s (remain 8m 57s) Loss: 0.8753(0.8753) Grad: 292309.3125  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 1.2623(1.2736) Grad: 182058.9688  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 1.4716(1.2628) Grad: 219553.8438  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 1.0850(1.0850) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2628  avg_val_loss: 1.0033  time: 65s\n",
      "Epoch 3 - avg_train_Score: 1.2628 avgScore: 1.0033\n",
      "Epoch 3 - Save Best Score: 1.0033 Model\n",
      "Epoch 3 - Save Best Loss: 1.0033 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 1.1410(1.0033) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 0s (remain 8m 38s) Loss: 1.1510(1.1510) Grad: 188941.3594  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 1.0129(1.0488) Grad: 176067.9844  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 1.0748(1.0465) Grad: 172751.2344  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 1.0028(1.0028) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0465  avg_val_loss: 0.9203  time: 65s\n",
      "Epoch 4 - avg_train_Score: 1.0465 avgScore: 0.9203\n",
      "Epoch 4 - Save Best Score: 0.9203 Model\n",
      "Epoch 4 - Save Best Loss: 0.9203 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0290(0.9203) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 0s (remain 8m 55s) Loss: 1.1020(1.1020) Grad: 168739.4688  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0803(0.9470) Grad: 120878.9297  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9515(0.9471) Grad: 148186.4531  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.9643(0.9643) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9471  avg_val_loss: 0.8683  time: 64s\n",
      "Epoch 5 - avg_train_Score: 0.9471 avgScore: 0.8683\n",
      "Epoch 5 - Save Best Score: 0.8683 Model\n",
      "Epoch 5 - Save Best Loss: 0.8683 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9629(0.8683) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 9m 3s) Loss: 0.9786(0.9786) Grad: 168899.6719  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7703(0.8517) Grad: 101026.3828  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.7442(0.8498) Grad: 123967.1719  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.8699(0.8699) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8498  avg_val_loss: 0.8243  time: 65s\n",
      "Epoch 6 - avg_train_Score: 0.8498 avgScore: 0.8243\n",
      "Epoch 6 - Save Best Score: 0.8243 Model\n",
      "Epoch 6 - Save Best Loss: 0.8243 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8819(0.8243) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 9m 7s) Loss: 1.0682(1.0682) Grad: 148269.7812  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7834(0.7767) Grad: 125002.2344  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7352(0.7752) Grad: 109857.4531  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 13s) Loss: 0.8273(0.8273) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7752  avg_val_loss: 0.7838  time: 64s\n",
      "Epoch 7 - avg_train_Score: 0.7752 avgScore: 0.7838\n",
      "Epoch 7 - Save Best Score: 0.7838 Model\n",
      "Epoch 7 - Save Best Loss: 0.7838 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8779(0.7838) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 0s (remain 8m 47s) Loss: 0.6592(0.6592) Grad: 122929.6406  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6073(0.6986) Grad: 90723.7188  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.7662(0.6984) Grad: 113133.1484  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7859(0.7859) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.6984  avg_val_loss: 0.7559  time: 64s\n",
      "Epoch 8 - avg_train_Score: 0.6984 avgScore: 0.7559\n",
      "Epoch 8 - Save Best Score: 0.7559 Model\n",
      "Epoch 8 - Save Best Loss: 0.7559 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7985(0.7559) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 0s (remain 8m 48s) Loss: 0.6617(0.6617) Grad: 123930.7188  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.7628(0.6333) Grad: 131020.2109  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4968(0.6330) Grad: 100621.6953  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7377(0.7377) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6330  avg_val_loss: 0.7378  time: 64s\n",
      "Epoch 9 - avg_train_Score: 0.6330 avgScore: 0.7378\n",
      "Epoch 9 - Save Best Score: 0.7378 Model\n",
      "Epoch 9 - Save Best Loss: 0.7378 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8093(0.7378) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 0s (remain 8m 38s) Loss: 0.4697(0.4697) Grad: 98762.9609  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6604(0.5741) Grad: 97594.3125  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5023(0.5736) Grad: 127558.1484  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 12s) Loss: 0.7211(0.7211) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5736  avg_val_loss: 0.7227  time: 64s\n",
      "Epoch 10 - avg_train_Score: 0.5736 avgScore: 0.7227\n",
      "Epoch 10 - Save Best Score: 0.7227 Model\n",
      "Epoch 10 - Save Best Loss: 0.7227 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7861(0.7227) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 9m 9s) Loss: 0.5338(0.5338) Grad: 88987.1250  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 0m 52s (remain 0m 4s) Loss: 0.4987(0.5184) Grad: 100750.0859  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 0m 56s (remain 0m 0s) Loss: 0.5319(0.5196) Grad: 90703.7109  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7170(0.7170) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5196  avg_val_loss: 0.7157  time: 66s\n",
      "Epoch 11 - avg_train_Score: 0.5196 avgScore: 0.7157\n",
      "Epoch 11 - Save Best Score: 0.7157 Model\n",
      "Epoch 11 - Save Best Loss: 0.7157 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7872(0.7157) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 0s (remain 8m 44s) Loss: 0.4140(0.4140) Grad: 91603.7656  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.4819(0.4697) Grad: 114093.1250  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.4639(0.4698) Grad: 83166.6328  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7226(0.7226) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4698  avg_val_loss: 0.7066  time: 65s\n",
      "Epoch 12 - avg_train_Score: 0.4698 avgScore: 0.7066\n",
      "Epoch 12 - Save Best Score: 0.7066 Model\n",
      "Epoch 12 - Save Best Loss: 0.7066 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7793(0.7066) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 0s (remain 8m 34s) Loss: 0.4372(0.4372) Grad: 86534.4141  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3727(0.4279) Grad: 75579.6016  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4416(0.4292) Grad: 142113.9062  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.7122(0.7122) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4292  avg_val_loss: 0.7046  time: 64s\n",
      "Epoch 13 - avg_train_Score: 0.4292 avgScore: 0.7046\n",
      "Epoch 13 - Save Best Score: 0.7046 Model\n",
      "Epoch 13 - Save Best Loss: 0.7046 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7813(0.7046) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 0s (remain 8m 57s) Loss: 0.4661(0.4661) Grad: 145278.6094  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3424(0.3849) Grad: 87412.2344  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4268(0.3850) Grad: 65843.5469  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.7162(0.7162) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.3850  avg_val_loss: 0.7011  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.3850 avgScore: 0.7011\n",
      "Epoch 14 - Save Best Score: 0.7011 Model\n",
      "Epoch 14 - Save Best Loss: 0.7011 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7742(0.7011) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 9m 3s) Loss: 0.3472(0.3472) Grad: 94145.7422  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3417(0.3429) Grad: 69257.3047  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3754(0.3425) Grad: 123806.6250  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.7153(0.7153) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3425  avg_val_loss: 0.6980  time: 64s\n",
      "Epoch 15 - avg_train_Score: 0.3425 avgScore: 0.6980\n",
      "Epoch 15 - Save Best Score: 0.6980 Model\n",
      "Epoch 15 - Save Best Loss: 0.6980 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7650(0.6980) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 0s (remain 8m 26s) Loss: 0.3041(0.3041) Grad: 92910.7109  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2661(0.3101) Grad: 78800.8516  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2903(0.3103) Grad: 88302.8594  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6964(0.6964) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3103  avg_val_loss: 0.6961  time: 63s\n",
      "Epoch 16 - avg_train_Score: 0.3103 avgScore: 0.6961\n",
      "Epoch 16 - Save Best Score: 0.6961 Model\n",
      "Epoch 16 - Save Best Loss: 0.6961 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7785(0.6961) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 0s (remain 8m 49s) Loss: 0.3150(0.3150) Grad: 94465.8203  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2522(0.2772) Grad: 50673.0703  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3511(0.2772) Grad: 73296.1719  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7001(0.7001) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.2772  avg_val_loss: 0.6962  time: 64s\n",
      "Epoch 17 - avg_train_Score: 0.2772 avgScore: 0.6962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7698(0.6962) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 9m 3s) Loss: 0.2403(0.2403) Grad: 71839.8672  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2400(0.2500) Grad: 76920.0000  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2055(0.2501) Grad: 66343.5703  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7050(0.7050) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2501  avg_val_loss: 0.6977  time: 64s\n",
      "Epoch 18 - avg_train_Score: 0.2501 avgScore: 0.6977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7765(0.6977) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 0s (remain 8m 52s) Loss: 0.2360(0.2360) Grad: 92832.0469  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2404(0.2276) Grad: 78533.7656  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2454(0.2280) Grad: 66370.3906  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 14s) Loss: 0.7049(0.7049) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2280  avg_val_loss: 0.6962  time: 63s\n",
      "Epoch 19 - avg_train_Score: 0.2280 avgScore: 0.6962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7729(0.6962) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_34009/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 0s (remain 8m 48s) Loss: 0.2112(0.2112) Grad: 78472.4219  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2149(0.2110) Grad: 101525.9531  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2042(0.2114) Grad: 78963.6016  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7069(0.7069) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2114  avg_val_loss: 0.6958  time: 63s\n",
      "Epoch 20 - avg_train_Score: 0.2114 avgScore: 0.6958\n",
      "Epoch 20 - Save Best Score: 0.6958 Model\n",
      "Epoch 20 - Save Best Loss: 0.6958 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7713(0.6958) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 4 result ==========\n",
      "score: 0.6958\n",
      "========== CV ==========\n",
      "score: 0.6976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC6xklEQVR4nOzdd5hU9fXH8fedPrNlZtssu/SmIAIiiiIWNDbsvWAido1dExNJfrGlEFvsvWFDI6jYiIoFNUhEKTYUQXpne5nZnXZ/f9zZgZUi4O7OzvJ5Pc8+Zu/euffciU9ynvM993wN0zRNRERERERERERE2pAt3QGIiIiIiIiIiMjOR0UpERERERERERFpcypKiYiIiIiIiIhIm1NRSkRERERERERE2pyKUiIiIiIiIiIi0uZUlBIRERERERERkTanopSIiIiIiIiIiLQ5FaVERERERERERKTNqSglIiIiIiIiIiJtTkUpEWkzS5YswTAMxo8fnzp20003YRjGNn3eMAxuuummFo1p5MiRjBw5skWvKSIiIrIjlCuJyM5GRSkR2azjjjsOn89HbW3tFs8566yzcLlclJeXt2Fk22/evHncdNNNLFmyJN2hpEybNg3DMJg0aVK6QxEREZEdoFyp9S1ZsoRzzz2X3r174/F46NSpEwceeCA33nhjukMTkRaiopSIbNZZZ51FOBzm1Vdf3ezfQ6EQr732GkceeSQFBQU7fJ//+7//IxwO7/Dnt8W8efO4+eabN5tovfvuu7z77ruten8RERHpeJQrta6FCxcyZMgQ3nnnHc4880zuv/9+LrvsMgoKCrj11lvbPB4RaR2OdAcgIu3TcccdR05ODhMmTODss8/e5O+vvfYa9fX1nHXWWb/oPg6HA4cjff9T5HK50nZvERERyVzKlVrXXXfdRV1dHXPnzqV79+7N/rZu3bo2jaW+vp6srKw2vafIzkKdUiKyWV6vl5NOOon3339/s//HP2HCBHJycjjuuOOoqKjg97//PQMHDiQ7O5vc3FxGjRrFl19++bP32dychMbGRq655hqKiopS91ixYsUmn126dCmXXnopu+66K16vl4KCAk499dRmq3zjx4/n1FNPBeDggw/GMAwMw2DatGnA5uckrFu3jvPPP5/i4mI8Hg+DBw/m6aefbnZO08yHO+64g0cffZTevXvjdrvZe++9+fzzz3/2ubfVokWLOPXUU8nPz8fn87Hvvvvy1ltvbXLefffdx4ABA/D5fOTl5bHXXnsxYcKE1N9ra2u5+uqr6dGjB263m2AwyGGHHcbs2bNbLFYREZGdiXKl1s2VfvzxR7p06bJJQQogGAxucuw///kPBx10EDk5OeTm5rL33ns3y4UAJk6cyNChQ/F6vRQWFvLrX/+alStXNjvnnHPOITs7mx9//JGjjjqKnJycVGExkUhw9913M2DAADweD8XFxVx88cVUVlb+7POIyOapU0pEtuiss87i6aef5qWXXuLyyy9PHa+oqEi1Unu9Xr799lsmT57MqaeeSs+ePVm7di2PPPIIBx10EPPmzaO0tHS77nvBBRfw3HPPMXr0aPbbbz8++OADjj766E3O+/zzz/n0008544wz6NKlC0uWLOGhhx5i5MiRzJs3D5/Px4EHHsiVV17Jvffey5/+9Cf69+8PkPrnT4XDYUaOHMnChQu5/PLL6dmzJxMnTuScc86hqqqKq666qtn5EyZMoLa2losvvhjDMLjttts46aSTWLRoEU6nc7ue+6fWrl3LfvvtRygU4sorr6SgoICnn36a4447jkmTJnHiiScC8Nhjj3HllVdyyimncNVVV9HQ0MBXX33FZ599xujRowG45JJLmDRpEpdffjm77bYb5eXl/Pe//+W7775jzz33/EVxioiI7KyUK7VertS9e3fee+89PvjgAw455JCtfh/jx4/nvPPOY8CAAYwdO5ZAIMCcOXN4++23U7nQ+PHjOffcc9l7770ZN24ca9eu5Z577mH69OnMmTOHQCCQul4sFuOII45g//3354477sDn8wFw8cUXp65z5ZVXsnjxYu6//37mzJnD9OnTf3HuJ7JTMkVEtiAWi5klJSXm8OHDmx1/+OGHTcB85513TNM0zYaGBjMejzc7Z/Hixabb7TZvueWWZscA86mnnkodu/HGG82N/6do7ty5JmBeeumlza43evRoEzBvvPHG1LFQKLRJzDNmzDAB85lnnkkdmzhxogmYH3744SbnH3TQQeZBBx2U+v3uu+82AfO5555LHYtEIubw4cPN7Oxss6amptmzFBQUmBUVFalzX3vtNRMw33jjjU3utbEPP/zQBMyJEydu8Zyrr77aBMxPPvkkday2ttbs2bOn2aNHj9R3fvzxx5sDBgzY6v38fr952WWXbfUcERER2T7KlSytkSt98803ptfrNQFzjz32MK+66ipz8uTJZn19fbPzqqqqzJycHHOfffYxw+Fws78lEolUfMFg0Nx9992bnfPmm2+agHnDDTekjo0ZM8YEzOuvv77ZtT755BMTMJ9//vlmx99+++3NHheRbaPX90Rki+x2O2eccQYzZsxo1uY9YcIEiouL+dWvfgWA2+3GZrP+5yQej1NeXk52dja77rrrdr8eNmXKFACuvPLKZsevvvrqTc71er2p/xyNRikvL6dPnz4EAoEdfi1typQpdOrUiTPPPDN1zOl0cuWVV1JXV8dHH33U7PzTTz+dvLy81O8HHHAAYL1290tNmTKFYcOGsf/++6eOZWdnc9FFF7FkyRLmzZsHQCAQYMWKFVtthQ8EAnz22WesWrXqF8clIiIiFuVKltbIlQYMGMDcuXP59a9/zZIlS7jnnns44YQTKC4u5rHHHkudN3XqVGpra7n++uvxeDzNrtH02uMXX3zBunXruPTSS5udc/TRR9OvX7/Njkb47W9/2+z3iRMn4vf7OeywwygrK0v9DB06lOzsbD788MOtPo+IbJ6KUiKyVU3v0De9k79ixQo++eQTzjjjDOx2O2C9X3/XXXfRt29f3G43hYWFFBUV8dVXX1FdXb1d91u6dCk2m43evXs3O77rrrtucm44HOaGG26ga9euze5bVVW13ffd+P59+/ZNJY5NmlrYly5d2ux4t27dmv3elHS1xGyBpUuXbva5fxrLH//4R7Kzsxk2bBh9+/blsssuY/r06c0+c9ttt/HNN9/QtWtXhg0bxk033dQihTMREZGdnXIlS2vkSrvssgvPPvssZWVlfPXVV/zjH//A4XBw0UUX8d577wHW7CmA3Xfffasxw+a/o379+m0Ss8PhoEuXLs2OLViwgOrqaoLBIEVFRc1+6urq2nz4ukhHoaKUiGzV0KFD6devHy+88AIAL7zwAqZpNttJ5h//+AfXXnstBx54IM899xzvvPMOU6dOZcCAASQSiVaL7YorruDvf/87p512Gi+99BLvvvsuU6dOpaCgoFXvu7GmZPOnTNNsk/uDlQTOnz+fF198kf3335+XX36Z/fffnxtvvDF1zmmnncaiRYu47777KC0t5fbbb2fAgAH85z//abM4RUREOiLlSlvXErmS3W5n4MCBjB07lldffRWA559/vkXi25yNO9uaJBIJgsEgU6dO3ezPLbfc0mrxiHRkGnQuIj/rrLPO4i9/+QtfffUVEyZMoG/fvuy9996pv0+aNImDDz6YJ554otnnqqqqKCws3K57de/enUQiwY8//thsNWv+/PmbnDtp0iTGjBnDnXfemTrW0NBAVVVVs/N+umPNz93/q6++IpFINEtGvv/++9Tf20r37t03+9ybiyUrK4vTTz+d008/nUgkwkknncTf//53xo4dm2pTLykp4dJLL+XSSy9l3bp17Lnnnvz9739n1KhRbfNAIiIiHZRypbbLlfbaay8AVq9eDZDqGPvmm2/o06fPZj/TFNP8+fM3GZo+f/78bYq5d+/evPfee4wYMaLZa5Ei8suoU0pEflbTSt8NN9zA3Llzm638gbV69dPVrokTJ26yxe62aCqQ3Hvvvc2O33333Zucu7n73nfffcTj8WbHsrKyADZJwDbnqKOOYs2aNfz73/9OHYvFYtx3331kZ2dz0EEHbctjtIijjjqKmTNnMmPGjNSx+vp6Hn30UXr06MFuu+0GQHl5ebPPuVwudtttN0zTJBqNEo/HN2nRDwaDlJaW0tjY2PoPIiIi0sEpV2r5XOmTTz4hGo1ucrxpplZTQe7www8nJyeHcePG0dDQ0Ozcpmffa6+9CAaDPPzww81yn//85z989913m9258KdOO+004vE4f/3rXzf5WywW26bvTkQ2pU4pEflZPXv2ZL/99uO1114D2CTROuaYY7jllls499xz2W+//fj66695/vnn6dWr13bfa4899uDMM8/kwQcfpLq6mv3224/333+fhQsXbnLuMcccw7PPPovf72e33XZjxowZvPfeexQUFGxyTbvdzq233kp1dTVut5tDDjmEYDC4yTUvuugiHnnkEc455xxmzZpFjx49mDRpEtOnT+fuu+8mJydnu59pa15++eXUyuLGxowZw/XXX88LL7zAqFGjuPLKK8nPz+fpp59m8eLFvPzyy6nVycMPP5xOnToxYsQIiouL+e6777j//vs5+uijycnJoaqqii5dunDKKacwePBgsrOzee+99/j888+brZyKiIjIjlGu1PK50q233sqsWbM46aSTGDRoEACzZ8/mmWeeIT8/PzXYPTc3l7vuuosLLriAvffem9GjR5OXl8eXX35JKBTi6aefxul0cuutt3Luuedy0EEHceaZZ7J27VruueceevTowTXXXPOz8Rx00EFcfPHFjBs3jrlz53L44YfjdDpZsGABEydO5J577uGUU05pkWcX2amka9s/EcksDzzwgAmYw4YN2+RvDQ0N5u9+9zuzpKTE9Hq95ogRI8wZM2ZssoXwtmxzbJqmGQ6HzSuvvNIsKCgws7KyzGOPPdZcvnz5JtscV1ZWmueee65ZWFhoZmdnm0cccYT5/fffm927dzfHjBnT7JqPPfaY2atXL9Nutzfb8vinMZqmaa5duzZ1XZfLZQ4cOLBZzBs/y+23377J9/HTODfnww8/NIEt/nzyySemaZrmjz/+aJ5yyilmIBAwPR6POWzYMPPNN99sdq1HHnnEPPDAA82CggLT7XabvXv3Nq+77jqzurraNE3TbGxsNK+77jpz8ODBZk5OjpmVlWUOHjzYfPDBB7cao4iIiGw75UpPNTvnl+ZK06dPNy+77DJz9913N/1+v+l0Os1u3bqZ55xzjvnjjz9ucv7rr79u7rfffqbX6zVzc3PNYcOGmS+88EKzc/7973+bQ4YMMd1ut5mfn2+eddZZ5ooVK5qdM2bMGDMrK2uLcT366KPm0KFDTa/Xa+bk5JgDBw40//CHP5irVq3a6vOIyOYZptmG03hFRERERERERETQTCkREREREREREUkDFaVERERERERERKTNqSglIiIiIiIiIiJtTkUpERERERERERFpcypKiYiIiIiIiIhIm1NRSkRERERERERE2pwj3QG0tUQiwapVq8jJycEwjHSHIyIiIu2caZrU1tZSWlqKzbbzrucphxIREZFtta35005XlFq1ahVdu3ZNdxgiIiKSYZYvX06XLl3SHUbaKIcSERGR7fVz+dNOV5TKyckBrC8mNzc3zdGIiIhIe1dTU0PXrl1TOcTOSjmUiIiIbKttzZ92uqJUU7t5bm6uEioRERHZZjv7K2vKoURERGR7/Vz+tPMORhARERERERERkbRRUUpERERERERERNqcilIiIiIiIiIiItLmVJQSEREREREREZE2p6KUiIiIiIiIiIi0ORWlRERERERERESkzakoJSIiIiIiIiIibU5FKRERERERERERaXMqSomIiIiIiIiISJtTUUpERERERERERNqcilIiIiIiIiIiItLmVJQSEREREREREZE2p6KUiIiIiIiIiIi0ORWlRERERERERESkzTnSHUBH8+X6L1lRu4I9g3tSkl2S7nBERERE2r3KhkrmrJuDzbAxsuvIdIcjIiIibUSdUi3srll3cf0n1zN3/dx0hyIiIiKSERZULuCqD6/izi/uTHcoIiIi0oZUlGphBZ4CAMrD5WmORERERCQzFHiT+VOD8icREZGdiYpSLUxJlYiIiMj2aVrUq43UEolH0hyNiIiItBUVpVqYOqVEREREtk+uOxeHYY06rWioSHM0IiIi0lZUlGphTZ1SZeGyNEciIiIikhlsho18Tz6gHEpERGRnoqJUC0t1Sun1PREREZFtlhqBoG5zERGRnYaKUi2s0FsIKKESERER2R6ayykiIrLzUVGqhTUlVBUNFZimmeZoRERERDKD5nKKiIjsfFSUamFNRaloIkpNpCbN0YiIiIhkBnVKiYiI7HxUlGphbrubbGc2oKRKREREZFupU0pERGTno6JUK9CgThEREZHto04pERGRnY+KUq1AO/CJiIiIbB8t6omIiOx8VJRqBUqqRERERLaPFvVERER2PipKtYJ8Tz6gopSIiIjItmpa1KturCYaj6Y5GhEREWkLKkq1As1EEBEREdk+AXcAu2EHlEOJiIjsLFSUagXaPUZERERk+9gMG3mePEBFKRERkZ2FilKtoNBbCKgoJSIiIrI9lEOJiIjsXFSUagV6fU9ERERk+6nbXEREZOeiolQr2DihMk0zzdGIiIiIZAYt7ImIiOxcVJRqBU0JVSQRoS5al+ZoRERERDKDOqVERER2LipKtQKvw4vP4QOUVImIiIhsK3VKiYiI7FxUlGolSqpEREREtk++Jx+AinBFmiMRERGRtqCiVCtR+7mIiIjI9tGinoiIyM5FRalWoqRKREREZPtoUU9ERGTnoqJUK1FSJSIiIrJ9mhb1qhqriCaiaY5GREREWpuKUq2kKakqC5elORIRERGRzJDnzsNm2DAxqWyoTHc4IiIi0spUlGolhd5CQK/viYiIiGwru81OnjsPULe5iIjIziCtRalx48ax9957k5OTQzAY5IQTTmD+/Pk/+7mJEyfSr18/PB4PAwcOZMqUKW0Q7fZpen1Pu8eIiIhIS+rI+RNoLqeIiMjOJK1FqY8++ojLLruM//3vf0ydOpVoNMrhhx9OfX39Fj/z6aefcuaZZ3L++eczZ84cTjjhBE444QS++eabNox8y659aS77/uN9lqy3vlolVCIiItKSOmL+NGtpJb+6cxpnPvo/zeUUERHZiRimaZrpDqLJ+vXrCQaDfPTRRxx44IGbPef000+nvr6eN998M3Vs3333ZY899uDhhx/+2XvU1NTg9/uprq4mNze3xWJvcv74z3n/+3X84ZhCHvrxAjx2DzPPmolhGC1+LxEREWl9rZ07/FJtkT9B634PX6+o5tj7/0txrptDDnifNxe9yTVDr+G83c9r0fuIiIhI29jWvKFdzZSqrq4GID8/f4vnzJgxg0MPPbTZsSOOOIIZM2a0amzbKpjrAaA+ZP2zId5AKBZKZ0giIiLSgXWE/Kk41w3A+tpG8j3Wc6hTSkREpONzpDuAJolEgquvvpoRI0aw++67b/G8NWvWUFxc3OxYcXExa9as2ez5jY2NNDY2pn6vqalpmYC3oCmpqqiz4XV4CcfClIfLyXJmtep9RUREZOfTWvkTtG0OVZDtxm4ziCdMPEYA0AgEERGRnUG76ZS67LLL+Oabb3jxxRdb9Lrjxo3D7/enfrp27dqi1/+pTslOqXU1DRtW+pRUiYiISCtorfwJ2jaHstsMirKthT2bmQOoU0pERGRn0C6KUpdffjlvvvkmH374IV26dNnquZ06dWLt2rXNjq1du5ZOnTpt9vyxY8dSXV2d+lm+fHmLxb05xcmi1Nrahg27xyipEhERkRbWmvkTpCOHsopSZiwb0KKeiIjIziCtRSnTNLn88st59dVX+eCDD+jZs+fPfmb48OG8//77zY5NnTqV4cOHb/Z8t9tNbm5us5/WFEwmVGtrGrV7jIiIiLS4tsifIB05lLWwF4lYIw+UP4mIiHR8aZ0pddlllzFhwgRee+01cnJyUnMN/H4/Xq8XgLPPPpvOnTszbtw4AK666ioOOugg7rzzTo4++mhefPFFvvjiCx599NG0PcfGmjqlyuoayXdbr++VNZSlMyQRERHpQDpi/gQbOqVCYR8AlQ2VxBIxHLZ2MwJVREREWlhaO6UeeughqqurGTlyJCUlJamff//736lzli1bxurVq1O/77fffkyYMIFHH32UwYMHM2nSJCZPnrzV4Z5tKd/nwmEzME3w2AOAVvpERESk5XTE/AmgOMda2KuudWFgYGJS1ViV3qBERESkVaV16ck0zZ89Z9q0aZscO/XUUzn11FNbIaJfzmYzCOa4WVXdgMP0AypKiYiISMvpiPkTbOg2X18XJc+XR0VDBeXhcgq9hWmOTERERFpLuxh03tE0zUQw4xrUKSIiIrItNp7LmdrBWAt7IiIiHZqKUq2gaSZCLGLNRFBCJSIiIrJ1TZ1S62o22sFYC3siIiIdmopSraApqQqHk7vHKKESERER2aqm/Km8PkKeWzsYi4iI7AxUlGoFTUlVbb3VMRWOhQlFQ+kMSURERKRdy/M5cdmt1NRrT87l1MKeiIhIh6aiVCtoKkqV1dlw263ClJIqERERkS0zDCM1V8pp5gLqlBIREenoVJRqBU0zpdbXNFLgUfu5iIiIyLZoWtgjngNoUU9ERKSjU1GqFTQlVGs1qFNERERkm6U2i4km53JqUU9ERKRDU1GqFRTnWEWpylCUgFtbGouIiIhsi2Ayhwo3WDsYl4XL0hmOiIiItDIVpVpBrteB22F9tT57AFBRSkREROTnNHWb14WSC3yNlcQT8XSGJCIiIq1IRalWYBhGKqlyod1jRERERLZF0+t71bXWPxNmgqrGqjRGJCIiIq1JRamWVv4j/PgBu2SFADASyUGd6pQSERER2bzGOlj+OX0bvgZgXU2MgDsAaGFPRESkI1NRqqVNvhSePZF9HfMBiDcN6lRCJSIiIrJ5Kz6HJw5ll89vAJKbxWgHYxERkQ5PRamWllsCQFd7FQCNjdagTiVUIiIiIluQWwqAK7QGgJqG2IbNYrSwJyIi0mGpKNXScjsD0MlWAUB9KFmUUkIlIiIisnnJopTRWEOhKwpAliMAaGFPRESkI1NRqqXlWJ1SBXErgaquswZ11kfraYg1pC0sERERkXbLnQMuaw7nbtl1ALgMbRYjIiLS0ako1dKSr+/lRtcDsL7GwGlzAkqqRERERLYomUP18dQCYNNmMSIiIh2eilItLcdqP/c2rANgXU2EAq8GdYqIiIhsVbLbvKerBoBENBvQop6IiEhHpqJUS0uu8jnq1wAmdY0x8pKDOsvCZWkMTERERKQdS86V6uyoBCAS0WYxIiIiHZ2KUi0tucpnxBvp7LZmSGXZA4BW+kRERES2KFmUKqZpsxgvoKKUiIhIR6aiVEtzuMFXCED/LGsmgscWAJRUiYiIiGxRcmEvP251ltfUWUWpioYKEmYibWGJiIhI61FRqjUkX+HrmxzUaTdzARWlRERERLYo2SmVE7E2i6modQEQN+NUN1anLSwRERFpPSpKtYbksPPuLiuBMmMa1CkiIiKyVclOKU9qs5gYuS4t7ImIiHRkKkq1hmSnVGd706DOLEAJlYiIiMgWJTul7KF12IkTisRTm8VoYU9ERKRjUlGqNeR2BiBoWoM6w+ENMxFEREREZDOyisDmwDAT9PTUA5DtzAO0sCciItJRqSjVGpLt53nJQZ219do9RkRERGSrbHbI7gRAf19ysxjDD6hTSkREpKNSUao1JF/fy45YMxHKa6xBnbXRWhrjjWkLS0RERKRdS+ZQvb1WUcqBZkqJiIh0ZCpKtYbkoHN32CpKra+24bA5AKgI6xU+ERERkc1Kdpt3c1YBYMZzAHVKiYiIdFQqSrWG5CqfvaESNxEaY2ZqUGdZuCydkYmIiIi0X8lh56W2KgBiyc1ilD+JiIh0TCpKtQZPAJw+APom289zmgZ1aqVPREREZPOSRaki08qXGho0l1NERKQjU1GqNRhGqv28X3JQp9cWAJRUiYiIiGxRcgRCILYegLqQtcinRT0REZGOSUWp1pJc6evlrgHA2TSoU0mViIiIyOYlRyBkNVpFqcrkZjEVDRWYppm2sERERKR1qCjVWlKDOqsBMBLJQZ3qlBIRERHZvGT+5AqtAUzKqq2iVCwRoyZSk8bAREREpDWoKNVakit9JUYlAPHkoE51SomIiIhsQbLT3BYLk0uISMxOtjMb0MKeiIhIR6SiVGvJ7QxAYdOgzsZkUUoJlYiIiMjmOb3gtTaH2dVXB2izGBERkY5MRanWkmw/90etmQi1IQ+ghEpERERkq5LDznfxWa/r+ewBQAt7IiIiHZGKUq0l2X7ua1wHQHWtG1BCJSIiIrJVyREITZvFuPADWtgTERHpiFSUai3JTilneB0GCcprrKJUTaSGaDyazshERERE2q9kDtXVXgVosxgREZGOTEWp1pJdDIYNIxGj0KghFvVgM+yAVvpEREREtijZbV5sVACQiGmzGBERkY5KRanWYndYhSmaBnXa8DsDgFb6RERERLYoWZQqSFj5UmNys5iycFnaQhIREZHWoaJUa0q2n+/qtWYiZDm0e4yIiIjIViUHnecmN4sJhbyAFvVEREQ6orQWpT7++GOOPfZYSktLMQyDyZMn/+xnnn/+eQYPHozP56OkpITzzjuP8vJ2mqQkV/p6uqoBcBnJQZ1KqkREROQX6NA5VHLQua8huVlMnXYwFhER6ajSWpSqr69n8ODBPPDAA9t0/vTp0zn77LM5//zz+fbbb5k4cSIzZ87kwgsvbOVId1CyU6qzowoAu5kLKKkSERGRX6ZD51DJTilHQzkuolRutIOxaZrpjExERERamCOdNx81ahSjRo3a5vNnzJhBjx49uPLKKwHo2bMnF198MbfeemtrhfjLJFf6imka1JkNqFNKREREfpkOnUP58sHuhngjxUYVy6MBAKKJKLXRWnJduemNT0RERFpMRs2UGj58OMuXL2fKlCmYpsnatWuZNGkSRx11VLpD27zczgDkx63BnJFGH6CilIiIiLStjMqhDCO1sNc/qxZMJ167cigREZGOKKOKUiNGjOD555/n9NNPx+Vy0alTJ/x+/1Zb1xsbG6mpqWn202aSr+/lNA3qDCcHder1PREREWlDmZdDWa/w9fHWAhttFqOilIiISIeSUUWpefPmcdVVV3HDDTcwa9Ys3n77bZYsWcIll1yyxc+MGzcOv9+f+unatWvbBZwcdO4JW4M6a+qTgzqVUImIiEgbyrwcylrY6+60Novx2JKbxWhhT0REpEPJqKLUuHHjGDFiBNdddx2DBg3iiCOO4MEHH+TJJ59k9erVm/3M2LFjqa6uTv0sX7687QJOdkrZo3VkEdbuMSIiIpIWmZpDdbZXAmA3cwAt7ImIiHQ0aR10vr1CoRAOR/OQ7XY7wBZ3Y3G73bjd7laPbfM3zwZ3LjTWUGqrZGHUGsxZ1VhFNBHFaXOmJy4RERHZqWRcDpXsNg+a1mYxZixZlNLCnoiISIeS1k6puro65s6dy9y5cwFYvHgxc+fOZdmyZYC1Qnf22Wenzj/22GN55ZVXeOihh1i0aBHTp0/nyiuvZNiwYZSWlqbjEX5eMqnqn1WLGfdhJL/yinBFOqMSERGRDNbhc6hk/pSX3CwmFtGgcxERkY4orZ1SX3zxBQcffHDq92uvvRaAMWPGMH78eFavXp1KrgDOOeccamtruf/++/nd735HIBDgkEMOaZ/bGTfJKYH139PbUwu1NrKdAWqjFZQ3lFOcVZzu6ERERCQDdfgcKjnoPDtibRYTbsgCn4pSIiIiHU1ai1IjR47cYss4wPjx4zc5dsUVV3DFFVe0YlQtLLnS191ZBYDX5qeWCiVVIiIissM6fA6VHHTuDq8DTGpDHqsopdf3REREOpSMGnSeCcoee4zll15G+OtvrAPJQZ0ltioAHKY1V0pJlYiIiIil4YcfWDX2T6z5+z+sA9mdALAlIuRTu2GzGC3qiYiIdCgqSrWw0GczqfvgAxp/mG8dSK70FZnJJCqu3WNERERENmaGQlS/+iq1771nHXC4IKsIgC72SsxYNmAt6m2tQ0xEREQyi4pSLczVrRsAkaXJOQ65nQEIxKyZCLFoFqBOKREREZEmzu7dAYitXk2iocE6mByBsKuvNrX7XmO8kfpofVpiFBERkZanolQLc3VPFqWahosmX9/LarSKUg0N2j1GREREZGP2QABbjlV4iq5YYR1MDjvv7akB04XLlnyFTwt7IiIiHYaKUi3MmeqUWmodSK7yuRrKcBCjLqSESkRERGRjhmFs1G3elENZC3tdHdUAeO0BQAt7IiIiHYmKUi3MlWw/jy5das088BWCzYmBSRHV1IW8gBIqERERkY2lus2bRiAkO6VKbJXW39FmMSIiIh2NilItzNmlCxgGiVCIeHk52GyQY+0g08VRlRrUWdFQkc4wRURERNqVVLf5suadUoUJqwhlaLMYERGRDkdFqRZmc7lwllhJVGquVPIVvn4bDeqsbKgkloilJUYRERGR9sbVLdlt/pP8yZ/cLCaeXNgrC5e1fXAiIiLSKlSUagXOTdrPrSJVL3c1ZjwLAxsmJpUNlekKUURERKRd2dLre1mN6wBobExuFqPX90RERDoMFaVaQdNcqQ3t51ZS1cVRBdjw2pPt50qqRERERICN5nKuXk0iEkm9vueI1OChkfqQdjAWERHpaFSUagWp9vOfdEp1MpoGdfoBJVUiIiIiTewFBdh8PkgkiK5YCe5ccGYB0MmoIBRObhajRT0REZEOQ0WpVpBqP//JTISC5KBOm6lOKREREZGNGYaBc+Nuc8NIdUt1c1SnNovRop6IiEjHoaJUCzNNk1hBZ+I2J5GlSzFNM1WUyo1aMxESUSVVIiIiIhuLRxPEu/YFNh12vquvhoR2MBYREelwVJRqYa/cPpsJD6+iMm9XErW1xKuqUq/veRvWASaNEc1EEBEREWmy9NtyHr5yGp8bBwAQWZKcy5kcdt7DVYMZtzrNw7EwoWgoLXGKiIhIy1JRqoVlBdwANAR7AxBdujRVlLLHG/FTTzik3WNEREREmuTkecCE2pgXk41HIFg5VGd7FSRcOAwXoIU9ERGRjkJFqRYWKLaGcDbmN81EWAZOD3jzAWtQZ7hRnVIiIiIiTfxFXjAgFjeIOnM2FKWSnVLFVAAGbiO5WYwW9kRERDoEFaVaWKDYKjiFfEEAIk078OV2BpLt502DOpVQiYiIiGB32sgt8ABWDhVduRIzGt1ksxi7mQtoYU9ERKSjUFGqhQWCVlGqDmvuwU/bz/t6NuweUxYua/sARURERNohfzKHCud2hnic6KpVqfwpJ2JtFqMcSkREpGNRUaqFNRWlwlEHcZvL2tIYUnOlNh7UWdVYRTwRT0ucIiIiIu1JUw7VNJczsmxZ6vU9T2MZNhJEI+o2FxER6UhUlGphnmwn7iwHACFvEdGlzbc0LrVVYsayAIOEmaCysTJNkYqIiIi0H01zOcPJnCmydBlkB8GwY5hxCqkmFLbO0et7IiIiHYOKUq0g1S3lCxKvqiJeXZ3qlApSAdhxG8mVPiVVIiIiIqn8KeTIA7C6zW12yC4GrM1iIpEsQJ1SIiIiHYWKUq2gadh5Q2FPINl+nhx0nhe3ZiCkBnUqqRIRERFJ5U91MQ8mBpGlyREIyc6pnu4Nczm1qCciItIxqCjVClIzEfK7Acn2858M6iQ5V0pJlYiIiAhk53uwOQwSpkGDJ2+jEQhWDtXHox2MRUREOhoVpVpB00pfyFsEJNvPk6/vuSJVuIkQi1rt5xUNFekJUkRERKQdsdkM/EVNOVSQyMqVmLFYath5d2c1ibg6pURERDoSFaVagT9oDeGsM61uqOjSZeDNA4cHgKBRSbhBgzpFRERENhZI5lDhnFKIRomuWZPqlCqxVWLGrNwqFAsRjoXTFqeIiIi0DBWlWoG/yEqoInE7UUeWNVPKMFLdUp2oJKYtjUVERESaSY1AKOoFYM2VSnZKFZkVkHBjwwloYU9ERKQjUFGqFbg8DrICbgBCviKrKAWpYee9PDUkNKhTREREpJmmEQjh5EJedNmy1KDzvFgZYOBEm8WIiIh0FCpKtZJAsdUtFfIGiZeXE6+r2zCoc+PdY5RQiYiIiAAb5U+OANC0WYxVlMpqXAeYGNosRkREpMNQUaqVpNrPC3oAyZW+5KpfV0cVZjKhKguXpSU+ERERkfbGn8yfQjE3CcNhdZsn8ydHPEQOYWJRa2FPOZSIiEjmU1GqlTS1nzcEugLJmQjJlb4SW1WqU6qyoZKEmUhPkCIiIiLtiC/XhdNjx8Qg7C2w8ieXDzx+AIqNChobrRxL3eYiIiKZT0WpVtLUKRXyFALJ9vPkSl+hWZ4qSsXNOFWNVWmJUURERKQ9MQxjQw7lLSa6bBlmPJ4adt7JqCQWzQL0+p6IiEhHoKJUK/EntzSuM7MxwWo/Tw4690fXA3YcKKkSERER2VggmUOFcjphRqPE1q5NdZtvPJezoqEibTGKiIhIy1BRqpXkFnoxbAZx00bE5SeybGlq0LmvcT0GCYxEclCn2s9FREREAPAnRyA0JudyWgt7Vg7V012DGdOgcxERkY5CRalWYnfYyC3wABDyBYkuXQbZxYCBzYxRQC2J5KBOJVUiIiIiltTre9lWIcoagWB1SlmbxWgHYxERkY5CRalW1DTsPOQtJrZ+PYnGKGQHAWtQZ6RpUKeKUiIiIiLARvmTPQDQrNu8k1GZen1P+ZOIiEjmU1GqFTWt9IXzkjvwLV+eGnZeYqsgHtNKn4iIiMjGmmZKNSRcxOweoss2dEoVJMpJJPOnumgdjfHGtMUpIiIiv5yKUq0oUJxMqgJdAKxtjZPDzntrJoKIiIjIJtw+J94cJwBhb5H1+l5y0HludB0kvBg4AOVQIiIimU5FqVbUNKgz5CkEmopSyUGdrpoN7efqlBIRERFJSb3C5wsSWbYMM7sTAN5IBU7i2BJa2BMREekIVJRqRU2v79UnskgYtmT7uVWU6uKoIhFXQiUiIiLyU/6mYedZnTAbGojVJ8DuAiDIhrlSZeGytMUoIiIiv5yKUq0oO+DG7rRhYtDgKWjWfl5MhQZ1ioiIiGxGaq5UfncAIsuWQY7VLVVsVBKNZAHqNhcREcl0Kkq1IsNmpJKqkDeYTKisTqn8eFmqKFXRUEHCTKQtThEREZH2pOn1vXDytb2Nh5032yxGC3siIiIZLa1FqY8//phjjz2W0tJSDMNg8uTJP/uZxsZG/vznP9O9e3fcbjc9evTgySefbP1gd1BqBz5fkNiaNSTc1nypnOh6zLiVUMXMGDWNNWmLUURERDJLR8+hUiMQbH5MaNZt3setuZwiIiIdhSOdN6+vr2fw4MGcd955nHTSSdv0mdNOO421a9fyxBNP0KdPH1avXk0i0X67jJqGnYf9XWAFRKvjuAFXrA6fGcNmekkYYcobygl4AmmNVURERDJDR8+h/EVeMCBqOog6s61u892tolQPVzVmzOo8V6eUiIhIZktrUWrUqFGMGjVqm89/++23+eijj1i0aBH5+fkA9OjRo5WiaxmpTil/ZwAia8pxu3IgUksno4KyWA44w5SHy+kd6J3OUEVERCRDdPQcyuGyk5PnobaigZA3SM6yZZBzCACl9irM6C6AOqVEREQyXUbNlHr99dfZa6+9uO222+jcuTO77LILv//97wmHw1v8TGNjIzU1Nc1+2lJqS2N3AdDUfm6t7hUblcSiGtQpIiIirSszc6jkXE5fkOjSpZjJQedBbRYjIiLSYaS1U2p7LVq0iP/+9794PB5effVVysrKuPTSSykvL+epp57a7GfGjRvHzTff3MaRbtA06Dxs+ojbnESWLoVeJVD2A6W2SubEsrGjpEpERERaTybmUP6gj+XfVRL2FZNYEyIey8ZB881itKgnIiKS2TKqUyqRSGAYBs8//zzDhg3jqKOO4l//+hdPP/30Flf6xo4dS3V1depn+fLlbRqzJ9uJ22fV/sLeIiLLlkKu9SpfH3e1kioRERFpdZmYQ6VGIOR3AyBSHQcgO7KeRNzqNK+N1BKJR9o0LhEREWk5GVWUKikpoXPnzvj9/tSx/v37Y5omK1as2Oxn3G43ubm5zX7akmEY+JNJVcgbJLrR63vdnNVqPxcREZFWl4k5VNMIhLCvGIBIWQgAeyJCIB4H00pjKxoq2jQuERERaTkZVZQaMWIEq1atoq6uLnXshx9+wGaz0aVLlzRGtnXNZiKsXk3CUwRAqa0SM54DQFm4LG3xiYiISMeWiTlUU/5Ub/NjYhBZsRJ8hQCUGFWgHEpERCTjpbUoVVdXx9y5c5k7dy4AixcvZu7cuSxbtgyw2sbPPvvs1PmjR4+moKCAc889l3nz5vHxxx9z3XXXcd555+H1etPxCNsk1X6eUwqmSTTsAaCIChJ6fU9ERES2086QQ+Xke7DZDRLYaHDnEV228WYxFcSj6jYXERHJdGktSn3xxRcMGTKEIUOGAHDttdcyZMgQbrjhBgBWr16dSq4AsrOzmTp1KlVVVey1116cddZZHHvssdx7771piX9bNbWfN/itWVKRKmsmQiBWhhmzVvmUUImIiMi22hlyKJvdhr8ouWGMr8jawTinFIDOtkrMuBb2REREMl1ad98bOXIkpmlu8e/jx4/f5Fi/fv2YOnVqK0bV8po6pepdBQBEyxsA8EXKscWsZKuioQLTNDEMIz1BioiISMbYWXIof9BH5ZoQIW+QyLK5mDnDMYDe7hrN5RQREekAMmqmVKbyB63CUwQ3UYeXyNoKsDmwkaAgbnVNRRNRaiI16QxTREREpF0JBJvmchaTqK0lbrcW+Lo5qzQCQUREpANQUaoNuDwOfH4XAGFv0Go/z+4EQAm1kLBmTCmpEhEREdkgNQIh0BWASK0TgBJb1YbX99QpJSIikrFUlGojTa/whXxBIj8Z1JnQoE4RERGRTWycPwFEa6xXFgsT5RvmcmpRT0REJGOpKNVGmlb6Qt4g0ZUrMb3FAHSxV5HQoE4RERGRTaTyJ1sOCcNOJDmX0x8r00wpERGRDkBFqTaSWunLLoFEgmg8D4Be7molVSIiIiKb4fO7cLjtgEHYW0hkXTUAnlg1zrgb0KKeiIhIJlNRqo0Eiq1BnQ251lbGkZCVSHV1VKkoJSIiIrIZhmGkhp2HvUVEVq4Gp7XQVxSPAlDdWE00+Z9FREQks6go1Uaa2s/rXfmYQMRa6KOTUYkZ10wEERERkc3ZMAKhmOjSZZBjzeUsiTeAaaWyyqFEREQyk4pSbSS30IthQBwnEVcukYpGAAoS5eqUEhEREdmCjYedx6uriTutuZydjEoSsSxARSkREZFMpaJUG7E7bOQUeABr2HnTTITc6HoVpURERES2IPX6nr8zAJGIH4AujirMuHIoERGRTKaiVBtKtZ/7gkRXrQPAmWjAG7MDWuUTERER+Sn/RjsYA0RC1iJfL1c1Ziw5AkFFKRERkYykolQbamo/D3uLiaxchem2duArSsQBK6EyTTNt8YmIiIi0N035U6Mti5jdTaTGAJKdUk3d5lrYExERyUgqSrWhVKdUdjHEYkSxZiKUxhsAiCQi1EXr0hafiIiISHvjyXLiyXYC1g580YoIAMVU6PU9ERGRDKeiVBtKdUrllAIQiQQAKKEWM+4ClFSJiIiI/FRq2Lk3SGR9DQD5iXIS6pQSERHJaCpKtSF/sTWoM+TMw8QgErZ+72avwownZyIoqRIRERFpJtCUQ/mCRFaXAZATLYfk7nsV4Yq0xSYiIiI7TkWpNpST58HusGFio8GTT7TW+vp7uKq1A5+IiIjIFvhTczmDxCuriEft2MwYOXFrvpQW9URERDKTilJtyLAZ+INNK33FRCqjAHS2V6r9XERERGQLUiMQcpMjEBLWTnxF8Q2bxYiIiEjmUVGqjTWfiVALQJCKDa/vKakSERERaaZps5h6bxATiDbN5Yw3AlDVWEU0EU1TdCIiIrKjVJRqYxvPRIiuKcdMQF68PPX6Xlm4LJ3hiYiIiLQ7TZ3mMcNN1JlFJGwVqTonQpimgYlJZUNlOkMUERGRHaCiVBtrmokQ8hVjRmPEwnayYpXYYlaypdf3RERERJpzuuxk57kBa65UpNYOQHdHNWbcGnaubnMREZHMo6JUG2tqP2/IKQEgEvIA4LdGImj3GBEREZHNaMqhQt5gai5nD1fVhs1itLAnIiKScVSUamOpQZ32XOI2B5FoPgCFTYM6lVCJiIiIbCI1l9MXJFJWD0CprRIzprmcIiIimUpFqTbmzXHi8tjBMAh7ioiErZbzTvEIYCVUpmmmM0QRERGRdie1g7E3SLyqjkTUoMiswIyrU0pERCRTqSjVxgzDSLWfh31BInXWTITO8RAADfEGQrFQ2uITERERaY9S+VPTCIQ6O4FY2YbX99QpJSIiknFUlEqD1LBzb5BolfXaXl9HLWbCCSipEhEREfmp1Ot7niJMDCJ1DtyJEM6YNQBdnVIiIiKZR0WpNEgN6vQFiZSHME3o6qzeMBNBSZWIiIhIMzmFHmw2g4ThoNHtJxK2OqRyk5vFaFFPREQk86golQaB4uRMBF8xZiRGLGyjxKhU+7mIiIjIFtjtNnKLmuZKFRNpsOZyarMYERGRzKWiVBqkduDLKgYgUuug0CwnEVdRSkREpKOLRCLMnz+fWCyW7lAyTiA57DzsKyJa5wCgUzwKKH8SERHJRCpKpUFTUSriyCZm9xCpc+CPlUOyU6qsoSyd4YmIiEgrCIVCnH/++fh8PgYMGMCyZcsAuOKKK/jnP/+Z5ugyg794w1zOSFUC2LBZTGVDJbGECn0iIiKZREWpNHB5HXhzXYA1Vypa58BhRnHHNOhcRESkoxo7dixffvkl06ZNw+PxpI4feuih/Pvf/05jZJkjNezcFyRW00giZrCrPYRpGpiYVDVWpTdAERER2S4qSqVJU/t5yBskErYSLH/cAFSUEhER6YgmT57M/fffz/77749hGKnjAwYM4Mcff0xjZJkj9fpeVgkAkTo73e3VmHErl1IOJSIikllUlEqTZjvw1VsdUvlxqw1dgzpFREQ6nvXr1xMMBjc5Xl9f36xIJVvWlD+F3fkkDBvROgedbBXaLEZERCRDqSiVJqlh594gkSoT04TieARQQiUiItIR7bXXXrz11lup35sKUY8//jjDhw9PV1gZJcvvxuGyYRo2GjyFROrsFCTKMeM5gBb2REREMo1jRz60fPlyDMOgS5cuAMycOZMJEyaw2267cdFFF7VogB3Vhk6pYsxogniDjc6+MABlKkqJiIh0OP/4xz8YNWoU8+bNIxaLcc899zBv3jw+/fRTPvroo3SHlxEMm4E/6KN8RZ3VbV67mPxYJcT0+p6IiEgm2qFOqdGjR/Phhx8CsGbNGg477DBmzpzJn//8Z2655ZYWDbCjSnVK+YKYQKTWQT/D2j2mIR4mFA2lMToRERFpafvvvz9ffvklsViMgQMH8u677xIMBpkxYwZDhw5Nd3gZIzXs3FtEpM6JgYm3abMYdUqJiIhklB0qSn3zzTcMGzYMgJdeeondd9+dTz/9lOeff57x48e3ZHwdlr/ICwbE7B6izhwidXZ62aoxE1bzmpIqERGRjiMajXLeeedhGAaPPfYYM2fOZN68eTz33HMMHDgw3eFllEBxcti5t5hIvbWbca42ixEREclIO1SUikajuN1uAN577z2OO+44APr168fq1atbLroOzO60kZNvbQcd8gWJ1DnoZFRpUKeIiEgH5HQ6efnll9MdRoeQ6pTyBYnVQSKuzWJEREQy1Q4VpQYMGMDDDz/MJ598wtSpUznyyCMBWLVqFQUFBS0aYEeWmivlDRKpdVAQL9OgThERkQ7qhBNOYPLkyekOI+Ol8qesYgCidQ6K41FAi3oiIiKZZocGnd96662ceOKJ3H777YwZM4bBgwcD8Prrr6de65OfFwj6WD6vgpAvSLTSjjdRiaFBnSIiIh1S3759ueWWW5g+fTpDhw4lKyur2d+vvPLKNEWWWZo6pRpdAeI2F5E6O53zGwBYFypLZ2giIiKynXaoKDVy5EjKysqoqakhLy8vdfyiiy7C5/O1WHAdXdNMhJA3SGS5E9MEb9xJIypKiYiIdDRPPPEEgUCAWbNmMWvWrGZ/MwxDRalt5Ml24s5y0FgfSw47X88uRj0AVY2VxBNx7DZ7mqMUERGRbbFDRalwOIxpmqmC1NKlS3n11Vfp378/RxxxRIsG2JFt2IGvmETUIN5oIzdmsB69viciItLRLF68ON0hdBiBoI+1i2sI+4JEa39gF2oBMElQ1VhFgVfjJERERDLBDs2UOv7443nmmWcAqKqqYp999uHOO+/khBNO4KGHHtrm63z88ccce+yxlJaWYhjGds1ZmD59Og6Hgz322GM7o28/mmYihL1FmBhE6uzkNQ3qVKeUiIhIh2WaJqZp7vDnlUNtNJezzk4plSSaRiBoYU9ERCRj7FBRavbs2RxwwAEATJo0ieLiYpYuXcozzzzDvffeu83Xqa+vZ/DgwTzwwAPbdf+qqirOPvtsfvWrX23X59qb7HwPNodBwuagwZNHpNZBMDmosyysmQgiIiIdzTPPPMPAgQPxer14vV4GDRrEs88+u93X2dlzqI134IvUOsiLl2HGtYOxiIhIptmh1/dCoRA5OdYuce+++y4nnXQSNpuNfffdl6VLl27zdUaNGsWoUaO2+/6XXHIJo0ePxm63Z/QuNjabgb/QS+WaECFvkGjdUjonGgFYH1JCJSIi0pH861//4i9/+QuXX345I0aMAOC///0vl1xyCWVlZVxzzTXbfK2dPYfyBzfM5YyG7DhiEewxL7jVKSUiIpJJdqhTqk+fPkyePJnly5fzzjvvcPjhhwOwbt06cnNzWzTAn3rqqadYtGgRN954Y6vep62kXuHzFROptdOXEKCESkREpKO57777eOihh7j11ls57rjjOO6447jtttt48MEHt6vTfEd1pBxq4/wJ0yBSb8cbdwLqlBIREckkO9QpdcMNNzB69GiuueYaDjnkEIYPHw5YXVNDhgxp0QA3tmDBAq6//no++eQTHI5tC72xsZHGxsbU7zU1Na0V3g5JtZ97g0QqHOyCFV9DPERDrAGPw5PO8ERERKSFrF69mv3222+T4/vttx+rV69u1Xt3tBzKX2R1SkWdWUQdWUTrHOTEbITRwp6IiEgm2aFOqVNOOYVly5bxxRdf8M4776SO/+pXv+Kuu+5qseA2Fo/HGT16NDfffDO77LLLNn9u3Lhx+P3+1E/Xrl1bJb4dlRrU6QsSqXPQPVGJmbC2MVZSJSIi0nH06dOHl156aZPj//73v+nbt2+r3bcj5lAuj4OsgBuAkK+ISJ2DQNwaHK9OKRERkcyxQ51SAJ06daJTp06sWLECgC5dujBs2LAWC+ynamtr+eKLL5gzZw6XX345AIlEAtM0cTgcvPvuuxxyyCGbfG7s2LFce+21qd9ramraVVIVKN4wEyERsZEdqsKMl2LYqigPl9M5u3OaIxQREZGWcPPNN3P66afz8ccfp2ZKTZ8+nffff3+zxaqW0pFzqPqqRqvbvNZOMB7lB6BMRSkREZGMsUNFqUQiwd/+9jfuvPNO6urqAMjJyeF3v/sdf/7zn7HZdqgBa6tyc3P5+uuvmx178MEH+eCDD5g0aRI9e/bc7Ofcbjdut7vF42kp/uTrew2efBKGg3idgT3mxXRWaQc+ERGRDuTkk0/ms88+46677koNGe/fvz8zZ85s1fEHHTWHCgR9rJxfRdhrdZt3Nq1XDdfWr09zZCIiIrKtdqgo9ec//5knnniCf/7zn812j7nppptoaGjg73//+zZdp66ujoULF6Z+X7x4MXPnziU/P59u3boxduxYVq5cyTPPPIPNZmP33Xdv9vlgMIjH49nkeCbx5bpweuxEGyDsLSBSux5v3EEIvb4nIiLS0QwdOpTnnnvuF19HOVTzEQjRcgd9CAN6fU9ERCST7FBR6umnn+bxxx/nuOOOSx0bNGgQnTt35tJLL93motQXX3zBwQcfnPq9qUV8zJgxjB8/ntWrV7Ns2bIdCTFjGIZBIOhj/bJaQt5iInULyI7ZraKUkioREZEOY8qUKdjtdo444ohmx9955x0SiQSjRo3a5msph9rQbR7yBonU2ekdrwagJlJJwkxgM1q+c19ERERa1g79v3VFRQX9+vXb5Hi/fv2oqKjY5uuMHDkS0zQ3+Rk/fjwA48ePZ9q0aVv8/E033cTcuXO3M/r2JxBMzpXyBYnUalCniIhIR3T99dcTj8c3OW6aJtdff/12XUs5VPP8yTQNelRVAZAgQXVjdRojExERkW21Q0WpwYMHc//9929y/P7772fQoEG/OKidjb94w0pftM5BUTwGwHrNlBIREekwFixYwG677bbJ8X79+jV7FU+2TW6hF8NmkLC7ibj8eGsaMONWoUoLeyIiIplhh17fu+222zj66KN57733GD58OAAzZsxg+fLlTJkypUUD3BkEku3nYV+QyFI7nRPJQZ11KkqJiIh0FH6/n0WLFtGjR49mxxcuXEhWVlZ6gspgdoeN3AIP1evDVrd53VLsMS8Je5jyhnL60CfdIYqIiMjP2KFOqYMOOogffviBE088kaqqKqqqqjjppJP49ttvefbZZ1s6xg4vsFGnVLzRTp/GekBbGouIiHQkxx9/PFdffTU//vhj6tjChQv53e9+12xOp2y7DTlUMdFaB564E1CnlIiISKbYoU4pgNLS0k0Gmn/55Zc88cQTPProo784sJ1J00yEiNtPzO6md00t4KAqsu3zuURERKR9u+222zjyyCPp168fXbp0AWD58uUceOCB3HHHHWmOLjMFgj6WUk7IV0SkzrFhsxjtYCwiIpIRdrgoJS3H7XPizXESro0S9gYpqfwOCNAQr6cx3ojb7k53iCIiIvIL+f1+Pv30U6ZOncqXX36J1+tl8ODBHHDAAekOLWMFiq2FvbA3SGS9HX/cZB3qlBIREckU2iu3nWiaKxXyBfHUJMC0/qupCKtbSkREJJPNmDGDN998EwDDMDj88MMJBoPccccdnHzyyVx00UU0NjamOcrM5N8of4rWOQhGrc1i1oU0l1NERCQTqCjVTjTbga/WgS1m/V6mHfhEREQy2i233MK3336b+v3rr7/mwgsv5LDDDuP666/njTfeYNy4cWmMMHM1zZQKe4qIm3Z614YBWF27Lp1hiYiIyDbartf3TjrppK3+vaqq6pfEslNrmisV8lnt5+64i7BTMxFEREQy3dy5c/nrX/+a+v3FF19k2LBhPPbYYwB07dqVG2+8kZtuuilNEWau7IAbu9NGPAoNngL61tQAsF6v74mIiGSE7SpK+f3+n/372Wef/YsC2lltvANf06DOMJqJICIikukqKyspLi5O/f7RRx8xatSo1O977703y5cvT0doGc+wGQSCXspX1hPyBulctRZwUdmo8QciIiKZYLuKUk899VRrxbHTa5opFfYFiTXYKQonWJ+tTikREZFMV1xczOLFi+natSuRSITZs2dz8803p/5eW1uL0+lMY4SZLRD0Ub6ynrAvSEl1A+CiLlqFaZoYhpHu8ERERGQrNFOqnfAXecGAmMNH1JlN78oIAGvr16c5MhEREfkljjrqKK6//no++eQTxo4di8/na7bj3ldffUXv3r3TGGFm2zCXswhnjVWEShCjJlKTzrBERERkG6go1U44XHZy8jyA9QpfzyprF55VtSpKiYiIZLK//vWvOBwODjroIB577DEee+wxXC5X6u9PPvkkhx9+eBojzGypHYy9QeJ1Doy49d1qBIKIiEj7t12v70nr8ge91FY0EPIF6VqzFDC0pbGIiEiGKyws5OOPP6a6uprs7Gzsdnuzv0+cOJHs7Ow0RZf5NmwWU0ykzoEn5iRsj1DeUE4veqU5OhEREdkadUq1I6ltjb1BCqut1/cqGzSoU0REpCPw+/2bFKQA8vPzm3VOyfZpyp8aPfnETBelVVZ6q04pERGR9k9FqXYk1X7uC5JdFQegJlqZzpBERERE2jVPthO3z2r+D3uL6FFh5VDaLEZERKT9U1GqHQkUb5iJYNRaK6mNiTqi8Wg6wxIRERFptwzDwL/RXKkelTEAVtdpLqeIiEh7p6JUOxIotmYihL1FxMMO3Nasc630iYiIiGxFUw4V8gXpUmUt5q2sWZfOkERERGQbqCjVjuTke7DZDRJ2F43uAJ3LnYBmIoiIiIhsTdMIhLA3SLDaKkqtrVenlIiISHunolQ7YrPb8BdtWOnrVmEA6pQSERER2ZrUCARfEH91AoAK5U8iIiLtnopS7czGMxG6VVhJVVmoLJ0hiYiIiLRrgY3yJ2etAaZJdUSbxYiIiLR3Kkq1M4Hghk6pzpXW7jGrNKhTREREZIv8yfwp6sohho9APYTiVZimmebIREREZGtUlGpnmtrPw95iiqutopQGdYqIiIhsmcvjwOd3AdZcqU6VkCBGbbQ2zZGJiIjI1qgo1c6k2s99QfKTMxHW1Ov1PREREZGt2TiH6lyWnMupzWJERETaNRWl2plUp5SnAFe9HWfUVEIlIiIi8jNSw869QbpUqCglIiKSCVSUamd8fhcOlw0MG2FPAcFqqIpUpDssERERkXZt42HnnSutWVIqSomIiLRvKkq1M4ZhbOiW8gXpVGlSH6tKb1AiIiIi7Vyg2Bp2HvYFKa6yRiCsqNVcThERkfZMRal2aMNKXzGdKiFi1hJNRNMclYiIiEj75d+oUyq/KgGmyfJqFaVERETaMxWl2qHUTARfkJLkm3sVYb3CJyIiIrIl/kIvhgFxhweMXHJDsLpORSkREZH2TEWpdigQtNrPQ94gpclaVHmDZiKIiIiIbIndaSMnzwlYOVSnSijTTCkREZF2TUWpdsi/UadUpyprUGdZuCydIYmIiIi0e4FOOUAyh6o0qWpUp7mIiEh7pqJUO9Q0UyriDpBb58Ie10wEERERkZ+T2izGW0xJhUldtDLNEYmIiMjWqCjVDnmynHi8VodUo7eIYBUsU1FKREREZKs2zOUsolMlNJjVmKaZ5qhERERkS1SUaqcChS6gaSaCyWptaSwiIiKyVYGNduArrjRJEKU+Wp/mqERERGRLVJRqp5rPRIB1IQ3qFBEREdkaf3KzmLC3iE6VBpimNosRERFpx1SUaqf8JX4AwslOqUoN6hQRERHZqux8D3ZbAtPmwG7kkx2G9SFtFiMiItJeqSjVTgWKs4ANnVK1GtQpIiIislU2m4E/kAAg5CumUyUsrVqb5qhERERkS1SUaqdSgzq9xRRXmoQTVekNSERERCQD/HQup4pSIiIi7ZeKUu1U00yEmNNHXn0WZqyWWCKW5qhERERE2rdAp2xgQ7f5qjptFiMiItJeqSjVTjlddrK9DQBEPEEKa0wqG/QKn4iIiMjW+LsUAdaw85JKk7X1miklIiLSXqko1Y4FmmYiJNvP12lQp4iIiMhWBUqtzWJC3iDFlSYV2n1PRESk3VJRqh3zFzqBDe3nSyrXpDkiERERkfYtELTmcjZ48glWOaiJqNNcRESkvUprUerjjz/m2GOPpbS0FMMwmDx58lbPf+WVVzjssMMoKioiNzeX4cOH884777RNsGkQ6JQDbOiUWqJBnSIiIoJyqK3x5jhx2RvBsOGgCKO+It0hiYiIyBaktShVX1/P4MGDeeCBB7bp/I8//pjDDjuMKVOmMGvWLA4++GCOPfZY5syZ08qRpkegSyEA4WSn1IoaDeoUERER5VBbYxgGgewQYOVQ+ZXVaY5IREREtsSRzpuPGjWKUaNGbfP5d999d7Pf//GPf/Daa6/xxhtvMGTIkBaOLv0C3UqAFYS8Qfosgg80qFNERERQDvVz/P4466qTc6WqviYUDeFz+tIdloiIiPxEWotSv1QikaC2tpb8/PwtntPY2EhjY2Pq95qamrYIrUXkBLMwiJOwuwiE/VRq0LmIiIi0gI6eQwUKHbBsw1zONfXr6RXonu6wRERE5CcyetD5HXfcQV1dHaeddtoWzxk3bhx+vz/107Vr1zaM8Jex22343dZwzog7iG29Bp2LiIjIL9fRc6ifzuVcVKEcSkREpD3K2KLUhAkTuPnmm3nppZcIBoNbPG/s2LFUV1enfpYvX96GUf5yG89EyC3ToE4RERH5ZXaGHKrZXM4KbRYjIiLSXmXk63svvvgiF1xwARMnTuTQQw/d6rlutxu3291GkbU8vz8O5dZKX0Hl7HSHIyIiIhlsZ8mhAj26AJVEXLkU1XiYW62ilIiISHuUcZ1SL7zwAueeey4vvPACRx99dLrDaXWBAqtuGPIFCVY2EE/E0xyRiIiIZKKdKYdy5RXiNaoAcBKkomxlegMSERGRzUprp1RdXR0LFy5M/b548WLmzp1Lfn4+3bp1Y+zYsaxcuZJnnnkGsNrNx4wZwz333MM+++zDmjXWfACv14vf70/LM7S2QKdsYMNMhHX1FZTkFKU5KhEREUkn5VA/wzAIeMoJhwOEvEHMVSvSHZGIiIhsRlo7pb744guGDBmS2or42muvZciQIdxwww0ArF69mmXLlqXOf/TRR4nFYlx22WWUlJSkfq666qq0xN8WAl0KAGjwFlJcabCwYnWaIxIREZF0Uw718wLZ9YDVbe5Zuy7N0YiIiMjmpLVTauTIkZimucW/jx8/vtnv06ZNa92A2qGsks44WEDM8OBvKGR+2SoO6D4o3WGJiIhIGimH+nkBfxzWQ9gbxF8+M93hiIiIyGZk3EypnY0RKMXvsLqjoq4iypf+mOaIRERERNq/QKEdSG4WU1Gf5mhERERkc1SUau9cWQRc6wGr/bxx2eI0ByQiIiLS/gWKk3M5fUGCVY1pjkZEREQ2R0WpDBDIslb3wt5i7KtWpTkaERERkfYvt7QASBB3eCmszaIqXJfukEREROQnVJTKAAF/FLBW+nzry9IcjYiIiEj758grJcdmdZu7zSALVixJb0AiIiKyCRWlMkCgwAlYMxEC5TVpjkZEREQkA+SWEnBYHeYhb5BV33+d5oBERETkp1SUygCBTlkANHryKKiMpTkaERERkQyQ04lAcrOYkC9I9Y8L0hyQiIiI/JSKUhnAU1iEw6gFIDcS2OoW0CIiIiIC2J0EfFaHecgbJL58WZoDEhERkZ9SUSoT5JaSm2w/j7mC1KzQsHMRERGRn9M0lzPsC+JauybN0YiIiMhPqSiVCXJKKLBvmImw9Jsv0xyQiIiISPsXyHcAEPYWkb2+Kr3BiIiIyCZUlMoEuZ3JbxrU6Quy/od5aQ5IREREpP3LDuYAURI2Jzl1jnSHIyIiIj+holQm8BXgd64DrE6phiWL0xyQiIiISPtn85fgclg5lCuRR6KhIc0RiYiIyMZUlMoENhuB3A0zEYxVK9MckIiIiEgGyCnF57S6zcO+YqLLl6c5IBEREdmYilIZwl9gByDqzMZZVpfmaEREREQyQG4JeY4NczlrflS3uYiISHuiolSGcAUKMW0VALjDHkzTTHNEIiIiIu1cTilBm9VhHvIFKZv/Q5oDEhERkY2pKJUpcjtjd1tbGcecBcTLy9MckIiIiEg7l1tKiT1ZlPIWUf3jgjQHJCIiIhtTUSpT5Jbgca4GIOwNElm2LM0BiYiIiLRznlwC3moAGjwFRFesSHNAIiIisjEVpTJFTgm5DqsoFfIFiSxdmuaARERERNo/n99HwmgAw0aiMprucERERGQjKkplitxSCptmIniD1C1akt54RERERDKA4S8h6lpr/RL1kohE0huQiIiIpKgolSlySijdaFBn7cJFaQ5IREREJAPklGIm53KGvUG9wiciItKOqCiVKXJK6GKsxiROwu6mZuX6dEckIiIi0v7lluJyJUcgeDUCQUREpD1RUSpTOD14s/yEHdaue5HqOKZppjkoERERkXYut5Ss5FzOsC9IVJvFiIiItBsqSmWSnFJC3nUANNr8xKuq0huPiIiISHuXU0KefRUAIW8RjUtVlBIREWkvVJTKJLklxDzWTISQL0hU7eciIiIiW5dbQnFyLmfEHaB2sWZKiYiItBcqSmWSnBIczg2DOhsXLU5zQCIiIiLtXE4pxdQSsdUCULW6DjORSHNQIiIiAuBIdwCyHXI743V8BFidUqv/9CcqnnoK715D8e05FN/QPXGWlqY5SBEREZF2JDtIYQKqPesoCuUQChn8sO9wfEOG4B1q5U+e3XfH5nanO1IREZGdjopSmSS3BL9jJTGsmQgJw0bjggU0LlhA1QsvAuAoKcG355749hqKd8+huPv2wbCpIU5ERER2UjY7uVnFVETWURTqTV1OKYl1s6j76CPqPrIW+wynE8/AgfiGDsU7dE98Q4Zg9/vTHLiIiEjHp6JUJskppcAoY6URwYGL4snv4lo2j/AXswjNnk3DvHnEVq+m5q23qHnrLQBsubl4h+yBb+heWgkUERGRnZKRU0I0tBaA+fseyq9uHUN49ixCyRwqXlZGePZswrNnw2PWZ9x9+6obXUREpJWpKJVJcksoTMSZ5y2jIFTKmlo7ux92GLmHHQZAIhQi/NVXhGbNIjxrFqG5X5KoqaH+o4+p/+hjQCuBIiIishPKLcGosIpS8doE3t0H4N19APlnn41pmkSXLSM0azahWV8QnjWbyJIl6kYXERFpAypKZZKcEgricao86ygIlbJw7np2GxLEZrcSIpvPR9a++5K1774AmLEYDd/P37aVwKF7prqptBIoIiIiHUpOKS7n/wBw19upWhsiUOwDwDAMXN274+rencBJJwIQKy8nNHu2utFFRERamWGappnuINpSTU0Nfr+f6upqcnNz0x3O9jFNVv2zM9fajueAxacCkF+axf6n9qVr//xt+PjmVwJ/auOVwNxjjsGek9PSTyIiIpIxMjp3aEEZ/T38927+b/YD5Cy8lexIAJvNYODBXdj76B64fc6f/fjmutHNUKjZORu60fck++CD8e25Z2s9jYiISLu3rXmDilIZpvHePdg7N8Fua0dwwKrToNH6r6/HoEJGnNwnteq3rTa3Ekg8nvq7e5dd6P78cypMiYjITivTc4eWktHfw1cvcdeHv+dldx/2//F8utaWAODJdrLPcb3YbURJqvN8W2ypG31jJX//G4GTT27RxxAREckUKkptQUYnVABPHc1wllBns7Ev/+TX7r58+/FKzISJzZ5c9Ttq21b9NmfjlcDKF14kXlaGb5996PrYo9hcrhZ+GBERkfYv43OHFpLR38PiT3jm1TO5vSCPWM0gHh38T5Z/sJLKNVa3U35pFvuf1peu/X6+83xzNu5Gr/3gfereex/sdro+9CDZBx7Ykk8iIiKSEbY1b9B0xkyTa82VAvhg6SKeCFVxzHV70m1AAYm4yZfvLee5G/7HNx+vJBFPbPflm+ZSFV12Gd0eexRbVhahzz5j9fVjMRPbfz0RERGRtMstTeVP2Ou4aOq3+E/oxgGn98Xtc1Cxqp7X757LWw9+RdXa0NavtRlNc6kCJ51Il/vuw3/88RCPs+Kqqwl//U0LP4yIiEjHoaJUptkoqXK7Q3z8w3rOnjSb/qf34pjLB5PXyUdDXZSPJsznpX98zvLvK3b4Vp7+/el87z3gcFAzZQrr7rizpZ5CREREpO3kbFjUy/HVE47GuezFOUyNhhh9874MOrgLhs1gyVdlvHDLZ0yftIDGcGyHbmUYBiV/vYWs/fbDDIdZfsklRJYta8mnERER6TBUlMo0OaUUJDugzj+okM4BL0vKQ5zwwKcssMU4/S/DUqt+5SutVb8pD31F1brtX/UDyB4xgtK//w2AiiefpOKZZ1rsUURERETahMtHoSPL+o+uei48oCcA932wkCtf+Yo9ju/JGX8ZRrcB+STiJnPfW87zN8ywOs8T2z/pwnC56Hzvvbh360+8vJxlF15IrGLHFwpFREQ6KhWlMk1uCUUxa6XvrWXPc+mxlQzrGaCuMcaFz37Bwx8vYuDILvz6luEMTK76Lf6yjBdu/ozpLy/coVU///HHU3TttQCsHfdPat5+u0UfSURERKS1FfqKAaiO1lGV/RQ3nNAJl8PGe9+t5aQHP6XGCcdesUeq8zxcm+w8//vnrNiBznN7dhZdH34YZ2kp0aXLWH7Jb0mEdmyRUEREpKPSoPNMs/xz5j8ziqtLOrHCbgCwW/4ACiKnMOVzLwBHDyzh9lMH4XNZMxKmT1rAsnlWMuXNsXaZ6T+iFJvN2ObbmqbJ2r/+jcoJEzCcTro+8ThZw4a1/POJiIi0MxmfO7SQjP8enj2J2ytn8aw/FxNw290c2eV03p2xG+uqIdfj4L7Re3LQLkXE4wm++Wgln7+5mMaQtaDXc3AhI07pg79o+3Y6bly0iKVnjiZeXU32yJF0uf8+DIejFR5QRESk/dDue1uQ8QlV9Qq4awARm4PnT7ydR75+lPpoPQD9cw5gzpcjiDYG6Ncph8fO3ouu+T5M02TpN+VMn7QwNbyzoEs2B5zal8675m3zrc14nJVXX03t1Pew5ebS4/nncPft2yqPKSIi0l5kfO7QQjL+e3jtMpjzHN/vdym3mWv5fM3nABR4CnFUH83CRbtiM2yMHdWfCw7oiWEYNNRFmfnGIr75ZJW107HDYPDBXdnrqB64vNteWArNnsOyc8/FbGwkcOqpdLrlZgxj2xcHRUREMo2KUluQ8QlVPAp/C4KZgN/9QJnDzgNzH+DlH17GxMRhuDCrDqRq9QHkebN58KyhDO9dYH00nuCbaSv5/K0Nq3699ihiv5N7b/OqX6KhgWXnnkd4zhwcnTrR48UXcHbq1GqPKyIikm4Znzu0kIz/Hj78B3x0Kww9F/OYu/hg+Qfc+cWdLK9dDkCurSdrFh1BPNyDE4d0ZtxJA/E47QCUr6pj+qSFLP8Fnee1773HiiuvgkSCwiuvoOjSS1vnOUVERNoBFaW2IOMTKoA7doW6NXDRNCgdAsD8ivnc9vltzFwzEwB7Ipe6NYdj1g7lxmN35zf7dk+tyIXrIsx8YzHffrwS08Ra9TukK3uN2rZVv1hlJUtHn0Vk8WLcu+xC9+efw56T02qPKyIikk4dIndoARn/PXzxFLx5NexyJIz+NwCReIQJ303gka8eoS5aB0CsZhAN645kYHFPHvnNUEr81niElug8r5gwgbW3/BWAkr//jcDJJ7fwQ4qIiLQP25o3pHXQ+ccff8yxxx5LaWkphmEwefLkn/3MtGnT2HPPPXG73fTp04fx48e3epztTm6J9c+a1alDu+bvyuOHP87dB99N15yuxG01eEsn4e52Pze/+yZjX/maxuSAdG+2i4PO3JXT/28YXfvnkYiZzHl3Gc/dMIN5/131s7vMOPLy6PrYY9iLCmn84QdWXH4FiUik1R5XREREmlMOtQNyS61/1qxKHXLZXZyz+zm8eeKbnLLLKdgMG47cr8ju/S++b3yJY+5/n1lLre4owzDoMbCQM/4yjBGn9MHldVC+oo7Jd83hP498TfX68M+GkD96NAUXXQTA6htupO7jj1v+OUVERDJIWotS9fX1DB48mAceeGCbzl+8eDFHH300Bx98MHPnzuXqq6/mggsu4J133mnlSNuZnKakamWzw4Zh8Ktuv2Ly8ZP53dDfke3Mxu5dia/HI0xe9U9Offwt1tU2pM4v6JzNsVfuwVGXDsIf9BKujfLhc98zcdznrPyhcqshuLp0ptujj2LLyiL02Wesvn4sZiLR4o8qIiIim1IOtQNymhb1VsFPcpYCbwE3Dr+Rl455iWGdhoERw134IQ2d/sFZL97HizOXpM61O2zscWg3fv3Xfdn9wM4YBiyas54JN/+PGa8uJPIzOx0XXXM1/uOPh3icFVddTfjrb1r6SUVERDJGu3l9zzAMXn31VU444YQtnvPHP/6Rt956i2++2fB/3meccQZVVVW8/fbb23SfjG89B5hyHcx8FGwO6Hs4DD7TakV3uJqdVh4u54G5DzDph5cxSWAmHLjqD+ahY37PPj1Km50bjyX4etoKPn9rSSqZ6j2kiBGn9iUn37PFUOqmT2f5xZdALEb+eedR/IfrWv55RURE0qi95w7KobZRqAJu62n950A3K38afAbk92p2mmmafLj8Q27//A5W1FnzpuLhzhxcdAF3n3ASTnvzNd3ylXX8d+ICVnxvLeh5c10MP6EX/YaXbHGYuRmJsPyS31L/6afYCwro8cIEXN26tfADi4iIpE9GvL63vWbMmMGhhx7a7NgRRxzBjBkztviZxsZGampqmv1kvL3Og9I9IRGD+VPgpd/AnbtaxaqVsyFZZyzwFnDD8BuYeOxLDCwYimGLEc2Zyvnvn8Jf3n+ShLlhlTC16nfLhlW/H+esZ+K4z1mzqHqLoWSPGEHp3/8GQMWTT1LxzDOt++wiIiKy3ZRDAb58OOD34M6FqmXW0PN7h8CTR8Ksp6HByncMw+CQbofw2gmTuXbPa3EaPuzelXxcdzMjnz6Pb9ctbnbZgs7ZHHfVRp3nNRE+eOZ7pj33PfHY5rvIDZeLzvfei3u3/sTLy1l24YXEKipa/SsQERFpbzKqKLVmzRqKi4ubHSsuLqampoZwePPv8Y8bNw6/35/66dq1a1uE2rqC/eGiD+HSz2DEVVY7erjC6p567GB4cF/4792pmVO75u/K80c/xT/3/xduM4jhqGXyirsYOeEEZq7+vNmlvTkuDhptzZsq6JJNuDbK5H/NYcHna7cYjv/44ym69loA1o77JzXbuOIqIiIibUM5VNKv/gK/mw8nPwG9DwHDBstmwBtXwh27wKTzYeH7kIjjsrs4d+C5TD11CsOLjsY0DWrsszhjykn85aPbqI/Wpy5rGAY9BxVy5g37sM/xvTAMmDd9NW/cN5eG+uhmQ7FnZ9H14YdxlpYSXbqM5Zf8lkQo1FbfhIiISLuQUUWpHTF27Fiqq6tTP8uXL093SC0n2A8OuwWu+RZ+/TLsfgo4PLD+e3jvRrhrN3j2JPh6EkasgaN7H8YnZ01haM7ZmHE3lbHFnP/ueVzx/jWsqF3R7NIFnbM56fd70mNQIfFYgnef+JbP31rMlt72LLjwAvJGjwbTZNV1f6B+5sy2+AZERESklXTYHMrlg4GnwG9etXKoQ2+Cwl0h1gDfTILnToK7doepN8L6+RR4C3j0qH/yr/2fxhHZBYwYk5c8y6EvjeLVBa9u0nm+16geHHXpIJxuOyvnV/HybbOoWrf5YpMzGKTr449h9/tp+OorVl77O8zY1mdSiYiIdCQZVZTq1KkTa9c279hZu3Ytubm5eL3ezX7G7XaTm5vb7KfDsdmhz6FwyhPw+x/g2Hug675gJuDH9+Hl863Vv9evwLtqNuNP/D1/HvQM8ep9MU2DaSve47jJx3P3rLubrfq5PA5GXTKQwYdaK6Mz31jMe0/NIxaNbxKCYRgU//lP5Bx2KGY0yorLr6BxwYI2+wpERERky5RDbUFuKex/DVz2GVz4Aex9IXjzoHYVTL8bHhgGjx4MMx/j8NLufDB6Aj3jl5OIFFAXq+SGT2/g9DdP54s1XzS7bI+BhZx03VCy89xUrQ0x6dYvtriJjLtXL7o89BCG203dtGmsufmWLS4CioiIdDQZVZQaPnw477//frNjU6dOZfjw4WmKqB3y+GHoOXD+O3DFbDjwD+DvBo01MPsZeOpIuHcIZ9a/yeQjriK77A/E6vsQTUR44psnOPqVo3l1wavEE1bhyWYz2P+Uvow8a1cMm8EPM9fy2l1zCddGNrm1YbdTevvteIcMIVFTw7ILLyK6Zk0bfwEiIiLyU8qhfoZhQOehcPQd1ut9pz0Lux5lbSqzajZM+T3csQt5b57Pq/t25ozgv2hYexRm3M33Fd9z7jvncu20a5t1nhd2yeaU6/ci2COXxvoYr98zl+8+Xb3Z2/v2HELnO+8Am42qiRMpe+ihtnpyERGRtErr7nt1dXUsXLgQgCFDhvCvf/2Lgw8+mPz8fLp168bYsWNZuXIlzySHZy9evJjdd9+dyy67jPPOO48PPviAK6+8krfeeosjjjhim+6Z8TvH7IhEApZOhy9fgG8nw0bdUNGuI3ikZhgP1jsxiqdic5UD0D+/P9ftfR17d9o7de7y7yp4+9FviIRj5BZ6OPrSweSXZm1yu1hlJUtHn0Vk8WLcu+xC9+efw56T0+qPKSIi0hraY+6gHKqN1K23Xumb+zys+XrDcV8hC4qP5JpFvVmQNw9XYCYYJk6bk7N3O5sLBl5AtisbgFgkznvjv+PH2esA2POI7ux7fC8M26Y781VMmMDaW/4KQMnf/0bg5JNb/xlFRERawbbmDWktSk2bNo2DDz54k+Njxoxh/PjxnHPOOSxZsoRp06Y1+8w111zDvHnz6NKlC3/5y18455xztvmeO2VCtbFIPXz3BsydAIs/Bqz/+iM2D69Gh/J4bhHrg98Rxxp6ekKfExg7bCw+pw+AyjX1vHn/l9SUNeDy2Dniot3ptlvBprdZsZIlZ55BfH0Zvn32oetjj2JzudrsMUVERFpKe8wdlEOlwZpvrAW+r16C+nWpwwuM7jzMED4sriDqXQJAkbeIcQeMY5+SfQAwEyYz31zMF1Osv/caUsSh5+6G02Xf5Dbr/nUX5Y8+CnY7XR96kOwDD2z1RxMREWlpGVGUSoedPqHaWNVy+OpFmPsCVPyYOvytUcBtRb2Ym7WOBCa9/b25c+Sd9A70BiBcF+E/D3/N6oXVGDaDA0/vy+4Hddnk8g3ffcfSs35NIhQi96ijKL3jdgxbRr0xKiIiotwhSd9DUjxmzeycOwHmT4G4NdIgatp4yNOfVzvbKDOrMTD47eDfctGgi7DbrOLT/P+t5oPnvicRMynqlsPRlw4iK+BudnnTNFl9/fVUv/Y6htdL92eewTtw9zZ/TBERkV9CRaktUEK1GaYJKz6HuROIfTUJR7QWgM89bn7fqTMVRgyvw8v/7ft/HNf7OADi0QQfPv898/9nzYwadEgXRpzSF9tPWtHrpk9n+cWXQCxG/nnnUfyH69r22URERH4h5Q4WfQ+bEa6Eb14hMXcCtpXWsPOwYXBLYTFvZlsd4vuU7MM/D/gnhd5CAFYtqOI/D39NQ32U7Dw3R106iKKuzcccmJEIyy/5LfWffoq9oIAeL0zA1a1b2z6biIjIL6Ci1BYoofoZ0QYq5kzmx6mPsWdkFpV2g2uKuzLXY/1rcmKfExm7z1i8Di+maTLr7aV89toiALoPLODw8wfg8jiaXbL6tddY9cfrASgeez35Y8a07TOJiIj8AsodLPoets5c/wNfv/UQRYsnU2JUMDk7m78WFREhTqG3kFsPuJVhJcMAqF4f4q0HvqJyTQiH287h5+1Gz8FFza4Xr6tn6W9+Q+N33+Hs3o0eL7yAIz8/HY8mIiKy3bY1b9C7VNKc00P+sDPY/bp3eaH//djiWYxfvZTzKsIYGLy68FVGvzWaRVWLMAyDvUb14IgLd8futLH063JeuX0WtRUNzS7pP/54iq69FoC1/7yVmrffTseTiYiIiLQao2gXBp1zFyt/M533nAdzQl0dE1csp1PUSVm4jAunXsjDXz5MPBHHX+Tj5D8MpUu/PGKNcaY8/DVzpi5j47Vie3YWXR95GGdpKdGly1h+yW9JhEJpfEIREZGWp6KUbJbXZefXZ/ya2jHvs9zVl2uq1/PIqnV4Yy4WVi3k9DdP540f3wCgz9AgJ167J95cF+Ur65n4zy9Yu7im2fUKLryAvNGjwTRZdd0fqJ85Mx2PJSIiItKq9upTysg/vszn/f5It2iC11cuYmQNJMwED8x9gEveu4TycDlun5NjrhjMbgeUggmfvryQaRPmE48nUtdyBoN0ffwx7H4/DV99xcprf4cZi6Xx6URERFqWilKyVd177UqP6z5hTc8TGd4YZsrKRfQKuWmIN/Cn//6JGz+9kXAsTHHPXE69fi8KOmcTronw6r9ms+CLtanrGIZB8Z//RM5hh2JGo6y4/AoaFyxI45OJiIiItA6Hw87eZ/yJhjNeIWH3c1/5Msauq8OWcPC/1f/jlNdP4fM1n2O32xg5eldGnNIHDJj3ySrevO9LGuqjqWu5e/Wiy0MPYbjd1E2bxpqbb2Enm74hIiIdmIpS8vOcXjqd/RTxI24l3zR4Ze0CTq0ATHhlwSuc+vqZLKpeRE6+h5Ou25PuAwuIRxO8+/i3fDFlcSpxMux2Sm+/He+QISRqalh24UVE16xJ77OJiIiItJLsfgeTdfl/aSgaxOj6CiauXE5uo4+yhjLOf+cCHp77CCYmexzajaN+OwiH286K7yt5+bZZVK/f8Kqeb88hdL7zDrDZqJo4kbKHHkrjU4mIiLQcFaVk2xgG9uGXYBvzOoavkBuql3HX6hocMQ9La3/k5NdO55UfXsflcXDUbwcx+JCuAHz2+mLeH/8d8ajVim7zeOjy4AO4evYktmYNyy+6mHhtbTqfTERERKT1BLriuehdzMFnskssytTV8xlS48MkwQNf3s/oN86nPFxOz0GFnPT7PcnOc1O1NsSkf85i1YKq1GVyDj2U4v/7MwBl995H1csvp+mBREREWo6KUrJ9euyP7eKPoHQIhzZW8c6qRQRDfmJmAzfO+DPnvHEdDbEG9j+tLweN3hXDZjD/szW8ds8cwnURABx5eXR97DHsRYU0/vADS848k5q338FMJH7m5iIiIiIZyOnFOOEhGHU7XsPOM+Xfc+k6IOHg28ovOPylE3h74XSKuuZwyvV7EeyeQ0N9lNfunsP3/1udukz+6NEUXHQRAKtvuJE1f/2bus5FRCSjGeZO9lK6tjNuIdEGeOtamPs8ceCPeQN521+DYZi4Ep352/DbGNVvEMvnVfD2Y98QCcfILfRw9GWDyS/JAqDhu+9Yes65JKqrAXD16U3hJb8ld9SRGHZ7Gh9ORERkA+UOFn0PLWTJdJg4BurX87UnwLn5XWl0V2OaBnv7z+T+o36Py7Dx/lPz+HHOegCGjurOPsf2wrAZmKbJ6v/7P6pffgUAw+nEf9JJFFx4Ia4undP5ZCIiIinbmjeoKCU7zjTh88fh7eshEeOdQG/+kOMk4QhhJlwMcl/AXceci7MuzlsPfklNWQMur4MjL9qdrv3zAYhXVVHxzLNUPPssieRrfK4ePSi4+GL8xx6D4XCk8wlFRESUOyTpe2hB1Svg37+BVbOpt9k5v3gvvvVYHVH2xn78cejNnL5HPz57YzGz314KQO89i/jVObvhdNkxTZPQZ59R9sCDhD7/3Lqmw4H/+OMovOgiXN27p+vJREREABWltkgJVStY+im8dDbUr2etL58xwX6sNFYAkKjZh4t2u4azh/TiwyfmsfrHagybwYFn7MLuB25YzYvX1lL53HNUjH+aeLJzytmlCwUXX0Tg+OMxXK60PJqIiIhyB4u+hxYWbYC3fgdznwPgqa7D+ZdtHdiiJKK59EhcxK1Hn4B9aYhpz39PIm4S7J7DUZcOIsvvTl0m9PnnlD30EPWfzrAO2GzkHnM0hZdcgrtXr3Q8mYiIiIpSW6KEqpVUr4SXfgMrZxE3bNzefxTPh74BwyTe0IlA3fn8368OxDm7kh9mrgVg8K+6st/JfbDZjNRl4nX1VL4wgYqnxhOvqADAUVpCwQUXEDj5ZGxu92ZvLyIi0lqUO1j0PbSCn3Sdzy/ux0U5OVTE12KaNiLrD+PEnr9mTN9SPn3mexrrY2TnuTn6skEUdslpdqnQnDmUPfww9R99bB0wDHJHHUnBJZfg2WWXNDyciIjszFSU2gIlVK0o2gBTfgdzrBW/Gbv+imvi66iPV2PGXTSsOYlhhYfwG38eiz5cBUCPgQUcdv4AXJ7mr+klQiEqX3qJ8ieeIL6+DABHMEjBBecTOO00bB5P2z6biIjstJQ7WPQ9tKKlM5Jd5+sIeQP8pf9I3q2cDUCsbhccZaO5fK8BZH1WSfW6ME63ncPPH0CPQYWbXCr89TeUPfwwde+/nzqWc9hhFP72Ejy77dZmjyQiIjs3FaW2QAlVKzNN+OIJ+M8fIRFjXXF/ruvcg9mV3wIQqdyH6LpjOK9rZwq+rSMRM8kt9DDs2F7ssncxxkZdUwCJhgaqJr1M+eOPE0vuLmMvLKTg3HPJO+N0bFlZbf6IIiKyc1HuYNH30Mo26jo3DRuT9z6Tv5Z/RjQRIRHNpWHlaHp7duPMqI/QinowoN++ndj7mJ7kFng3uVzD999T9tDD1L77rpWfAdkjR1J46W/xDhrU1k8nIiI7GRWltkAJVRvZaMUv5gnw8N6n8OiKqZiYxBtKCK84i97xEk4KuzHDcQAKOmez7wm96L57AYbxk+JUJEL1K69S/uijRFdZXVb2QID8c88l76zR2LOz2/wRRURk56DcwaLvoQ3EGq05U3OeBeCH/kfye2c9i2uXgmmjcf3hxMoO5NcuP8XrYwDYHAYDD+zC0FHd8eZsOoOzceFCyh5+hJopUyCRACBr//0pvPS3+Pbcs+2eTUREdioqSm2BEqo2VLPK2llm5Rdg2Ph0+PmMrZxJRUMlhukmtPIkjJrBjDQ8DK6zYcSsfxVLevvZ98TelPYJbHJJMxql+vXXKXvkUaLLlgFg8/vJ/81vyP/Nr7H7/W35hCIishNQ7mDR99BGTBO+eDLZdR4lFNyNv+6yF2+unAZAvH5XwitPozSSzbFGFjk11uKe021nj0O7sseh3XB5N929uHHxYsofeZTqN96AuPUZ3777Uvjb3+IbtvcmC4IiIiK/hIpSW6CEqo3FGmHK72H2MwCs638Uf8hxMGv9XABstftRvXIUnriTfSNOhkYc2KxFPLoPLGDf43ttMsgTwIzFqJkyhbKHHiayeLF1rexs8n59FvljxuDIy2uTxxMRkY5PuYNF30MbW/Y/q+u8bi2mx8+r+1/EP5ZMpjHeiIs8qpacTjzUgx4xG7+KusmPWB/zZDvZa1QPBhxYisNp3+SykeXLKX/0UapenQwxq9vKO3QohZf+lqz99lNxSkREWoSKUlughCpNvngKplwHiSixYH8eHHwUjy+chIlJnqsTrrpDWLioP9lxJ/s1OBkYcWADMKDvXsXsc1xP/EW+TS5rxuPUvvMOZQ89ROOChQAYPh/5o88k/9xzcRQUtO1ziohIh6PcwaLvIQ1qVltzplZ8DhjM3/8yfl/7JUtqlmLDRhfXfixbtA+1NUXsErVxQIOT/IQNgOw8N8OO7cmu+3TCZrdtcunoypWUPf441ZNexoxGAfAMHkTRpZeSdeCBKk6JiMgvoqLUFiihSqNln1mJVd1a8PiZfsh1/Hnxy5Q3lAPgd+XTxXYE33y3G856FyMaHPSPWu3nhg0G7N+ZvY7uQZbfvcmlzUSC2vfeo+yhh2n87jvrMx4PeaefTv755+EMBtvuOUVEpENR7mDR95AmsUb4zx9g1ngA6vsfw9+KS3lz6dupU/pmD6N+3YH8sKSQ3SN2RjQ4yTGtolJusZcRJ/Sh5x6Fmy00RdeupfzxJ6h66SXMxkYAPAMGUPjbS8g+5BAM26YFLRERkZ+jotQWKKFKs5rVViv6ipmAQWjkH3k12I2n5z3N6vrVAGQ5sxmcO4plS/akfImNAxuc9Iwl288dBruP7MK+R/XA7XNucnnTNKn7cBplDz1Ew9dfA2C4XAROPZWCiy7EWVzcVk8qIiIdhHIHi76HNNuo65yi/swbdQtPLn+Xd5e8i4mVzu8aGIS/8Qhmfl1Ev1ob+zQ68CaLU9mlPg45dRe69s/f7OVj69dT/tR4Kl94ATMcBsC9664UXnopOYcdquKUiIhsFxWltkAJVTsQa7SGd856yvq9yzCig0/nP1k+nvzhJX6s/hEAl83FgSVHEas4iHmzYuxb56A0biVEcYdB7wNLOPyEvjhdm85LME2T+v9Op+zBBwnPmQMki1NnnE7hhRfiKCpqm2cVEZGMp9zBou+hHVg+09pEpm4NuHNh8Bks7XUAT1XM5vUfXyeasF7D6+3vw26+E/jy667kLYsytNGBC6s45Sj1cuTofnTvs/n5m7GKCirGP03l88+TqK8HwN2vH0VXXG51Tum1PhER2QYqSm2BEqp2ZNZ4a8UvnpzMaXOQ6HUIH3UfzOPV3/JV+TfWYcPGIV0Pp9Qcxdf/tdF7TYzC5LyEBgcUDCvilFN3Jdu76TbIpmkS+t//WH/f/YRnzwaSr/WdcQYFF5yPo7CwTR5VREQyl3IHi76HdqJ2jdV1vvyzDccC3VjX/yie89j494oPCMVCAJRml3Jo6emsWTCA2tk17N5gw54sToWK3Yw8uQ97D9p8F3m8qoryp5+m8plnU8Upz267UXjF5WSPHKnilIiIbJWKUlughKqdqV4BX0+yftZ+nTpsOrx80Wd/nvCYTK/+IXX8gM4HsF/+Kcz/yIdvQT25CSshqrabmAP9nHzcLvQr3fS/V9M0qf/0U8ruvY/wl18CYHi95I0+k4Lzz8eRv/lWdhEREeUOFn0P7Ug8Bj9+AN9Mgu/fgkhd6k/VRbvy76678nz9Yioi1QDke/I5pc+Z2Nbsy9JpFXSrNTEwSGCyKs/O4CO7cfzw7ng3030eq6yk4qnxVDz3HGbIKnZ5Bg6k6IrLyTrgABWnRERks1SU2gIlVO3Y+vnJAtVEqFycOvx9dh5PlvbinWgZieTMhD2K9uDUnmNY+Uk+oblVeBLWuWvtCdZ0c3PkYT04amApnp9shWy91vdf1t9734aZUz4f+b/+NfnnnoMjb/Ot7CIisvNS7mDR99BORULww9vwzcuw4N1UB3rYMJjcpT/jPQarYrUAZDmzOG2X0+gdPoLv/lNOTkUMgBgm3/oSdBnRidEH9KBPMGeT28QqKqh48kkqnp+QmjnlHTyYwiuuIGvEfipOiYhIMypKbYESqgxgmrBqNnz9Mnz7CtRaA9CXOxyMLyxmstdJBKsK1SfQh7P7nkvs856snrkee9y6xDJHnDkBk5H7deXMYd3oWZj1k1uY1E2bRtl999Mwbx4Atqws8s7+DQXnnIPd72+75xURkXZNuYNF30MGCFfB929aC3yLPwYzQRR4JzuLJ4o6sRBr5pTT5uT4PsdzsO0Uvn6jCsqsXfcaMZnpiWHbNYcz9+vBEQM64XI0H3AeKy+n/PEnqJwwIbVbn3fPPSm64nJ8++6r4pSIiAAqSm2REqoMk4jD0k+t5Grea9BQxXq7jedyc/i33099Mu8pzSrlNz3PIX/eQBbNWIeR7Jxa4IjziTfKbv0KuHRkH/btld8sWTJNk7oPPmD9fffT+P33ANiys8kfM4b8MWdj178jIiI7PeUOFn0PGaZ2LcybbHWhr5iJCXzs9fB4IMBcjzWH02bYOLzb4RzrHM0P/wnTWNYAQL1hMsMTZUWend+M6MGY4T3w/2TX49j69ZQ//jiVL7yIGbG6s3x77UXhlVeQNWxYWz6piIi0QypKbYESqgwWi1jzE76eCPOnUBMP81JODs/6c6iwW6/p5blyGd31PLov2ItFM8vBBBOT75xxZnpidO3p57KD+3BIv2Dz4lQiQe1771F2/wM0/mDNsLLl5pJ/zhjyzz4be3Z2Wh5ZRETST7mDRd9DBqtcYr3e9/XLsO5bZrvdPBHI5WOfN3XKiE7DOcF2Dms+NKmvsDqgqmwJZrpjLMmGM/brzvn79ySY42l26ejadZQ/9hhV//43ZtTqxPLtsw9FV16Bb+jQNntEERFpX1SU2gIlVB1EpB7m/we+eZmGhVN5zefmKX8uK50OAHw2J6cXjKbX4pGs/Lom9bEljjifu2N4uvi49JC+HD2wBLvtJ8Wpd99l/f33E1n4IwB2v5/8c88l79e/xp7d/DVAERHp+JQ7WPQ9dBBr51kD0r+exPz6VTwZyOXtLB+J5GLd4KzenGi/guoZbsI1VgdUyDCZ64rxbVaCY4d15eKDetElz9fsstE1ayh75BGqJr0MyeJU1n77UXjF5fiGDGnbZxQRkbRTUWoLlFB1QKEK+O4NYt9M5N11s3nCn8MPbqst3QmcZDuE/tWnsubbCMk56ay3JfjCHSNU4uLig/tw4pAuzWYmmPE4NW+/Tdn9DxBZbA1dtwcC5J9/HvmjR2PLUnFKRGRnodzBou+hgzFNWDkLvp7I8nmv8rQrwqvZ2USSi3V9yeOkxMXEF/SgLtk5FcNknivOHG+cA4aW8tuRvTYZih5duZKyRx6l6pVXIGYNUs864ACKrrgc76BBbfuMIiKSNipKbYESqg6uZjXmN6/wybwJPBFfx2zPhhbzPSOd2b/uTOKLuxGPWP/a1xsms90xVhc6GDOyF2cM64rP5Uh9xozHqZkyxSpOLV0KgD0/n4ILLiDvzDOweb2IiEjHptzBou+hA0vEYcknlH05gedXfsj/t3fnUXJUd5r3vxGRa+1VKlWVdgltgJAEkpEMEgYDNmC3DW5v2DTg127TtsFjn+4+4z4z48E+Mz6eGea4Pe2mgX7HQNsYG+O3AbexwaBm382qHW1oryrVklVZlVss9/0js7Iqa9GGVOvz0cmTmRE3IuNGqEqPfjcy4lflEXrs/GBdZWDzsfQVzDryUVKH+wfwdoV8/hTzWLZyOt/48GKWzy69SUzuwAHa7ryTroceBj9/J5qKiy+m/pvfJH7OslHrmoiIjA0VpUagQDWFtO/izT/dyd37/8hzIR+/cFp6zI3xkY6PMqf5QwSp/EU7XQybIj47ai0+e/F8rr9gPtXx/gt6Gs+j63e/o+2f7sDdtw8Ap76e+q/+JTWf/zx2LDb080VEZFJQdsjTfpgivCzJbb/jwXf+X36Z3kdzKH/dTgycmVzM+o5rCB2eXWze4uTPPp9+di3fuHQRa8+YVrK63L59tN1xJ12PPAJB/k40FZdeyvRbbiZ29tmj1i0RERldKkqNQIFqCjKG9vee4Y+v386jnZt4u3AmlB04LG07jw+2Xkk0OT3fFMPOcMDmCsNHPjSHL190BvUV0f5VuS5dv/032v7pn3APHgQgNH060266iZrPfRY7Gh36+SIiMqEpO+RpP0w9QbqTN167nUd3/Rt/NEm6CzeWqU5P58KWy5nbej6Wn5+WtAyvRz1Ciyq46SOLuWTJ9JKbyuTee4+2O+6g699+VyxOVX7kI9TfcguxpUtGv3MiInJaqSg1AgWqKc7NsP+d+/jD5vt4NNvM7kgYDMzsXszqQ5cyK9E/YnfICXirzOf8dbP46iULmVXT/1U9k8uRePhh2u68E+/QYQBCDQ3UfvGL1Hz+c4Rqa0e9ayIicnooO+RpP0xtbusWnn/lxzx6+AWeDkPWtom6ZSxrWce5zZcQcfN3Ks5heCfq0z07xpevWMyV5zSV3FQmu3s3bbf/E92//33+ulbkz5yqu+F6ytauLSlkiYjIxKWi1AgUqKSP6TrIttf+iUd3/44/ODlaQyFqU42sOHwJS46cj2PyX99L2AFvxXwWnN/IX12+iIXTK/rXkcuR+Nd/pe3Ou/CamwGwolGqP/lJaq//C2JLNPInIjLRKTvkaT8IAIFP77t/YMObd/Fo13ZejkXAhFjc9gFWHvowdekZ+WYY3g37HGoM8/krF3HNubNKbiqT3bmTI7ffTvIPjxWnRZcsoe6G66n6sz/TpRFERCY4FaVGoEAlQxiDv+9lXv/T7Tza8ipPxMJ4fjXLWtazrHk9cS9fhMpYAe9EfGpWTuOrVyzmnFn9F/QMcjmSf/gDHf/yMzJbthSnl13wQepuuIGKiy/Gsu0hHy0iIuOfskOe9oMMkeqg7a2f8fjWX/Ko18nGaJQ5iTNZefhSZnctLTY74HjsqnO46qMLuHbtPOIRpzgvu3sPnff9nMRDD2PSaQCc2lpqPv85ar/wRcKNDaPeLRERef9UlBqBApUcVS5FbvNDPPfOT3m0dy/PR6uY376GlYcuoSbTCIBPwLZIDrOklq/82VI+ML+uuLgxhvQbb9DxLz8j+eSTxWsmhOfNpe4vrqf6U5/CqSgfk66JiMjJUXbI036Qo2rZwt4/3cXv33uc30cg6c5jxeEPs6htFY7JX8+z08mypSLEusvncf1FC6iK9d9Uxu/qIvGb/4/OX/wC99Ch/MRQiKorr6TuhuuJr1gxFr0SEZGTpKLUCBSo5Lgl9pF84194ctuvedTK0pJexYrDlzIzuajYZF+0i85ZNXzx6mVcPOiCnu7Bg3T84n4SDz5IkEwCYFdUUPPpT1P7F9cRmTNn1LskIiInTtkhT/tBjovvYt79I1ve/L/8rv0tng41MfPIJSxrWUfULwMgbWfZUpHh7HVL+X8uW8i0gTeV8TySG/6djp/9jPTrrxenx889l7obrqfyIx/BCoeHfKyIiIwvKkqNQIFKTlgQwL4XaX39bh478BTPBAupbLuChe3nYpM//bwj2snuaRnOXL2cy8+ZzcrZNYSc/Nf1gt5eEo88QufP7yO3Z09+nZZFxWWXUnfDDZSdf74u6ikiMo4pO+RpP8gJ623Df/tXvLrx5/w+08X+1KUsbb6Eqmw9AL7lsb2yGWtxIx9etYwPLZnO9Mr+AlV602Y6f/4zun7/B3BdAEJNTdRe90VqP/tZnJqaseiViIgcBxWlRqBAJe9LNglbHmH3m/fyh/Z2dvdcxey29UT8/MU4s06KrTW72FYecNa8D3D50vlcsmQ6DVUxTBDQ+/zzdPzs5/Q+/3xxldEzz6Tu+uup+rOPY0ejI32yiIiMEWWHPO0HeV8Ov03mjZ/xzLu/5dn0uYQ6rqShZ0H/7PJ9vFXZSm7aTC5fvJpLljayam5+kM87coTOX/6KzgcewG9vB8CKxai++mrqrv8LoosWjfSpIiIyRlSUGoEClZwy7bswb93Pm+88zFOJD+AnLqO8MPIHsL96GxtrdrLDijOvbA2XLV7KJUuns3peLcF7e+j4+c/pevgRTCYDgFNXR+21n6fm2msJN+iiniIi44WyQ572g5wSXhbefYyuN37Go++1szv5caq7z8U2+bPPU+Ek26a9wdtlCbLBIi6ctYZLz5zBh5ZMpzFm0/3o7+n4+c/Jbt1aXGX5unXU3XA95RddpBvLiIiMExOqKHX77bdz22230dzczMqVK/nJT37CmjVrRmz/4x//mDvuuIN9+/ZRX1/PZz7zGX74wx8SO45bxypQySkX+LDnGYK3H+T5jQd4J/lh6FmBRT4U9UQSbGl8gc3Ve+jOzCOSXcGFc87hkqWNfKgpQuyP/0bnL+7Ha27Ory8cpuqqK6m74Ubi5ywbw46JiAiM3+wwmvkJxu9+kAks2QwbH+TwG4+x4eA82pOXEfJqAQgI2Fe7mc3TX2G3Y+P1nM0Z5av58JK5XLyknmVHdpO8/z6ST26Awn9nIvPnU3v9X1BzzTXY5bqxjIjIWJowRakHHniAG264gTvvvJO1a9fy4x//mAcffJDt27fTMMzZIvfffz9f/vKXufvuu7nwwgt59913+dKXvsS1117Lj370o2N+ngKVnFZeFnb9O92vPcZLGx129nwI/PzfM9/y2VP3Npsbn+dgrB2vdxle8mwWVJzDpYvqubxtG/V/fIjsW28VVxdftYq6G26g8vLLsEKhMeqUiMjUNh6zw2jnJxif+0EmkY7d+BsfYseL23i55Rx6M8uLs7qibWxpep6t9a+Rys3ES55NJHsOF8xfyEfrfFa9/gT+7x4h6OkBwK6spOYzn6H2uuuIzJ41Vj0SEZnSJkxRau3atZx//vn84z/+IwBBEDBnzhy++c1v8nd/93dD2t9yyy1s3bqVDRs2FKf9zd/8Da+88grPD7hOz0gUqGTUuGn8rX9k19Nv8uauRtqyS4uzOuKH2dz4Ajumv0bWsvCSZ+L1LCOcPZM/j2e4avszTHvtWfB9AEIzZ1B33XXUfOYzONXVY9UjEZEpaTxmh9HOTzA+94NMUkfepfOl37PplS42d6zCN/mznjzLZVf9m2xufJ7Wir34mdl4yfwg39L4DP6iazMrXnmM0OGD+fXYNpWXXUbdjTcQX71aN5YRERlFE6IolcvlKCsr4ze/+Q3XXHNNcfqNN95IIpHgkUceGbLM/fffzze+8Q3++Mc/smbNGnbv3s3HP/5xrr/+ev7Tf/pPx/xMBSoZE9ke2l58gk3PHmJ78wI8k/+qhGtneXf6n9jS+Dzt5YcwQQi/dxFez9lUtc7mi4e2cumOF4j1dgNgRaOUrVlDxfp1lK9bR2ThQgUsEZHTbLxlh7HITzD+9oNMAcbgHtjMjsdfYOOmOG2Z2cVZR8r3s7nxeXbWv47nuAS5aXjJs/G7z+aD+9Ncd+AV5u/dXGwfnjeXinXrKV+/nvK1a/T1PhGR0+x4c8OYfh+ora0N3/dpbGwsmd7Y2Mi2bduGXeaLX/wibW1trF+/HmMMnufxta99bcRAlc1myWazxffd3d2nrgMixytaQf2HP8UlH4YLOjvY/uizbHrDozNVx7KWdSxrWUdHxU7eaHqR3dPeIlS5jWyTxU8Xz+WeFeu5cIvhUzveYn7XYXqfe47e554DIDRjBuXrLqRi/XrKL7hAZ1GJiEwBo5GfQBlKxgHLIjznHM7+y3M4Kwho+dMbbN7wLjv2TWN67xwu2f0FLtr7SbZOf5WNTS/SNe05mPYcb84p57Wes5i1/1P82aYDXLb/Ldi7j86999N5//0QDlN23nmUr1tH+fp1xM46SxdIFxEZI2N6ptShQ4eYNWsWL774IhdccEFx+n/8j/+RZ555hldeeWXIMk8//TTXXnst//2//3fWrl3Lzp07+da3vsVXv/pVvvvd7w5p/73vfY/vf//7Q6ZrlE/GmjGGQxv3s+mxd9i9J0pQuOsMTjf761/k2ZmvkIx1FNv7mXpm7pvHubtCnHewlXPa9xAJvP4V2jax5ecURwHjK5brOlQiIqfAeDtDaDTyEyhDyfiVSebY+thrbHqpk+5UWf/08i28OvNFtk7bjLECAEwQJtx5Bst21HDu3gyrmvcwI9VRsj5n2jTKL7wwfyb6hRcSmj59VPsjIjIZTdqv71100UV88IMf5LbbbitOu++++7jpppvo6enBHjTKMdwo35w5cxSoZFzp7cqy5d93sOX5Q/T09hWSAsLlb/Nuw3NsaNiNZw/4UfXjWJ2LWLq7mvP2ZljVsod5yZaSddqVlZR/8IOUr19Pxfp1hGfpQp8iIidjvBWlRiM/gTKUjH8mMOzb3Mamxzezd6ePIX9Jg7DTTk/dszw960/sifeULONnZlF/YA4rdzucd/AIK9t2UeZlS9pEz1yaPwt93Triq1djRyKj1icRkcliQnx9LxKJsHr1ajZs2FAMVUEQsGHDBm655ZZhl0mlUkOCk+PkzzAZrr4WjUaJRqOndsNFTrHy6ijnf+ocVn/ybN7b2M6mDbvZv6MXt/c8Fuw5j2/vayZS/RTvNL7Cc9XQ5aQx9RvZVg/b1lj8KjuHikMXs3x3hPMOtrCqdQeVySTJJ54g+cQTAITmzafyovWUr19H+Zo12GVlx9gqEREZj0YjP4EylIx/lm0xb/l05i2/hO72NFuePcCW5/aTTk0jeuRTXHXkE9THX+NI/dM813CQTREHJ3aQzkUHeXoRPBtUECTOYeGeOla+l2FV6x6WJA6Q3bad7LbttP/fn0IsRvmaNfki1fp1RBYs0PU8RUROoTG/+94DDzzAjTfeyF133cWaNWv48Y9/zK9//Wu2bdtGY2MjN9xwA7NmzeKHP/whkD+V/Ec/+hH//M//XDz9/Otf/zqrV6/mgQceOObnjbfRTpGRJFpSbHr2INteOEg2kz8F3cZlRmQr4bK32VW3ledru9kacUqWC1ONSS5hxs5aVu7NsrplF2d27sMxQbGNCYWJrVpF1UXrqFi/nuiZZypgiYiMYDxmh9HOTzA+94PIYL4bsOutVjY9fYDDu/qvg1Zht9IQe4uuqs28XreLF6pCJK3+/wZZWET9M7AOzmHZrjCrDrayqvVd6rLJ0g9obKL6QxcVruf5QRz9LIiIDGtCfH2vzz/+4z9y22230dzczLnnnss//MM/sHbtWgAuueQS5s+fz7333guA53n84Ac/4Oc//zkHDx5k+vTpfOITn+AHP/gBNTU1x/wsBSqZaNycz84/tbDx6YMc2VcajCrsI0yPvUWiajN/mraLFyttegfUlmwcqqzFWC3zmbvV4dxDLaxu3U5TqrNkPX5NLRXr1lF78UWUr1tHaNq00eiaiMiEMF6zw2jmJxi/+0FkJO0He9j0zEG2v9KMm/WL021cmiLbcMrfZnfNVp6t62LHoEG+MruOcPYsyrfXcs7uDKtbd3FO+27CQf96jGVjnb2MaZd8iMqL1hNbvhzLKV2PiMhUNaGKUqNJgUomskRLir2b2tm3pZ2D7ybw3f6zn2w8GiNbscvfYWftNp6t7WTPoIBVHW6gKjiH8M565m3pZlXLLla07SLu50ra+TNnU7P6PMpWriS+cgWxpUuxdD0FEZmilB3ytB9konJzPge3d7Jvcwd7N7fTfSRdMr/CbqM+9hYd1Zv5U91OXqq0SQ8Y5AtZYRrCZ2F3LaTmTYsV+w+xqvVd5va0lqzHj8WJLjuH6lUria1YQXzFSsKNDaPRRRGRcUdFqREoUMlk4eZ8Dr2bYO/mdvZtaqdrUMAqt9uYFnuL9qrNvFa/m1cqIDsgYIXtCPPKVhBPnUnZWzDn3b2san2XRV0Hh35YOEzs7LOIr1hJfMUK4iuWE547V1/5E5EpQdkhT/tBJotES4p9W9rZu6mDg+92Dhnka4hswyrfyM7abTxd187+cOkgX0NsFg2hcwnva6T69XaWH9rBuUd2UOmmB38UoaYm4suX5wf5VqwgvmwZdnn5ae+jiMhYU1FqBApUMlklWlPs29zBvs3tHNzeiTcgYFn4NES2QflGdtRu5am6dg4PClhzK+YzJ74Kq3kOva91MvvwAc7s3MeSzn1U51JDPs+pqSG2YnmhULWc2PLlhGprT3s/RURGm7JDnvaDTEZezufgjgT7Nrezb3MHiZbSzFNut1Mbe5v26s28Om0nr1SAN2BMLubEOLNmFVXeMjIbQ4Q2H2Jp5wGWdu5jXnczDoP+q2XbRBct6i9SrVhJdNFCfe1PRCYdFaVGoEAlU4Hn+hzakWDfpvxp6oMDVpndQW38LY5UbeHVabv4U0VQErAApkWbCPuzONJWS8XhOIsPeyw50sHZiQMs7DqI43tDPjc8b26+SFUYEYyedZZuoywiE56yQ572g0wFXUfS7Nvczt7N7RzcNnSQb3p4O0HFRnbUbuOpuiO0DhrkizoxakNzyfQ20nW4kgWHLRa3pFja0cyyrgPU9nYO/kissjLiy5b1F6pWriTc2Hja+yoicjqpKDUCBSqZirrb0oVrUXVwYFsHXq40YNWH3yWoeIftddt4vraN5rA9/IqCMH62ESvVwNxDZZzV6rMulWNR2wEizUO/9meFw0TPOqtYpIqvWEF43jx97U9EJhRlhzztB5lqPNfn8I6u/KUSNrfT2Tx0kK86/g5HqjbzSv1O3ikPSi6VMFDg1hBkmqjuqGXRQZsPdHus7uli2oHdWOmhZ6SHGhpKzqaKLVuGU6Gv/YnIxKGi1AgUqGSq892AQzv7r0U1OGDF7QR14Z2Y2D4SZQc5UNnC1tosOxyfjPGHXWfgVlHePZ0PdtWyvifC2W29hLfvIkgkhrS1q6uJL19OZP58Qo0NhBsbCTU2EW5sINTYiB2Pn45ui4icNGWHPO0Hmeq629Ls29LB3k3tHNjeiTfgjn4WAfWh3USje8nED9BSfpBdtQm2lcEhkx12fSYIQbqBhR11fKi3gjXdAU37W/F27oYgKG1s20QXLiR29lmEmmYQbmok1NhIqKGRcFMjTl0dlj3CoKKIyBhQUWoEClQipbrb08VrUR3Y1llyy+Q+ISvNtNA+opG9ZMoO0lqf4d36NJutblrcoaehA1iBw9npGXyoZxpntVjU7+kgtPMA5HLDtu9jV1cTbsgXqEJNjYQb8qGrGL4aG3FqanS2lYiMGmWHPO0HkX6+G3BoV6KYoToO9Q7brtJpoSb8Hia6n0R1B/sacmypSPJurpPcCMWqKreG9T2NrE2UM2d/moqdh6Gl7egbFA4Tml5PuLEpn5saGwg1NvUPADY1EWpo0GUVRGTUqCg1AgUqkZH5XkDre920HejJP/YnaT/Yg+8N92sioNpppia8l6AuRcs0j9djCV4PDpF2DmM5Q4OW4xtWJKpZk6hjTipGfY9NZSJHtLMXc6QNkxp615rhWJFIoUDVkC9aNfWfaZUPYo2Epk/HCoff5x4REVF26KP9IDKyZEdmUIbqpqdz+IG4iNVLXWgvsbJWUg0e2ytTvBhqYV9wGCvcPuwy9b0O6xLTObO7gqZUmNqkoawzg9OewG9rh+P8L51TW9uflfqyVFMToYb+13ZlpQb/ROR9U1FqBApUIicm8AMSrWnaD/TQdiBJ254jtB3oIZUa/i4xUStJbVkbmfIMG0NpXnY6aa3YixU/iBVpx7KG/5UTd2IsiczhLL+BM3LVzEzHqO+xqOpy4UgHbmsLXnMLfkfH8W24ZRGeMYPI4kVEFy0iunAR0cWLiJ5xhm7FLCInRNkhT/tB5MRket1CfuqhbV+CtveO0HEkIAiGfs3Owqcm3EKsopcDsQwv08PWsmYy5e/hxJqxnMyInzMz1sjZzGCxO4152Qoae8PUJgOiHb34LS24ra14LS2Y7PBnZg1ml5URWbiQ6MKF+ey0aBGRhYsIz5yhrwiKyHFTUWoEClQip0Y6mcuHrB37adtxgPbmHJ3JCgKGFqssfELhLpqtHO9GAw7F20hU7Ccd30+0rJ0g1I5h+OtVAUyPT2d+9XzmVc1jQXw2Z3i1zErFqEkGBK1teC0tuC3NeC350OW2toLrjri+8MyZKlaJyHFTdsjTfhB5/3w/INGcom1vJ23bdtO2t5P29hBpr2zY9mEnRcbpYbfjszOWoaP8IF3l72GXHcGJHsG3hv/aIEDEjjC3ai4Lqhcwv3IeZ9gNzM2U05gKE+nowWtuwWttwW3JD/x5LS34XV0jrs8qK8sXqlSsEpHjoKLUCBSoRE4f3w3o2LGHtk2baNvdSnuroS3dSNZUDtveWC4ZO0WHE3Aw7NES7SZR1kyqch/RynZMqJWsGTkchewQcyvnMq9qHvOr57OgakG+eFUxl8reAHfvXrI7d5LduavwvBO/beRrMqhYJSLDUXbI034QOT2MMaQ6eml7523atu2m/WAvbYlyEl4TZpjBPjD4doqk49LiBByIZOiIt5EoO4BdfZhQvI0srQR4I35mXayO+VXzmV89n/lV84tZalZoOjS3kt2Vz065Qo7K7dmDGWHAT8UqERmOilIjUKASGV2m+zA9m16ibeu7tO9N0NZVQZs3n26/cYSgBWAwdg8pJ0ur47M/kqMlliBX04xT0wyRI/QEh/HMyBdNr4pUsahmEYtrF7O0bilLa5eyqGYRkZ4suULQyu7YWQxdKlaJyEiUHfK0H0RGke/i7n2TjnfeoG3nQdqbPdqys+jw5pI1FSMvZ+Vw7V66HJeDYZ/90RTd5a0w7TBOeSs5q5Vef/jrVgE4lsOcyjksrl3MktolLKldwtK6pcyINuDu31/IToVi1Y6dZN97b8Sz01WsEpnaVJQagQKVyBjrOQL7X8Zv2UHXgVYSLSk62yGRqqLTn0XCm3XUsGVZWXwnSY+d43DIZX8sS6a+B2vaYUyshd7gMJ251uGXxWJu1dz+kFW7lCV1S5hZPhM/kTipYlV4zhycmpr8o7q6/7l20Pvqal14XWSCUnbI034QGUO+B81vYw69TfrwfhKHuuk8kqWzK0rCbSThzTzmgJ9ld5Nx0nQ4HvsjOY5UZDANbVB5GC/UQqd7iKw//E1nKsIVxfy0pG5JcbAvbkXI7dt34sWqBQsI1dfj1OSzkj0wMxVzVf7ZLi/ThddFJiAVpUagQCUyTmV7oHMPpn036YN7SRxsp7M1Q2enQyJdQ6c3i6TfcJSwFeDY3bhOLwknR2vco6cqQ7o8QSp+mE5nN112M2aYC62PFLTKwmV4nZ0nXKwaiV1ePihwVQ8paNnV1YQGhrOqKixnpD6LyGhQdsjTfhAZh4IAkoegYzd+6y669jfTeThJot2nsztOwm2k05tNzox8drdtZcDpImVnaIm4JCo90pU9pOLtJCN7abN249ppGFQXOtpgH75/wsWqEYXDxQG+IYOAw+WqwnsrFlMxS2QMqSg1AgUqkQko1wsde/Bbd5HYd5DEoS4SrTk6u8IkMrXHDFsl7BSekyYdStMb7qUn3EMm1EsmnCIbSpEJ9ZIN5V9XVVYxv3E2ixsXcOa0/qBlWVa+WLVzJ25z/qKgfiIx4DmBn+ifFnR3H/etmoewLOzycqxoFCsawQ5HCq+jWJEIdjSCFcm/LraJFKZFo1iRMHY0WtImv0yhfWRAm1gcOx7DisWw4/H8fIU5EWWHAu0HkQkmCCB5GNO+i/TB90jsP0Jncy+dHdDZU07CbTrGgN9ALoGTJhdKkQql6Qn30BvuJRvuJRNKkS3kp0yoFyKGmdOaWNQ0jyUNi1lat6Q42Gc8j9y+feT27MHv7CzNT4lBeSqRwORGvlTDsViRSH+eGZiTjpGlijlpcJaKRrHCkfw6StYZxY7HsWMxrL7nUOikt1tkslBRagQKVCKTTK4X07GH9IE9dO5tJnG4m842n47uGBk3TiaoIBtUHH/RagRZJ0UmlMINpbBCWSLhgIq4Q3V5nIrqMmrr6qhrbKKqoYGq+nLKy8JEHBvLsjC+j9/djZ9IEHR14RWehw1iA6YFvSPfUWdUWFa+QBWLYcVj2LEBgSveV8CKl87vm9ZX3Bo4LRaFUAgrFMYKh7AcJx/aQiGswgMnlJ8XCoFtqygm44KyQ572g8gkEgTQ05wf8Nuzj8TBDjpb0nR22nSnysn6ZWRMPkMFnPzlBzzLLQ72BaE0TsglHoXqsiiVVWVUV1dSO72B2qYZVE6voao6SiwSwrHz//4H6XRJkWpI4WqEZ7yRL/I+KsJh7GKGGpyV4kNz09GyVDyeL4YVshOhEFY4PGyOshwHwmHlJxkXVJQagQKVyBRiDOR6IN2J39NBtqODbFc3ma4eert6SSbS9CZdsmmfXNbCzTl4fgTXj5M2FaSDSgIz/C2aj8Wz03ihJH6oh8BJETgZAsfHd8APRfAj5bixGjJlNbjRGkKRKNGQXXg4RMM2MSugPJuiLJemzAqI4REnIGZ8ooFH1HhEfY9w4BEKPMKeC7kcJpfD5LIE2cLrbDb/PpfDZPve5why2fz7XA6TyRBks5h0esS764yJAaGrJHj1BbFwCMvpm+fkC16OgxVywHb6w5tt5+fbhXlOCMuxwclPI+Tk1+PYpfOcEDh24TMK6ww5+TAYjWHHogOeB7yOxfpHVaNRXdB1glN2yNN+EJlCfBfSCUyqA7e7g0xngmxXkkxXL92JXnq6s6R7PXJpQy5n47lhPD9KNignZSrJBRXAiZ8tFODjhZJ4oSSB00sQyhA4OQInIHAc3EgMP1JFNl5LNlaNiVYSDYf6M1TYIepYlPlZyjMp4sYlTkAcn5jxiAYeMRMQCVwigUck8Aj5LrbrYtyj5KSRslQ2i8lmCTIZTDp98mfGn2q2PTQ7DclKpTlqSGYamIEcp5CLnKG5aOC8Y2WtUKgkH9mxWEl2siLR0hyl67FOaMebG3ReoYhMXpYF0UqIVuLUzKVsNhxXialQzDKpdrraWmk9fITEkQ66O7o4nOykLZOiyw1IBw6BX4bjVRDxqihzK4nnqgiZMKEgTigXh1zD0T+KADfUghfqJgglCZwejN2LZ2dJWC5HMGRx8P0oromTNWHSREkTopcIaStCr4nQS5y0VU0kFKIs4lAWCRGPO5RVO8TDTv+0SP51POJQFg4VX8fDDpGQnX8QEPE9Il6OsJcj7GYJezlCbpaQm8PJZXHcLHY2i53LQjZDkM5gMmmCdIYgk8akM8WAFmT6pxnPKz7oe+37I49oui7GdRknEe+kWZFIf8CKxQqn/sdKptmxwtcpY1HsaAxCDng+xvcxngu+j/F8jO/1Tx/wGt/Lz/e84acX9nN+OT+/b43JnxFnWWDb+Z8Z28Ky+l4Xzlbrm29bWPS9tsEi3/ZYy0JJUDd9R7Rv0sAQ3/d6mGlDlhswr/LSDzP9P/yHU3TERESmOCcMFdOxKqYTaYDI8S5XKGZlk60cOdxKR0s73e0J2hKdNPd205nL0eNDLohieRWEvEribhVxt5K4V4GNQ8SrIeLVHPujrCxu+Ah+qJvASWKcXgI7TcrK0W155LBxTQTPj5AzcTJESBEmXXjuJUqKMD2UY5wwZWGnPydV9+Wp/hwVjwyYNmB6LOwQcWzCtkXEeEQ8d0CGyhFyM4TcfI7K56ccdi6Dlc3mBwUH5qaRslQul88CxX/nC/+Oe97whbAgKAxSnvzXH8cFx+kvYg3MTMVp0f48VRgotCIRMKaQhbyjZqeS1337d3C+GpidBr6mcJk1uz8HnVCesiywjrFsn8E5KP+mZN7RstRRlwPm3HUX4caj/5/ldFJRSkRksEIxy4pWUlM7n5rFx17EDVwSqXbaE+/R0rqfltYjJDq6SXZnyfQa3HSIIBvDcuOEchVE3Sribl/4qiTiVb7vzTYEWJYPeFhWFvCxrACLvofJ/7EMARBgSFmQxMIHspZFFoe0ZZMiRC9hMpaNB3hW4QF4lo1LHM+K42EwjoXlWFghGytkE6q1CU23iIRswo5dfI6GbGzLwrbIP9sDXlsQMkH+Efg4xhDCIxQEOBSmBQGOKTwHPiETYJuAUODhBD62CXD8gBD5ZZwgvz4Hk18OgxPkl3HMgOfCNDvwsY0pPPvYQYAVBFiBjx0YCHzswAffx/JcgkwWk83kR0sz+RFUM2AahcACFINh8L6Psoxo0WKmj/U2iIhMdYViVrRiOrNnLGP2MZobY+h1e+lMHqK5bTetLYdpb+ukqytFKumRTVn42QgmF8fOlRNx80WsiB/DMRGcXD3k6k/Bhntg+WB5WPj5RyFD5f8XbzAEGCufnzygy4IOLFwsspZFBoeU5ZAiRMoK42LhWqY/R2FwrQieFcGjHN8GHAvLsbHDNla5RajaJhyyiRTyU9+zY1s4tlWao6z8NIuAUBAQJp+PwoV8EzI+jilkpqDvdX66XcxTAU7g9ecrTGHZgBCmmJUGPuzCemzTl5/6MlThOQiwCtOtwC9mKSsIsDyvPy8Nyk0DH0W+T5BKQSp1Co6xjMTNZt/Hl3TfPxWlREROgbAdZnpFE9Mrmjhz9geP2d4NXDpSnbS2t3Kk5RCdrW10dSbp7cqR7TW4aQeTiWLnynDcOJZxsI2NbRxs4+CYob++LWwwNhDuHyA5yjZY5P8R6FvTyX1RcXg+AZ7lFwIYAwIZ+CXvDZ4FOcAbFNyGa+dj41kWnuUMWJcprjvo69ipYBceJ7t44BP1XSKBS9R3CfteyfuI7+bPSBv4PvCI+Pn3jgnwLAffsvFtm8Cy8ezC+8IjGPTet238wjJB8X3fwyl5HxQeFgbLmAHPYJe873+2C/MtE/S3xYAx2IPa24W/hLYxmL6TpYY5OKZwJlXp39XCtAHN+5YtrmPQvFXzFvHdkz9cIiIyBizLoiJSQcW0JcyZtgSWHr19XxGrtbstX8BqaSHR3kFPIk2qxyPXa+NnwljZOE6uHDsI5TNU4BQzlD3sP+4hMCEwfSWoo3MKjzAQA97/0GKhfxg8vMJgoMFlwKAghdwzJD8NzEngWgZ/yPS+7GTjWaHCfFOSz05pfgKO6/r5w7BMQDjwCznJJRq4xWwU9r2S94OzU9/7wLKKeWhwPurPQqUZa7isNFLWCiwbg4Vt8sON+cxj+vMTYA/ISiX5aEhuYlDO6s9Rw+WggYZmKGvAvNLlSpYfNO//hMuZe3KH65RQUUpEZAyE7TCNFQ00VjTAvHOO2d4YQ8pL0Z3tpjvXnX/OJulOJ+lJd9OdStCb7qYnk6Qn20sqm6bXTZPOuWQ8j6zv94exQjhzikWuELZxCAVhQkEYxw/3vw4iA173Tw8FkZL3ThAmZPrHWBwsHGMRhWMnu1MqwLKC/mfLFB+G/D/Q+VFOC98qjIVa+VFODzs/qomND/gYfFMIdMYUil6mOGZ6bCHI74GhFcAB3MJjsHzBB2wsLAOOlc93NlY+CFsWNv3T7AHti69N/tkqvB78TN8+AYLBz4VHfn9RPLuu77U/6LVv+tv4Jr+eoHDCePFM8UH7zhQ6Onh/Dgxh+bzUH7qsQfP6lvVj04bZiyIiMpkUi1j1FZxRPx+WHXsZ13fpznWTzCVJ5pJ0ZbtIpnvoznTTne6iJ5WgN52kJ9NDbzZFKpchlcuS9nKkPQ8TWMUBwYEDhP15KjQgJ0VKM5M/QmYqeR0pFsosLMJYhI+nMnbK+YXs1JelyOcnyxT/vQ0si6BY5MrnKA87/1VJ+t735ycf8IwpyQzHL55/6gs1g07l8YF04TFY3yKWKWQlq5BNyeemUDFDWVj0ZakBGWpgfmJohrIKxyawB+QoSjOTKRQJ8/NMIS/l3xf3hzGFzFnIT0BgILBK8+bg577XpUWrAqs/OpnChIH5qa9dMU9ZFt4wg92jSUUpEZEJwLIsysPllIfLmcGME17eCzx6cj0kc8l8UasQzvqee9we3MDF9V28wMu/dlN4XjuumyLnZXDdNK6fxfNzuH4OL3Dz7YyHFwS4gcEENkEQxpgImAjGRAYVskI4QXhQgCtM63tt+gNbX/uQGdRmwPyQGXilCxtj8sFupGuN9oWPYw/g6c41Qw2pFo0bS3M9Y70JIiIyDoWdMNPi05gWP/HBC2MMaS9dLGgNzE99j5yfw/XzmcgLvHxWcjtx3RSulyblpXG9LJ6fxS3JUB6e8XEDHy+wIXAITASCfH6yzOAiV19GCpVmqqCQqcwwmaovL5kBmaovX5kwjhmYhhyMcYpniw2nr9gzYNhtGOMrH4wPg/fJ+NpH9YzttcdUlBIRmQJCdoiaWA01sZrT/2G+C5mu4l17/EwHbqqdXLoDN/DIGJ+c8ckEHjl8MoFfePbImixZ4xceHtnAG/DaJ1N4zhqvf37gkfMNrmfhexD4hiCw8I3BBBaBsXEKp+1bg0c4B30tctjXg5YtDXCnk8FYPoEVYCwfYwX51/S99wnIj2jm2wT97QkK7f3ifAbOI78+LINt+kaA7QF9tLBNfrzQ6dsXQf59fr/YxX1kGRu7ML1vecv0t7MKY435gdZ8CLNM/4iwle9q4ZVVGH20in8ovmfIevLv88u9l+kGLh6lYyMiIlOBZVmUhcsoC5fRWN54ej/MGMj1QiYB6U6CVDteqpCh3F6yJiCLn88+hdyUM34hV2XJmBS5wTnL5LNTLvDoLWan/oyV831cD7zAwhuQoYIAAmMdOzcFx5GjBjzy//6f7lPATD77MCAXFTOUX5KX+tuVtu3LXWbAvIHZylj5r9g5xsHpy0Z9+wKrJCcV89CAvFSSoQbkJTtwilnLMvnzuErzUyEr5d8VztrKT+ufzzAZqi9v9S87MHN1p6qp4lRcm+3kqCglIiKnlhOG8noory/51lp8jDbHGNN/9ldhFLPvfclrP4ubTeK5vXi5HtxcD56bwnV7cXNpPC+F5yULZ4y5uEGOXOCSCzxyxsMNPFzjkQvyYbDv4ZmAnAnIkX92CXAx5IoPiteMkJP3ycrjuCOBiIjIeGVZEK3IP6pnY5O/62EEKB+jTfID/6jZKZ+fCmfXu7242W48N4WX68V18znKc9O4Xrr4nCssmyucbZ8LCo/CWWOu8QoZKp+d3EJ2yr/PZye3+Jy/rpZRhnpfbqj46Jh+vopSIiIyqVmWRdgJE3bG8r4ixxaYIB/S/Bw5P4dvfMxwt/KFkul984a0LT6N3MYUztEPTFBsZ0z/c0BQOI3f9LcxQ9sNfO5rhyE/+mhKP79kmUK7IZ89cPqAaSX9NqXT5lTOOeXHREREZCpzbAfHHq0zxE+OMQbPeLh+PkO5gTskMwxsO3Da4HxSmHjMNoNzSl/egWEy1YB81LdsYErzUUnGGpynOHZ+KmlzEhmqdtrYDuypKCUiIjIO2JZN1IkSdUa+SoOIiIiI9LMsi7AVJmyHKQufyntJy2h5Hze7FhEREREREREROTkqSomIiIiIiIiIyKhTUUpEREREREREREadilIiIiIiIiIiIjLqVJQSEREREREREZFRp6KUiIiIiIiIiIiMOhWlRERERERERERk1KkoJSIiIiIiIiIio05FKRERERERERERGXUqSomIiIiIiIiIyKhTUUpEREREREREREadilIiIiIiIiIiIjLqVJQSEREREREREZFRp6KUiIiIiIiIiIiMOhWlRERERERERERk1IXGegNGmzEGgO7u7jHeEhEREZkI+jJDX4aYqpShRERE5Hgdb36ackWpZDIJwJw5c8Z4S0RERGQiSSaTVFdXj/VmjBllKBERETlRx8pPlpliw35BEHDo0CEqKyuxLOuUr7+7u5s5c+awf/9+qqqqTvn6xzP1XX1X36cO9V19n0p9N8aQTCaZOXMmtj11r3ygDHX6qO/qu/o+NUzVfoP6PhX7frz5acqdKWXbNrNnzz7tn1NVVTWl/sINpL6r71ON+q6+TzVTse9T+QypPspQp5/6rr5PNVO171O136C+T7W+H09+mrrDfSIiIiIiIiIiMmZUlBIRERERERERkVGnotQpFo1GufXWW4lGo2O9KaNOfVffpxr1XX2faqZy3+X0m8p/v9R39X2qmap9n6r9BvV9qvb9eEy5C52LiIiIiIiIiMjY05lSIiIiIiIiIiIy6lSUEhERERERERGRUaeilIiIiIiIiIiIjDoVpU7C7bffzvz584nFYqxdu5ZXX331qO0ffPBBzjzzTGKxGMuXL+f3v//9KG3pqfPDH/6Q888/n8rKShoaGrjmmmvYvn37UZe59957sSyr5BGLxUZpi0+d733ve0P6ceaZZx51mclwzAHmz58/pO+WZXHzzTcP234iH/Nnn32WT3ziE8ycORPLsnj44YdL5htj+K//9b8yY8YM4vE4l19+OTt27Djmek/098VYOFrfXdflO9/5DsuXL6e8vJyZM2dyww03cOjQoaOu82R+bsbCsY77l770pSH9uPLKK4+53ol+3IFhf/Yty+K2224bcZ0T5bjL2FGGUoZShlKGUoZShjqaiX7cQRnqRKkodYIeeOAB/vqv/5pbb72VN954g5UrV3LFFVfQ2to6bPsXX3yRL3zhC3zlK1/hzTff5JprruGaa65h06ZNo7zl788zzzzDzTffzMsvv8wTTzyB67p89KMfpbe396jLVVVVcfjw4eJj7969o7TFp9ayZctK+vH888+P2HayHHOA1157raTfTzzxBACf/exnR1xmoh7z3t5eVq5cye233z7s/P/1v/4X//AP/8Cdd97JK6+8Qnl5OVdccQWZTGbEdZ7o74uxcrS+p1Ip3njjDb773e/yxhtv8K//+q9s376dT37yk8dc74n83IyVYx13gCuvvLKkH7/85S+Pus7JcNyBkj4fPnyYu+++G8uy+PSnP33U9U6E4y5jQxlKGUoZShlKGUoZ6mgmw3EHZagTZuSErFmzxtx8883F977vm5kzZ5of/vCHw7b/3Oc+Zz7+8Y+XTFu7dq35q7/6q9O6nadba2urAcwzzzwzYpt77rnHVFdXj95GnSa33nqrWbly5XG3n6zH3BhjvvWtb5mFCxeaIAiGnT9ZjjlgHnrooeL7IAhMU1OTue2224rTEomEiUaj5pe//OWI6znR3xfjweC+D+fVV181gNm7d++IbU7052Y8GK7vN954o7n66qtPaD2T9bhfffXV5tJLLz1qm4l43GX0KEPlKUONbLIec2OUoZSh8pShjm6yHndlqKPTmVInIJfL8frrr3P55ZcXp9m2zeWXX85LL7007DIvvfRSSXuAK664YsT2E0VXVxcAdXV1R23X09PDvHnzmDNnDldffTWbN28ejc075Xbs2MHMmTM544wzuO6669i3b9+IbSfrMc/lctx33318+ctfxrKsEdtNlmM+0J49e2hubi45rtXV1axdu3bE43oyvy8miq6uLizLoqam5qjtTuTnZjx7+umnaWhoYOnSpXz961+nvb19xLaT9bi3tLTw6KOP8pWvfOWYbSfLcZdTSxmqnzKUMtRIJssxH0gZqpQylDLU0UyW436iVJQ6AW1tbfi+T2NjY8n0xsZGmpubh12mubn5hNpPBEEQ8O1vf5t169ZxzjnnjNhu6dKl3H333TzyyCPcd999BEHAhRdeyIEDB0Zxa9+/tWvXcu+99/LYY49xxx13sGfPHi666CKSyeSw7SfjMQd4+OGHSSQSfOlLXxqxzWQ55oP1HbsTOa4n8/tiIshkMnznO9/hC1/4AlVVVSO2O9Gfm/Hqyiuv5Gc/+xkbNmzgf/7P/8kzzzzDVVddhe/7w7afrMf9X/7lX6isrOTP//zPj9pushx3OfWUofKUoZShRjJZjvlgylD9lKGUoY5mshz3kxEa6w2Qiefmm29m06ZNx/yO6wUXXMAFF1xQfH/hhRdy1llncdddd/Hf/tt/O92becpcddVVxdcrVqxg7dq1zJs3j1//+tfHVfGeLH76059y1VVXMXPmzBHbTJZjLsNzXZfPfe5zGGO44447jtp2svzcXHvttcXXy5cvZ8WKFSxcuJCnn36ayy67bAy3bHTdfffdXHfddce86O5kOe4ip4sy1NT8naAMJcpQylDKUCPTmVInoL6+HsdxaGlpKZne0tJCU1PTsMs0NTWdUPvx7pZbbuF3v/sdTz31FLNnzz6hZcPhMOeddx47d+48TVs3OmpqaliyZMmI/Zhsxxxg7969PPnkk/zlX/7lCS03WY5537E7keN6Mr8vxrO+MLV3716eeOKJo47wDedYPzcTxRlnnEF9ff2I/Zhsxx3gueeeY/v27Sf88w+T57jL+6cMpQwFylAnYrIcc2UoZag+ylAnZrIc9+OhotQJiEQirF69mg0bNhSnBUHAhg0bSkY2BrrgggtK2gM88cQTI7Yfr4wx3HLLLTz00EP8+7//OwsWLDjhdfi+z8aNG5kxY8Zp2MLR09PTw65du0bsx2Q55gPdc889NDQ08PGPf/yElpssx3zBggU0NTWVHNfu7m5eeeWVEY/ryfy+GK/6wtSOHTt48sknmTZt2gmv41g/NxPFgQMHaG9vH7Efk+m49/npT3/K6tWrWbly5QkvO1mOu7x/ylDKUKAMdSImyzFXhlKG6qMMdWImy3E/LmN7nfWJ51e/+pWJRqPm3nvvNVu2bDE33XSTqampMc3NzcYYY66//nrzd3/3d8X2L7zwggmFQuZ//+//bbZu3WpuvfVWEw6HzcaNG8eqCyfl61//uqmurjZPP/20OXz4cPGRSqWKbQb3/fvf/755/PHHza5du8zrr79urr32WhOLxczmzZvHogsn7W/+5m/M008/bfbs2WNeeOEFc/nll5v6+nrT2tpqjJm8x7yP7/tm7ty55jvf+c6QeZPpmCeTSfPmm2+aN9980wDmRz/6kXnzzTeLd0f5H//jf5iamhrzyCOPmHfeecdcffXVZsGCBSadThfXcemll5qf/OQnxffH+n0xXhyt77lcznzyk580s2fPNm+99VbJz382my2uY3Dfj/VzM14cre/JZNL87d/+rXnppZfMnj17zJNPPmlWrVplFi9ebDKZTHEdk/G49+nq6jJlZWXmjjvuGHYdE/W4y9hQhlKGUobqN5mOuTKUMpQylDLU+6Gi1En4yU9+YubOnWsikYhZs2aNefnll4vzLr74YnPjjTeWtP/1r39tlixZYiKRiFm2bJl59NFHR3mL3z9g2Mc999xTbDO479/+9reL+6mxsdF87GMfM2+88cbob/z79PnPf97MmDHDRCIRM2vWLPP5z3/e7Ny5szh/sh7zPo8//rgBzPbt24fMm0zH/Kmnnhr273hf/4IgMN/97ndNY2OjiUaj5rLLLhuyT+bNm2duvfXWkmlH+30xXhyt73v27Bnx5/+pp54qrmNw34/1czNeHK3vqVTKfPSjHzXTp0834XDYzJs3z3z1q18dEowm43Hvc9ddd5l4PG4SicSw65iox13GjjKUMpQyVN5kOubKUMpQylDKUO+HZYwxJ3uWlYiIiIiIiIiIyMnQNaVERERERERERGTUqSglIiIiIiIiIiKjTkUpEREREREREREZdSpKiYiIiIiIiIjIqFNRSkRERERERERERp2KUiIiIiIiIiIiMupUlBIRERERERERkVGnopSIiIiIiIiIiIw6FaVERN4ny7J4+OGHx3ozRERERCYM5ScRARWlRGSC+9KXvoRlWUMeV1555VhvmoiIiMi4pPwkIuNFaKw3QETk/bryyiu55557SqZFo9Ex2hoRERGR8U/5SUTGA50pJSITXjQapampqeRRW1sL5E8Nv+OOO7jqqquIx+OcccYZ/OY3vylZfuPGjVx66aXE43GmTZvGTTfdRE9PT0mbu+++m2XLlhGNRpkxYwa33HJLyfy2tjY+9alPUVZWxuLFi/ntb397ejstIiIi8j4oP4nIeKCilIhMet/97nf59Kc/zdtvv811113Htddey9atWwHo7e3liiuuoLa2ltdee40HH3yQJ598siQ03XHHHdx8883cdNNNbNy4kd/+9rcsWrSo5DO+//3v87nPfY533nmHj33sY1x33XV0dHSMaj9FREREThXlJxEZFUZEZAK78cYbjeM4pry8vOTxgx/8wBhjDGC+9rWvlSyzdu1a8/Wvf90YY8w///M/m9raWtPT01Oc/+ijjxrbtk1zc7MxxpiZM2ea//yf//OI2wCY//Jf/kvxfU9PjwHMH/7wh1PWTxEREZFTRflJRMYLXVNKRCa8D3/4w9xxxx0l0+rq6oqvL7jggpJ5F1xwAW+99RYAW7duZeXKlZSXlxfnr1u3jiAI2L59O5ZlcejQIS677LKjbsOKFSuKr8vLy6mqqqK1tfVkuyQiIiJyWik/ich4oKKUiEx45eXlQ04HP1Xi8fhxtQuHwyXvLcsiCILTsUkiIiIi75vyk4iMB7qmlIhMei+//PKQ92eddRYAZ511Fm+//Ta9vb3F+S+88AK2bbN06VIqKyuZP38+GzZsGNVtFhERERlLyk8iMhp0ppSITHjZbJbm5uaSaaFQiPr6egAefPBBPvCBD7B+/Xp+8Ytf8Oqrr/LTn/4UgOuuu45bb72VG2+8ke9973scOXKEb37zm1x//fU0NjYC8L3vfY+vfe1rNDQ0cNVVV5FMJnnhhRf45je/ObodFRERETlFlJ9EZDxQUUpEJrzHHnuMGTNmlExbunQp27ZtA/J3dvnVr37FN77xDWbMmMEvf/lLzj77bADKysp4/PHH+da3vsX5559PWVkZn/70p/nRj35UXNeNN95IJpPh7//+7/nbv/1b6uvr+cxnPjN6HRQRERE5xZSfRGQ8sIwxZqw3QkTkdLEsi4ceeohrrrlmrDdFREREZEJQfhKR0aJrSomIiIiIiIiIyKhTUUpEREREREREREadvr4nIiIiIiIiIiKjTmdKiYiIiIiIiIjIqFNRSkRERERERERERp2KUiIiIiIiIiIiMupUlBIRERERERERkVGnopSIiIiIiIiIiIw6FaVERERERERERGTUqSglIiIiIiIiIiKjTkUpEREREREREREZdSpKiYiIiIiIiIjIqPv/ARBw4ytfqsViAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "            if CFG.objective_cv == 'binary':\n",
    "                outputs.append(torch.sigmoid(model(x)).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'multiclass':\n",
    "                outputs.append(torch.softmax(\n",
    "                    model(x), axis=1).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'regression':\n",
    "                outputs.append(model(x).to('cpu').numpy())\n",
    "\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "\n",
    "def test_fn(valid_loader, model, device):\n",
    "    preds = []\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        preds.append(y_preds)\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def inference():\n",
    "    test = pd.read_csv(CFG.comp_dataset_path +\n",
    "                       'test_features.csv')\n",
    "\n",
    "    test['base_path'] = CFG.comp_dataset_path + 'images/' + test['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in test['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    print(test.head(5))\n",
    "\n",
    "    valid_dataset = CustomDataset(\n",
    "        test, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = EnsembleModel()\n",
    "    folds = [0] if CFG.use_holdout else list(range(CFG.n_fold))\n",
    "    for fold in folds:\n",
    "        _model = CustomModel(CFG, pretrained=False)\n",
    "        _model.to(device)\n",
    "\n",
    "        model_path = CFG.model_dir + \\\n",
    "            f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth'\n",
    "        print('load', model_path)\n",
    "        state = torch.load(model_path)['model']\n",
    "        _model.load_state_dict(state)\n",
    "        _model.eval()\n",
    "\n",
    "        # _model = tta.ClassificationTTAWrapper(\n",
    "        #     _model, tta.aliases.five_crop_transform(256, 256))\n",
    "\n",
    "        model.add_model(_model)\n",
    "\n",
    "    preds = test_fn(valid_loader, model, device)\n",
    "\n",
    "    test[CFG.target_col] = preds\n",
    "    test.to_csv(CFG.submission_dir +\n",
    "                'submission_oof.csv', index=False)\n",
    "    test[CFG.target_col].to_csv(\n",
    "        CFG.submission_dir + f'submission_{CFG.exp_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-0.5.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8a1618bd2c4c19866ae10c452e633e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     ID      vEgo      aEgo  steeringAngleDeg  \\\n",
      "0  012baccc145d400c896cb82065a93d42_120  3.374273 -0.019360        -34.008415   \n",
      "1  012baccc145d400c896cb82065a93d42_220  2.441048 -0.022754        307.860077   \n",
      "2  012baccc145d400c896cb82065a93d42_320  3.604152 -0.286239         10.774388   \n",
      "3  012baccc145d400c896cb82065a93d42_420  2.048902 -0.537628         61.045235   \n",
      "4  01d738e799d260a10f6324f78023b38f_120  2.201528 -1.898600          5.740093   \n",
      "\n",
      "   steeringTorque  brake  brakePressed  gas  gasPressed gearShifter  \\\n",
      "0            17.0    0.0         False  0.0       False       drive   \n",
      "1           295.0    0.0          True  0.0       False       drive   \n",
      "2          -110.0    0.0          True  0.0       False       drive   \n",
      "3           189.0    0.0          True  0.0       False       drive   \n",
      "4           -41.0    0.0          True  0.0       False       drive   \n",
      "\n",
      "   leftBlinker  rightBlinker  \\\n",
      "0        False         False   \n",
      "1        False         False   \n",
      "2        False         False   \n",
      "3         True         False   \n",
      "4        False         False   \n",
      "\n",
      "                                           base_path  \n",
      "0  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "1  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "2  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "3  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "4  ../raw/atmacup_18_dataset/images/01d738e799d26...  \n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_exp001/atmacup_18-models/resnet34d_fold0_last.pth\n",
      "pretrained: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34009/610043316.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path)['model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../proc/baseline/outputs/atmacup_18_cnn_exp001/atmacup_18-models/resnet34d_fold1_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_exp001/atmacup_18-models/resnet34d_fold2_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_exp001/atmacup_18-models/resnet34d_fold3_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_exp001/atmacup_18-models/resnet34d_fold4_last.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38df7344e465473d926570af8b9a3a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
