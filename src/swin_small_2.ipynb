{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from timm.utils.model_ema import ModelEmaV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set dataset path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'atmacup_18'  # comp名\n",
    "\n",
    "    comp_dataset_path = '../raw/atmacup_18_dataset/'\n",
    "\n",
    "    exp_name = 'atmacup_18_cnn_swin_small_2'\n",
    "\n",
    "    is_debug = False\n",
    "    use_gray_scale = False\n",
    "\n",
    "    model_in_chans = 9  # モデルの入力チャンネル数\n",
    "\n",
    "    # ============== file path =============\n",
    "    train_fold_dir = \"../proc/baseline/folds\"\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = \"swin_small_patch4_window7_224\"\n",
    "\n",
    "    num_frames = 3  # model_in_chansの倍数\n",
    "    norm_in_chans = 1 if use_gray_scale else 3\n",
    "\n",
    "    use_torch_compile = False\n",
    "    use_ema = True\n",
    "    ema_decay = 0.995\n",
    "    # ============== training cfg =============\n",
    "    size = 224  # 224\n",
    "\n",
    "    batch_size = 64  # 32\n",
    "\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 40\n",
    "    if is_debug:\n",
    "        epochs = 2\n",
    "\n",
    "    # adamW warmupあり\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-4\n",
    "    if scheduler == 'GradualWarmupSchedulerV2':\n",
    "        lr /= warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    n_fold = 5\n",
    "    use_holdout = False\n",
    "    use_alldata = False\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    skf_col = 'class'\n",
    "    group_col = 'scene'\n",
    "    fold_type = 'gkf'\n",
    "\n",
    "    objective_cv = 'regression'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'minimize'  # 'maximize', 'minimize'\n",
    "    metrics = 'calc_mae_atmacup'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 18\n",
    "    target_col = ['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1', 'x_2', 'y_2',\n",
    "                  'z_2', 'x_3', 'y_3', 'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5']\n",
    "\n",
    "\n",
    "    # ============== ほぼ固定 =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'last'  # 'best'\n",
    "\n",
    "    min_lr = 1e-8\n",
    "    weight_decay = 1e-5\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== set dataset path =============\n",
    "    if exp_name is not None:\n",
    "        print('set dataset path')\n",
    "\n",
    "        outputs_path = f'../proc/baseline/outputs/{exp_name}/'\n",
    "\n",
    "        submission_dir = outputs_path + 'submissions/'\n",
    "        submission_path = submission_dir + f'submission_{exp_name}.csv'\n",
    "\n",
    "        model_dir = outputs_path + \\\n",
    "            f'{comp_name}-models/'\n",
    "\n",
    "        figures_dir = outputs_path + 'figures/'\n",
    "\n",
    "        log_dir = outputs_path + 'logs/'\n",
    "        log_path = log_dir + f'{exp_name}.txt'\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        A.Downscale(p=0.25),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.ShiftScaleRotate(p=0.5),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        # A.CoarseDropout(max_holes=1, max_height=int(\n",
    "        #     size * 0.3), max_width=int(size * 0.3), p=0.5),\n",
    "        A.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "\n",
    "        A.Normalize(\n",
    "            mean=[0] * norm_in_chans*num_frames,\n",
    "            std=[1] * norm_in_chans*num_frames, \n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * norm_in_chans*num_frames,\n",
    "            std=[1] * norm_in_chans*num_frames,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA が利用可能か: True\n",
      "利用可能な CUDA デバイス数: 1\n",
      "現在の CUDA デバイス: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA が利用可能か:\", torch.cuda.is_available())\n",
    "print(\"利用可能な CUDA デバイス数:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"現在の CUDA デバイス:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "def get_fold(train, cfg):\n",
    "    if cfg.fold_type == 'kf':\n",
    "        Fold = KFold(n_splits=cfg.n_fold,\n",
    "                     shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.target_col])\n",
    "    elif cfg.fold_type == 'skf':\n",
    "        Fold = StratifiedKFold(n_splits=cfg.n_fold,\n",
    "                               shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.skf_col])\n",
    "    elif cfg.fold_type == 'gkf':\n",
    "        Fold = GroupKFold(n_splits=cfg.n_fold)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.group_col], groups)\n",
    "    elif cfg.fold_type == 'sgkf':\n",
    "        Fold = StratifiedGroupKFold(n_splits=cfg.n_fold,\n",
    "                                    shuffle=True, random_state=cfg.seed)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.skf_col], groups)\n",
    "    # elif fold_type == 'mskf':\n",
    "    #     Fold = MultilabelStratifiedKFold(\n",
    "    #         n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    #     kf = Fold.split(train, train[cfg.skf_col])\n",
    "\n",
    "    for n, (train_index, val_index) in enumerate(kf):\n",
    "        train.loc[val_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "    print(train.groupby('fold').size())\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_folds():\n",
    "    train_df = pd.read_csv(CFG.comp_dataset_path + 'train_features.csv')\n",
    "\n",
    "    train_df['scene'] = train_df['ID'].str.split('_').str[0]\n",
    "\n",
    "    print('group', CFG.group_col)\n",
    "    print(f'train len: {len(train_df)}')\n",
    "\n",
    "    train_df = get_fold(train_df, CFG)\n",
    "\n",
    "    # print(train_df.groupby(['fold', CFG.target_col]).size())\n",
    "    print(train_df['fold'].value_counts())\n",
    "\n",
    "    os.makedirs(CFG.train_fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(CFG.train_fold_dir +\n",
    "                    'train_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group scene\n",
      "train len: 43371\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "dtype: int64\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "make_train_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数固定\n",
    "def set_seed(seed=None, cudnn_deterministic=True):\n",
    "    if seed is None:\n",
    "        seed = 42\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def make_dirs(cfg):\n",
    "    for dir in [cfg.model_dir, cfg.figures_dir, cfg.submission_dir, cfg.log_dir]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def cfg_init(cfg, mode='train'):\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if mode == 'train':\n",
    "        make_dirs(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_init(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common_utils.logger import init_logger, wandb_init, AverageMeter, timeSince\n",
    "# from common_utils.settings import cfg_init\n",
    "\n",
    "def init_logger(log_file):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- exp_info -----------------\n",
      "2024年11月21日 23:09:40\n"
     ]
    }
   ],
   "source": [
    "Logger = init_logger(log_file=CFG.log_path)\n",
    "\n",
    "Logger.info('\\n\\n-------- exp_info -----------------')\n",
    "Logger.info(datetime.datetime.now().strftime('%Y年%m月%d日 %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    # return roc_auc_score(y_true, y_pred)\n",
    "    eval_func = eval(CFG.metrics)\n",
    "    return eval_func(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calc_mae_atmacup(y_true, y_pred):\n",
    "    abs_diff = np.abs(y_true - y_pred)  # 各予測の差分の絶対値を計算して\n",
    "    mae = np.mean(abs_diff.reshape(-1, ))  # 予測の差分の絶対値の平均を計算\n",
    "\n",
    "    return mae\n",
    "\n",
    "def get_result(result_df):\n",
    "\n",
    "    # preds = result_df['preds'].values\n",
    "\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "    preds = result_df[pred_cols].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    Logger.info(f'score: {score:<.4f}')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_traffic_light(image, id):\n",
    "    path = f'./datasets/atmacup_18/traffic_lights/{id}.json'\n",
    "    traffic_lights = json.load(open(path))\n",
    "\n",
    "    traffic_class = ['green',\n",
    "                     'straight', 'left', 'right', 'empty', 'other', 'yellow', 'red']\n",
    "    class_to_idx = {\n",
    "        cls: idx for idx, cls in enumerate(traffic_class)\n",
    "    }\n",
    "\n",
    "    for traffic_light in traffic_lights:\n",
    "        bbox = traffic_light['bbox']\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # int\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        point1 = (x1, y1)\n",
    "        point2 = (x2, y2)\n",
    "\n",
    "        idx = class_to_idx[traffic_light['class']]\n",
    "        color = 255 - int(255*(idx/len(traffic_class)))\n",
    "\n",
    "        cv2.rectangle(image, point1, point2, color=color, thickness=1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_image_for_cache(path):\n",
    "    if CFG.use_gray_scale:\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # image = cv2.resize(image, (CFG.size, CFG.size))\n",
    "\n",
    "    # 効かない\n",
    "    # image = draw_traffic_light(image, path.split('/')[-2])\n",
    "    return (path, image)\n",
    "\n",
    "\n",
    "def make_video_cache(paths):\n",
    "    debug = []\n",
    "    for idx in range(9):\n",
    "        color = 255 - int(255*(idx/9))\n",
    "        debug.append(color)\n",
    "    print(debug)\n",
    "\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        res = pool.imap_unordered(read_image_for_cache, paths)\n",
    "        res = tqdm(res)\n",
    "        res = list(res)\n",
    "\n",
    "    return dict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import ReplayCompose\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        # aug = A.Compose(cfg.train_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        # aug = A.Compose(cfg.valid_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, cfg, labels=None, transform=None):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.base_paths = df['base_path'].values\n",
    "        # self.labels = df[self.cfg.target_col].values\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def read_image_multiframe(self, idx):\n",
    "        base_path = self.base_paths[idx]\n",
    "\n",
    "        images = []\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "\n",
    "            image = self.cfg.video_cache[path]\n",
    "\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.read_image_multiframe(idx)\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image=image)['image']\n",
    "            replay = None\n",
    "            images = []\n",
    "            for img in image:\n",
    "                if replay is None:\n",
    "                    sample = self.transform(image=img)\n",
    "                    replay = sample['replay']\n",
    "                else:\n",
    "                    sample = ReplayCompose.replay(replay, image=img)\n",
    "                images.append(sample['image'])\n",
    "\n",
    "            image = torch.concat(images, dim=0)\n",
    "\n",
    "        if self.labels is None:\n",
    "            return image\n",
    "\n",
    "        if self.cfg.objective_cv == 'multiclass':\n",
    "            label = torch.tensor(self.labels[idx]).long()\n",
    "        else:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aug_video(train, cfg, plot_count=1):\n",
    "    transform = CFG.train_aug_list\n",
    "    transform = A.ReplayCompose(transform)\n",
    "\n",
    "    dataset = CustomDataset(\n",
    "        train, CFG, transform=transform)\n",
    "\n",
    "    for i in range(plot_count):\n",
    "        image = dataset.read_image_multiframe(i)\n",
    "\n",
    "        if cfg.use_gray_scale:\n",
    "            image = np.stack(image, axis=2)\n",
    "        else:\n",
    "            image = np.concatenate(image, axis=2)\n",
    "\n",
    "        aug_image = dataset[i]\n",
    "        # torch to numpy\n",
    "        aug_image = aug_image.permute(1, 2, 0).numpy()*255\n",
    "\n",
    "        for frame in range(image.shape[-1]):\n",
    "            if frame % 3 != 0:\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            if cfg.use_gray_scale:\n",
    "                axes[0].imshow(image[..., frame], cmap=\"gray\")\n",
    "                axes[1].imshow(aug_image[..., frame], cmap=\"gray\")\n",
    "            else:\n",
    "                axes[0].imshow(image[..., frame:frame+3].astype(int))\n",
    "                axes[1].imshow(aug_image[..., frame:frame+3].astype(int))\n",
    "            plt.savefig(cfg.figures_dir +\n",
    "                        f'aug_{i}_frame{frame}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "        # self.cfg = cfg\n",
    "\n",
    "        if model_name is None:\n",
    "            model_name = cfg.model_name\n",
    "\n",
    "        print(f'pretrained: {pretrained}')\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=0,\n",
    "            in_chans=cfg.model_in_chans)\n",
    "\n",
    "        # モデルの出力サイズを取得\n",
    "        if hasattr(self.model, 'num_features'):\n",
    "            self.n_features = self.model.num_features  # num_featuresで取得するモデルが多い\n",
    "        elif hasattr(self.model, 'classifier') and hasattr(self.model.classifier, 'in_features'):\n",
    "            self.n_features = self.model.classifier.in_features  # classifierが存在する場合\n",
    "        elif hasattr(self.model, 'fc') and hasattr(self.model.fc, 'in_features'):\n",
    "            self.n_features = self.model.fc.in_features  # fcが存在する場合\n",
    "        else:\n",
    "            raise AttributeError(\"Could not find the output feature size.\")\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "\n",
    "        # nn.Dropout(0.5),\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(self.n_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, self.target_size),\n",
    "        )\n",
    "\n",
    "    def feature(self, image):\n",
    "\n",
    "        feature = self.model(image)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.feature(image)\n",
    "        output = self.final_fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(\n",
    "            optimizer, multiplier, total_epoch, after_scheduler)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [\n",
    "                        base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "def get_scheduler(cfg, optimizer):\n",
    "    if cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=cfg.factor, patience=cfg.patience, verbose=True, eps=cfg.eps)\n",
    "    elif cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer, T_max=cfg.epochs, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=cfg.T_0, T_mult=1, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'GradualWarmupSchedulerV2':\n",
    "        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, cfg.epochs, eta_min=1e-7)\n",
    "        scheduler = GradualWarmupSchedulerV2(\n",
    "            optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def scheduler_step(scheduler, avg_val_loss, epoch):\n",
    "    if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        scheduler.step(avg_val_loss)\n",
    "    elif isinstance(scheduler, CosineAnnealingLR):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, GradualWarmupSchedulerV2):\n",
    "        scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device,\n",
    "             model_ema=None):\n",
    "    \"\"\" 1epoch毎のtrain \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.use_amp)\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    preds_labels = []\n",
    "    start = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with autocast(CFG.use_amp):\n",
    "            y_preds = model(images)\n",
    "\n",
    "            if y_preds.size(1) == 1:\n",
    "                y_preds = y_preds.view(-1)\n",
    "\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.detach().to('cpu').numpy())\n",
    "\n",
    "        preds_labels.append(labels.detach().to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(preds_labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        if y_preds.size(1) == 1:\n",
    "            y_preds = y_preds.view(-1)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # binary\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(folds, fold):\n",
    "\n",
    "    Logger.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    if CFG.use_alldata:\n",
    "        train_folds = folds.copy().reset_index(drop=True)\n",
    "    else:\n",
    "        train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # train_folds = train_downsampling(train_folds)\n",
    "\n",
    "    train_labels = train_folds[CFG.target_col].values\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        train_folds, CFG, labels=train_labels, transform=get_transforms(data='train', cfg=CFG))\n",
    "    valid_dataset = CustomDataset(\n",
    "        valid_folds, CFG, labels=valid_labels, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                              )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "\n",
    "    model = CustomModel(CFG, pretrained=CFG.pretrained)\n",
    "    model.to(device)\n",
    "\n",
    "    if CFG.use_ema:\n",
    "        model_ema = ModelEmaV2(model, decay=CFG.ema_decay)\n",
    "    else:\n",
    "        model_ema = None\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr)\n",
    "    scheduler = get_scheduler(CFG, optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    if CFG.objective_cv == 'binary':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif CFG.objective_cv == 'multiclass':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif CFG.objective_cv == 'regression':\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "    if CFG.metric_direction == 'minimize':\n",
    "        best_score = np.inf\n",
    "    elif CFG.metric_direction == 'maximize':\n",
    "        best_score = -1\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    df_score = pd.DataFrame(columns=[\"train_loss\", 'train_score', 'val_loss', 'val_score'])\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss, train_preds, train_labels_epoch = train_fn(fold, train_loader, model,\n",
    "                                                             criterion, optimizer, epoch, scheduler, device, model_ema)\n",
    "        train_score = get_score(train_labels_epoch, train_preds)\n",
    "\n",
    "        # eval\n",
    "        if model_ema is not None:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model_ema.module, criterion, device)\n",
    "        else:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model, criterion, device)\n",
    "\n",
    "        scheduler_step(scheduler, avg_val_loss, epoch)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, valid_preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        # Logger.info(f'Epoch {epoch+1} - avgScore: {avg_score:.4f}')\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_Score: {train_score:.4f} avgScore: {score:.4f}')\n",
    "        \n",
    "        df_score.loc[epoch] = [avg_loss, train_score, avg_val_loss, score]\n",
    "\n",
    "        if CFG.metric_direction == 'minimize':\n",
    "            update_best = score < best_score\n",
    "        elif CFG.metric_direction == 'maximize':\n",
    "            update_best = score > best_score\n",
    "\n",
    "        if update_best:\n",
    "            best_loss = avg_val_loss\n",
    "            best_score = score\n",
    "\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "\n",
    "            if model_ema is not None:\n",
    "                torch.save({'model': model_ema.module.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "            else:\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save({'model': model.state_dict(),\n",
    "                'preds': valid_preds},\n",
    "               CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    \"\"\"\n",
    "    if model_ema is not None:\n",
    "        torch.save({'model': model_ema.module.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    else:\n",
    "        torch.save({'model': model.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "\n",
    "    check_point = torch.load(\n",
    "        CFG.model_dir + f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth', map_location=torch.device('cpu'))\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "\n",
    "    check_point_pred = check_point['preds']\n",
    "\n",
    "    # Columns must be same length as key 対策\n",
    "    if check_point_pred.ndim == 1:\n",
    "        check_point_pred = check_point_pred.reshape(-1, CFG.target_size)\n",
    "\n",
    "    print('check_point_pred shape', check_point_pred.shape)\n",
    "    valid_folds[pred_cols] = check_point_pred\n",
    "    return valid_folds, df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = pd.read_csv(CFG.train_fold_dir + 'train_folds.csv')\n",
    "    train['ori_idx'] = train.index\n",
    "\n",
    "    train['scene'] = train['ID'].str.split('_').str[0]\n",
    "\n",
    "    \"\"\"\n",
    "    if CFG.is_debug:\n",
    "        use_ids = train['scene'].unique()[:100]\n",
    "        train = train[train['scene'].isin(use_ids)].reset_index(drop=True)\n",
    "    \"\"\"\n",
    "\n",
    "    train['base_path'] = CFG.comp_dataset_path + 'images/' + train['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in train['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    # plot_aug_video(train, CFG, plot_count=10)\n",
    "\n",
    "    # train\n",
    "    oof_df = pd.DataFrame()\n",
    "    list_df_score = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold not in CFG.train_folds:\n",
    "            print(f'fold {fold} is skipped')\n",
    "            continue\n",
    "\n",
    "        _oof_df, _df_score = train_fold(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        list_df_score.append(_df_score)\n",
    "        Logger.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "\n",
    "        if CFG.use_holdout or CFG.use_alldata:\n",
    "            break\n",
    "\n",
    "    oof_df = oof_df.sort_values('ori_idx').reset_index(drop=True)\n",
    "\n",
    "    # CV result\n",
    "    Logger.info(\"========== CV ==========\")\n",
    "    score = get_result(oof_df)\n",
    "\n",
    "    # 学習曲線を可視化する\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.grid(alpha=0.1)\n",
    "    ax2.grid(alpha=0.1)\n",
    "    for i, df_score in enumerate(list_df_score):\n",
    "        ax1.plot(df_score['train_score'], label=f'fold {i}')\n",
    "        ax2.plot(df_score['val_score'], label=f'fold {i}')\n",
    "    ax1.set_title('Train Score')\n",
    "    ax2.set_title('Val Score') \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Score')\n",
    "    ax2.set_ylabel('Val Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CFG.figures_dir + f'learning_curve_{CFG.exp_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # save result\n",
    "    oof_df.to_csv(CFG.submission_dir + 'oof_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-0.5.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e071289e95543dc8219a3a92707ded4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 2s (remain 20m 28s) Loss: 6.0125(6.0125) Grad: inf  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 4.6669(5.2647) Grad: 21603.8887  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 4.4345(5.2420) Grad: 23755.6484  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 4.7394(4.7394) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 5.2420  avg_val_loss: 5.2858  time: 131s\n",
      "Epoch 1 - avg_train_Score: 5.2420 avgScore: 5.2858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.8063(5.2858) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Save Best Score: 5.2858 Model\n",
      "Epoch 1 - Save Best Loss: 5.2858 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 11m 45s) Loss: 5.1981(5.1981) Grad: 301958.0625  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 4.1290(4.9653) Grad: 50945.7852  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 4.5313(4.9551) Grad: 75085.0938  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 4.4533(4.4533) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 4.9551  avg_val_loss: 4.9773  time: 130s\n",
      "Epoch 2 - avg_train_Score: 4.9551 avgScore: 4.9773\n",
      "Epoch 2 - Save Best Score: 4.9773 Model\n",
      "Epoch 2 - Save Best Loss: 4.9773 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.5242(4.9773) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 11m 48s) Loss: 5.6882(5.6882) Grad: 210391.9844  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 2.5645(3.8669) Grad: 20414.2148  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 2.1014(3.7632) Grad: 22071.3926  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 2.6025(2.6025) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 3.7632  avg_val_loss: 3.1532  time: 130s\n",
      "Epoch 3 - avg_train_Score: 3.7632 avgScore: 3.1532\n",
      "Epoch 3 - Save Best Score: 3.1532 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 3.0079(3.1532) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Save Best Loss: 3.1532 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 12m 19s) Loss: 2.4214(2.4214) Grad: 516767.5938  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.3973(1.7476) Grad: 53443.2500  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.4580(1.7240) Grad: 18416.2344  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 1.1237(1.1237) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.7240  avg_val_loss: 1.2563  time: 130s\n",
      "Epoch 4 - avg_train_Score: 1.7240 avgScore: 1.2563\n",
      "Epoch 4 - Save Best Score: 1.2563 Model\n",
      "Epoch 4 - Save Best Loss: 1.2563 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 1.2275(1.2563) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 1.2583(1.2583) Grad: 275720.1250  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.4816(1.3040) Grad: 44387.6406  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0132(1.3064) Grad: 30606.7188  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 1.0150(1.0150) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.3064  avg_val_loss: 1.0319  time: 130s\n",
      "Epoch 5 - avg_train_Score: 1.3064 avgScore: 1.0319\n",
      "Epoch 5 - Save Best Score: 1.0319 Model\n",
      "Epoch 5 - Save Best Loss: 1.0319 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0594(1.0319) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 12m 5s) Loss: 1.5693(1.5693) Grad: 392394.1875  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.1368(1.1847) Grad: 80076.7422  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.6822(1.1832) Grad: 87447.5391  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.9628(0.9628) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 1.1832  avg_val_loss: 0.9794  time: 130s\n",
      "Epoch 6 - avg_train_Score: 1.1832 avgScore: 0.9794\n",
      "Epoch 6 - Save Best Score: 0.9794 Model\n",
      "Epoch 6 - Save Best Loss: 0.9794 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9857(0.9794) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 1.0242(1.0242) Grad: 434281.1250  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9891(1.1572) Grad: 16317.4697  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.1288(1.1622) Grad: 15975.4336  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.9291(0.9291) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 1.1622  avg_val_loss: 0.9698  time: 130s\n",
      "Epoch 7 - avg_train_Score: 1.1622 avgScore: 0.9698\n",
      "Epoch 7 - Save Best Score: 0.9698 Model\n",
      "Epoch 7 - Save Best Loss: 0.9698 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9905(0.9698) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 12m 7s) Loss: 1.2593(1.2593) Grad: 323271.7188  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 1.0299(1.1001) Grad: 33503.8477  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2650(1.0994) Grad: 26333.3848  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.8699(0.8699) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 1.0994  avg_val_loss: 0.9380  time: 129s\n",
      "Epoch 8 - avg_train_Score: 1.0994 avgScore: 0.9380\n",
      "Epoch 8 - Save Best Score: 0.9380 Model\n",
      "Epoch 8 - Save Best Loss: 0.9380 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.9522(0.9380) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 12m 51s) Loss: 0.8201(0.8201) Grad: 246771.5000  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0174(1.0237) Grad: 26145.3652  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8406(1.0247) Grad: 33598.2539  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8676(0.8676) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 1.0247  avg_val_loss: 0.9093  time: 130s\n",
      "Epoch 9 - avg_train_Score: 1.0247 avgScore: 0.9093\n",
      "Epoch 9 - Save Best Score: 0.9093 Model\n",
      "Epoch 9 - Save Best Loss: 0.9093 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9437(0.9093) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 12m 15s) Loss: 0.9344(0.9344) Grad: 280299.2812  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0596(0.9748) Grad: 67658.0781  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8267(0.9735) Grad: 54645.0859  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.8847(0.8847) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.9735  avg_val_loss: 0.8968  time: 130s\n",
      "Epoch 10 - avg_train_Score: 0.9735 avgScore: 0.8968\n",
      "Epoch 10 - Save Best Score: 0.8968 Model\n",
      "Epoch 10 - Save Best Loss: 0.8968 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9024(0.8968) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 12m 32s) Loss: 0.7831(0.7831) Grad: 282077.5000  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9216(0.9285) Grad: 35450.9297  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9786(0.9319) Grad: 28746.5547  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 0.8633(0.8633) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9014(0.8807) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.9319  avg_val_loss: 0.8807  time: 130s\n",
      "Epoch 11 - avg_train_Score: 0.9319 avgScore: 0.8807\n",
      "Epoch 11 - Save Best Score: 0.8807 Model\n",
      "Epoch 11 - Save Best Loss: 0.8807 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 12m 23s) Loss: 0.8734(0.8734) Grad: 206552.1875  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2348(0.9504) Grad: 33138.2227  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8886(0.9485) Grad: 33779.0312  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.8861(0.8861) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.9485  avg_val_loss: 0.8779  time: 130s\n",
      "Epoch 12 - avg_train_Score: 0.9485 avgScore: 0.8779\n",
      "Epoch 12 - Save Best Score: 0.8779 Model\n",
      "Epoch 12 - Save Best Loss: 0.8779 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9125(0.8779) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 11m 59s) Loss: 0.8950(0.8950) Grad: 239911.4375  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7091(0.8924) Grad: 45141.5117  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9193(0.8947) Grad: 70701.9453  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8516(0.8516) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.8947  avg_val_loss: 0.8688  time: 130s\n",
      "Epoch 13 - avg_train_Score: 0.8947 avgScore: 0.8688\n",
      "Epoch 13 - Save Best Score: 0.8688 Model\n",
      "Epoch 13 - Save Best Loss: 0.8688 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8773(0.8688) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 12m 5s) Loss: 0.7548(0.7548) Grad: 345312.4375  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7697(0.8955) Grad: 72800.3438  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7918(0.8940) Grad: 55547.1289  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.8405(0.8405) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.8940  avg_val_loss: 0.8644  time: 130s\n",
      "Epoch 14 - avg_train_Score: 0.8940 avgScore: 0.8644\n",
      "Epoch 14 - Save Best Score: 0.8644 Model\n",
      "Epoch 14 - Save Best Loss: 0.8644 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8794(0.8644) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 1.0050(1.0050) Grad: 225660.8438  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7820(0.8693) Grad: 116637.6328  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.1200(0.8734) Grad: 46483.5039  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8360(0.8360) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8622(0.8562) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.8734  avg_val_loss: 0.8562  time: 130s\n",
      "Epoch 15 - avg_train_Score: 0.8734 avgScore: 0.8562\n",
      "Epoch 15 - Save Best Score: 0.8562 Model\n",
      "Epoch 15 - Save Best Loss: 0.8562 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 12m 27s) Loss: 0.8178(0.8178) Grad: 295174.3750  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 1.1493(0.8544) Grad: 57796.5000  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7147(0.8542) Grad: 53409.2109  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8268(0.8268) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8573(0.8542) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.8542  avg_val_loss: 0.8542  time: 129s\n",
      "Epoch 16 - avg_train_Score: 0.8542 avgScore: 0.8542\n",
      "Epoch 16 - Save Best Score: 0.8542 Model\n",
      "Epoch 16 - Save Best Loss: 0.8542 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 0.7346(0.7346) Grad: 535046.5000  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9841(0.8397) Grad: 23429.5859  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7450(0.8399) Grad: 33434.1250  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8192(0.8192) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8498(0.8481) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.8399  avg_val_loss: 0.8481  time: 130s\n",
      "Epoch 17 - avg_train_Score: 0.8399 avgScore: 0.8481\n",
      "Epoch 17 - Save Best Score: 0.8481 Model\n",
      "Epoch 17 - Save Best Loss: 0.8481 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 12m 22s) Loss: 0.8950(0.8950) Grad: 289149.8125  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.6363(0.8568) Grad: 28002.0645  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7936(0.8565) Grad: 47823.4102  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8360(0.8360) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.8565  avg_val_loss: 0.8497  time: 130s\n",
      "Epoch 18 - avg_train_Score: 0.8565 avgScore: 0.8497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8727(0.8497) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 13m 1s) Loss: 0.8052(0.8052) Grad: 334409.1250  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7830(0.8033) Grad: 49084.5039  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7674(0.8020) Grad: 69197.3359  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.8129(0.8129) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8425(0.8382) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.8020  avg_val_loss: 0.8382  time: 130s\n",
      "Epoch 19 - avg_train_Score: 0.8020 avgScore: 0.8382\n",
      "Epoch 19 - Save Best Score: 0.8382 Model\n",
      "Epoch 19 - Save Best Loss: 0.8382 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 12m 35s) Loss: 1.0387(1.0387) Grad: 160255.1875  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6812(0.7847) Grad: 62550.2695  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7287(0.7843) Grad: 78707.3750  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7974(0.7974) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.7843  avg_val_loss: 0.8394  time: 130s\n",
      "Epoch 20 - avg_train_Score: 0.7843 avgScore: 0.8394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8361(0.8394) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 12m 5s) Loss: 0.5759(0.5759) Grad: 225143.9375  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.9383(0.8140) Grad: 47688.3711  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7311(0.8120) Grad: 25439.8438  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.8015(0.8015) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8380(0.8398) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.8120  avg_val_loss: 0.8398  time: 130s\n",
      "Epoch 21 - avg_train_Score: 0.8120 avgScore: 0.8398\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 11m 51s) Loss: 0.7014(0.7014) Grad: 204528.5938  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7570(0.7729) Grad: 106811.8750  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6880(0.7732) Grad: 87959.7109  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7925(0.7925) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8244(0.8341) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.7732  avg_val_loss: 0.8341  time: 130s\n",
      "Epoch 22 - avg_train_Score: 0.7732 avgScore: 0.8341\n",
      "Epoch 22 - Save Best Score: 0.8341 Model\n",
      "Epoch 22 - Save Best Loss: 0.8341 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 12m 5s) Loss: 1.3625(1.3625) Grad: 604404.5000  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5753(0.7573) Grad: 122864.5938  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6645(0.7564) Grad: 84172.1406  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.7964(0.7964) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.7564  avg_val_loss: 0.8315  time: 130s\n",
      "Epoch 23 - avg_train_Score: 0.7564 avgScore: 0.8315\n",
      "Epoch 23 - Save Best Score: 0.8315 Model\n",
      "Epoch 23 - Save Best Loss: 0.8315 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8157(0.8315) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 12m 44s) Loss: 0.5445(0.5445) Grad: 180028.4844  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7531(0.7477) Grad: 41840.0000  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5894(0.7527) Grad: 98471.5625  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7944(0.7944) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.7527  avg_val_loss: 0.8300  time: 130s\n",
      "Epoch 24 - avg_train_Score: 0.7527 avgScore: 0.8300\n",
      "Epoch 24 - Save Best Score: 0.8300 Model\n",
      "Epoch 24 - Save Best Loss: 0.8300 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8067(0.8300) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 12m 31s) Loss: 0.7599(0.7599) Grad: 404482.8438  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.0364(0.7411) Grad: 95134.2734  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7848(0.7398) Grad: 107364.7188  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7715(0.7715) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.7398  avg_val_loss: 0.8286  time: 131s\n",
      "Epoch 25 - avg_train_Score: 0.7398 avgScore: 0.8286\n",
      "Epoch 25 - Save Best Score: 0.8286 Model\n",
      "Epoch 25 - Save Best Loss: 0.8286 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8053(0.8286) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 0.8250(0.8250) Grad: 188428.2812  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5555(0.7300) Grad: 94749.5703  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7295(0.7259) Grad: 83202.5859  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7942(0.7942) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.7259  avg_val_loss: 0.8269  time: 130s\n",
      "Epoch 26 - avg_train_Score: 0.7259 avgScore: 0.8269\n",
      "Epoch 26 - Save Best Score: 0.8269 Model\n",
      "Epoch 26 - Save Best Loss: 0.8269 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8016(0.8269) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 13m 9s) Loss: 0.6155(0.6155) Grad: 224099.6875  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0667(0.7362) Grad: 40301.5977  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6404(0.7389) Grad: 24280.4609  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7841(0.7841) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8029(0.8274) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.7389  avg_val_loss: 0.8274  time: 130s\n",
      "Epoch 27 - avg_train_Score: 0.7389 avgScore: 0.8274\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 12m 51s) Loss: 0.7563(0.7563) Grad: 237540.1094  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6534(0.7233) Grad: 101982.6328  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7270(0.7188) Grad: 102246.5234  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.8168(0.8168) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.7188  avg_val_loss: 0.8255  time: 130s\n",
      "Epoch 28 - avg_train_Score: 0.7188 avgScore: 0.8255\n",
      "Epoch 28 - Save Best Score: 0.8255 Model\n",
      "Epoch 28 - Save Best Loss: 0.8255 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8121(0.8255) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 12m 40s) Loss: 1.0346(1.0346) Grad: 169125.6250  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8726(0.7132) Grad: 47342.1914  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.3712(0.7170) Grad: 46876.2109  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8052(0.8052) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.7170  avg_val_loss: 0.8237  time: 130s\n",
      "Epoch 29 - avg_train_Score: 0.7170 avgScore: 0.8237\n",
      "Epoch 29 - Save Best Score: 0.8237 Model\n",
      "Epoch 29 - Save Best Loss: 0.8237 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8161(0.8237) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 12m 13s) Loss: 0.6995(0.6995) Grad: 224716.4531  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.7022(0.7153) Grad: 39264.2617  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5138(0.7152) Grad: 69308.5078  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7913(0.7913) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.7152  avg_val_loss: 0.8227  time: 129s\n",
      "Epoch 30 - avg_train_Score: 0.7152 avgScore: 0.8227\n",
      "Epoch 30 - Save Best Score: 0.8227 Model\n",
      "Epoch 30 - Save Best Loss: 0.8227 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8136(0.8227) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 12m 57s) Loss: 0.5799(0.5799) Grad: 167623.3125  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5820(0.7089) Grad: 108959.2656  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8809(0.7076) Grad: 119887.9922  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7817(0.7817) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.7076  avg_val_loss: 0.8237  time: 130s\n",
      "Epoch 31 - avg_train_Score: 0.7076 avgScore: 0.8237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8036(0.8237) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 11m 54s) Loss: 0.5409(0.5409) Grad: 218411.0469  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6122(0.7005) Grad: 87228.8906  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6524(0.6993) Grad: 83961.8438  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7863(0.7863) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.6993  avg_val_loss: 0.8210  time: 130s\n",
      "Epoch 32 - avg_train_Score: 0.6993 avgScore: 0.8210\n",
      "Epoch 32 - Save Best Score: 0.8210 Model\n",
      "Epoch 32 - Save Best Loss: 0.8210 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7940(0.8210) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 11m 58s) Loss: 0.7670(0.7670) Grad: 179023.7344  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.5830(0.6963) Grad: 62721.8164  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8888(0.6972) Grad: 19444.1953  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.7889(0.7889) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.6972  avg_val_loss: 0.8200  time: 130s\n",
      "Epoch 33 - avg_train_Score: 0.6972 avgScore: 0.8200\n",
      "Epoch 33 - Save Best Score: 0.8200 Model\n",
      "Epoch 33 - Save Best Loss: 0.8200 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8038(0.8200) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 0.5934(0.5934) Grad: 153365.9844  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5220(0.6951) Grad: 86570.0391  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6767(0.6937) Grad: 116093.1719  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7922(0.7922) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.6937  avg_val_loss: 0.8215  time: 130s\n",
      "Epoch 34 - avg_train_Score: 0.6937 avgScore: 0.8215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8113(0.8215) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 12m 8s) Loss: 0.7536(0.7536) Grad: 190961.4844  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8067(0.6948) Grad: 54052.1523  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.4962(0.6949) Grad: 44611.0508  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7868(0.7868) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.6949  avg_val_loss: 0.8203  time: 130s\n",
      "Epoch 35 - avg_train_Score: 0.6949 avgScore: 0.8203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8105(0.8203) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 12m 33s) Loss: 0.7199(0.7199) Grad: 240363.8906  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7754(0.6892) Grad: 36200.9531  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.3165(0.6921) Grad: 34283.6602  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7770(0.7770) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8136(0.8197) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.6921  avg_val_loss: 0.8197  time: 130s\n",
      "Epoch 36 - avg_train_Score: 0.6921 avgScore: 0.8197\n",
      "Epoch 36 - Save Best Score: 0.8197 Model\n",
      "Epoch 36 - Save Best Loss: 0.8197 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 12m 41s) Loss: 1.0488(1.0488) Grad: 180273.1562  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7642(0.6894) Grad: 46057.7578  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6128(0.6883) Grad: 47172.8125  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7746(0.7746) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.6883  avg_val_loss: 0.8209  time: 130s\n",
      "Epoch 37 - avg_train_Score: 0.6883 avgScore: 0.8209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8109(0.8209) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 12m 8s) Loss: 0.6758(0.6758) Grad: 235092.4688  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6738(0.6783) Grad: 37516.9570  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5765(0.6771) Grad: 51962.7891  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7750(0.7750) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.6771  avg_val_loss: 0.8196  time: 130s\n",
      "Epoch 38 - avg_train_Score: 0.6771 avgScore: 0.8196\n",
      "Epoch 38 - Save Best Score: 0.8196 Model\n",
      "Epoch 38 - Save Best Loss: 0.8196 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8151(0.8196) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 12m 6s) Loss: 0.6320(0.6320) Grad: 406282.5312  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5740(0.6774) Grad: 100932.4922  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6416(0.6797) Grad: 87751.0156  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.7703(0.7703) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8084(0.8205) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.6797  avg_val_loss: 0.8205  time: 130s\n",
      "Epoch 39 - avg_train_Score: 0.6797 avgScore: 0.8205\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 0.7657(0.7657) Grad: 284216.8438  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5748(0.6782) Grad: 55797.1445  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5588(0.6801) Grad: 46014.0234  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.7734(0.7734) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.6801  avg_val_loss: 0.8188  time: 130s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8124(0.8188) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 - avg_train_Score: 0.6801 avgScore: 0.8188\n",
      "Epoch 40 - Save Best Score: 0.8188 Model\n",
      "Epoch 40 - Save Best Loss: 0.8188 Model\n",
      "/tmp/ipykernel_217129/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 0 result ==========\n",
      "score: 0.8188\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8675, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 4.9714(4.9714) Grad: 253322.9219  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 5.4985(5.3729) Grad: 19259.7363  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 5.0047(5.3566) Grad: 36616.0508  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 5.4739(5.4739) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 5.3566  avg_val_loss: 5.1603  time: 130s\n",
      "Epoch 1 - avg_train_Score: 5.3566 avgScore: 5.1603\n",
      "Epoch 1 - Save Best Score: 5.1603 Model\n",
      "Epoch 1 - Save Best Loss: 5.1603 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 3.5336(5.1603) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 12m 26s) Loss: 5.9214(5.9214) Grad: 174646.0625  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 4.7860(5.0537) Grad: 92213.6641  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 5.8172(5.0447) Grad: 74642.6406  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 5.1247(5.1247) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 5.0447  avg_val_loss: 4.7939  time: 130s\n",
      "Epoch 2 - avg_train_Score: 5.0447 avgScore: 4.7939\n",
      "Epoch 2 - Save Best Score: 4.7939 Model\n",
      "Epoch 2 - Save Best Loss: 4.7939 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 3.2772(4.7939) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 12m 18s) Loss: 5.0510(5.0510) Grad: 543450.8125  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 3.0723(3.9287) Grad: 30440.5977  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 2.5823(3.8165) Grad: 99182.4219  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 49s) Loss: 3.6847(3.6847) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 3.8165  avg_val_loss: 3.3023  time: 130s\n",
      "Epoch 3 - avg_train_Score: 3.8165 avgScore: 3.3023\n",
      "Epoch 3 - Save Best Score: 3.3023 Model\n",
      "Epoch 3 - Save Best Loss: 3.3023 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 2.2829(3.3023) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 12m 39s) Loss: 2.0585(2.0585) Grad: 322928.1875  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.5667(1.8036) Grad: 36432.6094  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.5417(1.7789) Grad: 23742.2207  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 1.4518(1.4518) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.7789  avg_val_loss: 1.2184  time: 130s\n",
      "Epoch 4 - avg_train_Score: 1.7789 avgScore: 1.2184\n",
      "Epoch 4 - Save Best Score: 1.2184 Model\n",
      "Epoch 4 - Save Best Loss: 1.2184 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.1793(1.2184) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 12m 23s) Loss: 2.0776(2.0776) Grad: 334082.9062  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9761(1.2995) Grad: 50978.3047  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.4424(1.2966) Grad: 130100.1719  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 1.1631(1.1631) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0186(1.0128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.2966  avg_val_loss: 1.0128  time: 130s\n",
      "Epoch 5 - avg_train_Score: 1.2966 avgScore: 1.0128\n",
      "Epoch 5 - Save Best Score: 1.0128 Model\n",
      "Epoch 5 - Save Best Loss: 1.0128 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 14m 31s) Loss: 1.1023(1.1023) Grad: 347664.6562  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0097(1.1898) Grad: 120453.4219  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2523(1.1882) Grad: 241289.6406  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 51s) Loss: 1.0879(1.0879) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9319(0.9605) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 1.1882  avg_val_loss: 0.9605  time: 131s\n",
      "Epoch 6 - avg_train_Score: 1.1882 avgScore: 0.9605\n",
      "Epoch 6 - Save Best Score: 0.9605 Model\n",
      "Epoch 6 - Save Best Loss: 0.9605 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 12m 57s) Loss: 1.1523(1.1523) Grad: 432864.1562  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.5409(1.1506) Grad: 54957.1289  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.3154(1.1517) Grad: 82614.2266  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 1.0687(1.0687) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 1.1517  avg_val_loss: 0.9422  time: 130s\n",
      "Epoch 7 - avg_train_Score: 1.1517 avgScore: 0.9422\n",
      "Epoch 7 - Save Best Score: 0.9422 Model\n",
      "Epoch 7 - Save Best Loss: 0.9422 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8969(0.9422) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 12m 37s) Loss: 0.9635(0.9635) Grad: 226846.7500  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7589(1.0857) Grad: 139164.1562  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.3961(1.0836) Grad: 73723.5469  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 1.0273(1.0273) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 1.0836  avg_val_loss: 0.9145  time: 130s\n",
      "Epoch 8 - avg_train_Score: 1.0836 avgScore: 0.9145\n",
      "Epoch 8 - Save Best Score: 0.9145 Model\n",
      "Epoch 8 - Save Best Loss: 0.9145 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8641(0.9145) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 12m 33s) Loss: 1.0048(1.0048) Grad: 353981.8750  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2072(1.0888) Grad: 52103.8359  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2347(1.0954) Grad: 48542.6250  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.9894(0.9894) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 1.0954  avg_val_loss: 0.9081  time: 130s\n",
      "Epoch 9 - avg_train_Score: 1.0954 avgScore: 0.9081\n",
      "Epoch 9 - Save Best Score: 0.9081 Model\n",
      "Epoch 9 - Save Best Loss: 0.9081 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8338(0.9081) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 0.8409(0.8409) Grad: 286513.5625  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9209(1.0297) Grad: 80876.9219  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0303(1.0302) Grad: 52927.2227  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.9271(0.9271) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 1.0302  avg_val_loss: 0.8835  time: 130s\n",
      "Epoch 10 - avg_train_Score: 1.0302 avgScore: 0.8835\n",
      "Epoch 10 - Save Best Score: 0.8835 Model\n",
      "Epoch 10 - Save Best Loss: 0.8835 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8073(0.8835) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 12m 14s) Loss: 0.9866(0.9866) Grad: 322541.0312  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.1386(1.0101) Grad: 54484.5000  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9034(1.0090) Grad: 72795.4766  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.8825(0.8825) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 1.0090  avg_val_loss: 0.8712  time: 130s\n",
      "Epoch 11 - avg_train_Score: 1.0090 avgScore: 0.8712\n",
      "Epoch 11 - Save Best Score: 0.8712 Model\n",
      "Epoch 11 - Save Best Loss: 0.8712 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8216(0.8712) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 12m 13s) Loss: 0.8766(0.8766) Grad: 260448.6094  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8674(0.9569) Grad: 77229.5859  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8734(0.9609) Grad: 76707.9375  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.9033(0.9033) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.9609  avg_val_loss: 0.8549  time: 130s\n",
      "Epoch 12 - avg_train_Score: 0.9609 avgScore: 0.8549\n",
      "Epoch 12 - Save Best Score: 0.8549 Model\n",
      "Epoch 12 - Save Best Loss: 0.8549 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8102(0.8549) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 12m 13s) Loss: 0.8830(0.8830) Grad: nan  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8999(0.9913) Grad: 47374.1133  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0202(0.9937) Grad: 31187.5605  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 0.8985(0.8985) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.9937  avg_val_loss: 0.8472  time: 130s\n",
      "Epoch 13 - avg_train_Score: 0.9937 avgScore: 0.8472\n",
      "Epoch 13 - Save Best Score: 0.8472 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8020(0.8472) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Save Best Loss: 0.8472 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 12m 44s) Loss: 1.0358(1.0358) Grad: 346807.7188  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.9683(0.9290) Grad: 61748.9766  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7617(0.9266) Grad: 83613.3828  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 0.8879(0.8879) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.9266  avg_val_loss: 0.8418  time: 130s\n",
      "Epoch 14 - avg_train_Score: 0.9266 avgScore: 0.8418\n",
      "Epoch 14 - Save Best Score: 0.8418 Model\n",
      "Epoch 14 - Save Best Loss: 0.8418 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7886(0.8418) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 12m 7s) Loss: 1.0240(1.0240) Grad: nan  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8897(0.8958) Grad: 53011.7852  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7585(0.8988) Grad: 91373.5469  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.8862(0.8862) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7433(0.8220) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.8988  avg_val_loss: 0.8220  time: 130s\n",
      "Epoch 15 - avg_train_Score: 0.8988 avgScore: 0.8220\n",
      "Epoch 15 - Save Best Score: 0.8220 Model\n",
      "Epoch 15 - Save Best Loss: 0.8220 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 0.7295(0.7295) Grad: 331729.3438  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.8620(0.8718) Grad: 71500.0078  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8277(0.8733) Grad: 79477.2422  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.9272(0.9272) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.8733  avg_val_loss: 0.8272  time: 130s\n",
      "Epoch 16 - avg_train_Score: 0.8733 avgScore: 0.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7500(0.8272) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 12m 40s) Loss: 0.8762(0.8762) Grad: 309773.9375  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7431(0.8594) Grad: 64076.3945  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6653(0.8570) Grad: 48313.7656  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8858(0.8858) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.8570  avg_val_loss: 0.8187  time: 130s\n",
      "Epoch 17 - avg_train_Score: 0.8570 avgScore: 0.8187\n",
      "Epoch 17 - Save Best Score: 0.8187 Model\n",
      "Epoch 17 - Save Best Loss: 0.8187 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7491(0.8187) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 0.8745(0.8745) Grad: 247091.2656  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0442(0.8481) Grad: 48077.5430  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8650(0.8477) Grad: 85775.3438  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8830(0.8830) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7424(0.8199) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.8477  avg_val_loss: 0.8199  time: 130s\n",
      "Epoch 18 - avg_train_Score: 0.8477 avgScore: 0.8199\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 12m 24s) Loss: 0.7652(0.7652) Grad: 296408.6562  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8552(0.8027) Grad: 100542.2656  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6769(0.8047) Grad: 119274.5312  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.8821(0.8821) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.8047  avg_val_loss: 0.8112  time: 130s\n",
      "Epoch 19 - avg_train_Score: 0.8047 avgScore: 0.8112\n",
      "Epoch 19 - Save Best Score: 0.8112 Model\n",
      "Epoch 19 - Save Best Loss: 0.8112 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7105(0.8112) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 12m 19s) Loss: 0.7617(0.7617) Grad: 227201.6094  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0377(0.8231) Grad: 63743.8125  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6534(0.8242) Grad: 73369.7500  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.8589(0.8589) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.8242  avg_val_loss: 0.8059  time: 130s\n",
      "Epoch 20 - avg_train_Score: 0.8242 avgScore: 0.8059\n",
      "Epoch 20 - Save Best Score: 0.8059 Model\n",
      "Epoch 20 - Save Best Loss: 0.8059 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7125(0.8059) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 12m 19s) Loss: 0.7372(0.7372) Grad: 218736.1094  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0456(0.8271) Grad: 30781.2988  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8625(0.8293) Grad: 41021.9375  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.8509(0.8509) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.8293  avg_val_loss: 0.8087  time: 130s\n",
      "Epoch 21 - avg_train_Score: 0.8293 avgScore: 0.8087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7252(0.8087) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.7661(0.7661) Grad: 194392.5000  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7029(0.7999) Grad: 53016.4180  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9222(0.8027) Grad: 60636.1250  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.8544(0.8544) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.8027  avg_val_loss: 0.8043  time: 130s\n",
      "Epoch 22 - avg_train_Score: 0.8027 avgScore: 0.8043\n",
      "Epoch 22 - Save Best Score: 0.8043 Model\n",
      "Epoch 22 - Save Best Loss: 0.8043 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7549(0.8043) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 12m 19s) Loss: 0.6888(0.6888) Grad: 218315.0625  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6604(0.7749) Grad: 72508.9141  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8577(0.7777) Grad: 106966.4844  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 0.8434(0.8434) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7516(0.7996) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.7777  avg_val_loss: 0.7996  time: 130s\n",
      "Epoch 23 - avg_train_Score: 0.7777 avgScore: 0.7996\n",
      "Epoch 23 - Save Best Score: 0.7996 Model\n",
      "Epoch 23 - Save Best Loss: 0.7996 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 12m 27s) Loss: 0.7179(0.7179) Grad: 257537.0156  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.6347(0.7664) Grad: 27849.1426  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6288(0.7699) Grad: 30258.0449  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.8289(0.8289) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.7699  avg_val_loss: 0.7989  time: 130s\n",
      "Epoch 24 - avg_train_Score: 0.7699 avgScore: 0.7989\n",
      "Epoch 24 - Save Best Score: 0.7989 Model\n",
      "Epoch 24 - Save Best Loss: 0.7989 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7351(0.7989) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 12m 22s) Loss: 0.8725(0.8725) Grad: 180544.6250  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9192(0.7593) Grad: 47903.7617  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6019(0.7575) Grad: 57314.3672  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.8370(0.8370) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.7575  avg_val_loss: 0.7957  time: 130s\n",
      "Epoch 25 - avg_train_Score: 0.7575 avgScore: 0.7957\n",
      "Epoch 25 - Save Best Score: 0.7957 Model\n",
      "Epoch 25 - Save Best Loss: 0.7957 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7034(0.7957) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 12m 23s) Loss: 0.5181(0.5181) Grad: 223732.5625  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.5657(0.7547) Grad: 32631.4551  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6700(0.7531) Grad: 28500.9980  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.8326(0.8326) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7061(0.7917) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.7531  avg_val_loss: 0.7917  time: 130s\n",
      "Epoch 26 - avg_train_Score: 0.7531 avgScore: 0.7917\n",
      "Epoch 26 - Save Best Score: 0.7917 Model\n",
      "Epoch 26 - Save Best Loss: 0.7917 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 12m 39s) Loss: 0.6480(0.6480) Grad: 222194.3594  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6565(0.7362) Grad: 55869.6602  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6407(0.7362) Grad: 53286.3984  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.8304(0.8304) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.7362  avg_val_loss: 0.7914  time: 130s\n",
      "Epoch 27 - avg_train_Score: 0.7362 avgScore: 0.7914\n",
      "Epoch 27 - Save Best Score: 0.7914 Model\n",
      "Epoch 27 - Save Best Loss: 0.7914 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7176(0.7914) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 12m 56s) Loss: 0.8397(0.8397) Grad: 484140.0938  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.7891(0.7301) Grad: 40499.7734  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6271(0.7336) Grad: 30714.4590  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.8281(0.8281) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.7336  avg_val_loss: 0.7883  time: 130s\n",
      "Epoch 28 - avg_train_Score: 0.7336 avgScore: 0.7883\n",
      "Epoch 28 - Save Best Score: 0.7883 Model\n",
      "Epoch 28 - Save Best Loss: 0.7883 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7035(0.7883) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 12m 5s) Loss: 0.6392(0.6392) Grad: 230496.8438  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9897(0.7402) Grad: 17230.8203  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8181(0.7399) Grad: 10417.8096  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8302(0.8302) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.7399  avg_val_loss: 0.7923  time: 130s\n",
      "Epoch 29 - avg_train_Score: 0.7399 avgScore: 0.7923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.6899(0.7923) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 12m 37s) Loss: 0.7199(0.7199) Grad: 245065.6406  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6283(0.7208) Grad: 73906.9922  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9036(0.7212) Grad: 62714.0938  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.8175(0.8175) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.7212  avg_val_loss: 0.7931  time: 130s\n",
      "Epoch 30 - avg_train_Score: 0.7212 avgScore: 0.7931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.6821(0.7931) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 12m 18s) Loss: 0.5747(0.5747) Grad: 205881.2031  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6129(0.7124) Grad: 49530.2148  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6169(0.7134) Grad: 45893.1953  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.8234(0.8234) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.7134  avg_val_loss: 0.7891  time: 130s\n",
      "Epoch 31 - avg_train_Score: 0.7134 avgScore: 0.7891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.6794(0.7891) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 13m 9s) Loss: 0.6985(0.6985) Grad: 173610.8281  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6860(0.7056) Grad: 26974.6387  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7157(0.7081) Grad: 25438.7617  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8252(0.8252) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.7081  avg_val_loss: 0.7879  time: 130s\n",
      "Epoch 32 - avg_train_Score: 0.7081 avgScore: 0.7879\n",
      "Epoch 32 - Save Best Score: 0.7879 Model\n",
      "Epoch 32 - Save Best Loss: 0.7879 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7010(0.7879) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 12m 55s) Loss: 0.6176(0.6176) Grad: 219075.7344  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8484(0.7039) Grad: 27173.5254  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6678(0.7066) Grad: 24142.8594  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.8179(0.8179) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.7066  avg_val_loss: 0.7843  time: 130s\n",
      "Epoch 33 - avg_train_Score: 0.7066 avgScore: 0.7843\n",
      "Epoch 33 - Save Best Score: 0.7843 Model\n",
      "Epoch 33 - Save Best Loss: 0.7843 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7173(0.7843) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 0.6010(0.6010) Grad: 309320.8125  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.5386(0.6881) Grad: 52432.6484  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5578(0.6895) Grad: 47111.0391  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.8273(0.8273) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.6895  avg_val_loss: 0.7874  time: 130s\n",
      "Epoch 34 - avg_train_Score: 0.6895 avgScore: 0.7874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7151(0.7874) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 12m 24s) Loss: 0.6313(0.6313) Grad: 459347.5000  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7283(0.6984) Grad: 66740.1328  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6377(0.7008) Grad: 28190.5879  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.8346(0.8346) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.7008  avg_val_loss: 0.7861  time: 130s\n",
      "Epoch 35 - avg_train_Score: 0.7008 avgScore: 0.7861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7112(0.7861) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 12m 46s) Loss: 0.6511(0.6511) Grad: 170116.9062  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7101(0.6974) Grad: 36584.1289  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.1382(0.6969) Grad: 18680.3008  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 0.8344(0.8344) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7076(0.7875) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.6969  avg_val_loss: 0.7875  time: 130s\n",
      "Epoch 36 - avg_train_Score: 0.6969 avgScore: 0.7875\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 12m 30s) Loss: 0.9197(0.9197) Grad: 178752.5156  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5587(0.6821) Grad: 70837.1484  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6685(0.6840) Grad: 169377.5938  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8372(0.8372) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.6840  avg_val_loss: 0.7861  time: 130s\n",
      "Epoch 37 - avg_train_Score: 0.6840 avgScore: 0.7861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7055(0.7861) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 12m 38s) Loss: 0.7053(0.7053) Grad: 180738.2656  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6068(0.6719) Grad: 96502.1094  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6253(0.6719) Grad: 122595.1719  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.8322(0.8322) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.6719  avg_val_loss: 0.7849  time: 130s\n",
      "Epoch 38 - avg_train_Score: 0.6719 avgScore: 0.7849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7029(0.7849) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 12m 4s) Loss: 0.7619(0.7619) Grad: 166132.5781  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8104(0.6933) Grad: 22418.8281  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.1045(0.6952) Grad: 21388.7773  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.8333(0.8333) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7094(0.7843) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.6952  avg_val_loss: 0.7843  time: 130s\n",
      "Epoch 39 - avg_train_Score: 0.6952 avgScore: 0.7843\n",
      "Epoch 39 - Save Best Score: 0.7843 Model\n",
      "Epoch 39 - Save Best Loss: 0.7843 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 12m 25s) Loss: 0.5723(0.5723) Grad: 191449.9062  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.4369(0.6746) Grad: 45600.4570  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8051(0.6746) Grad: 46754.3008  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 0.8270(0.8270) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.6746  avg_val_loss: 0.7841  time: 130s\n",
      "Epoch 40 - avg_train_Score: 0.6746 avgScore: 0.7841\n",
      "Epoch 40 - Save Best Score: 0.7841 Model\n",
      "Epoch 40 - Save Best Loss: 0.7841 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7156(0.7841) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 1 result ==========\n",
      "score: 0.7841\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 12m 35s) Loss: 5.8193(5.8193) Grad: 381270.9375  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 5.3461(5.3151) Grad: 143264.9062  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 5.4475(5.3011) Grad: 41490.0352  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 4.9969(4.9969) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 5.3011  avg_val_loss: 5.1389  time: 130s\n",
      "Epoch 1 - avg_train_Score: 5.3011 avgScore: 5.1389\n",
      "Epoch 1 - Save Best Score: 5.1389 Model\n",
      "Epoch 1 - Save Best Loss: 5.1389 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.9631(5.1389) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 12m 26s) Loss: 5.3932(5.3932) Grad: 245220.6406  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 4.6049(4.9589) Grad: 134743.0312  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 4.6373(4.9472) Grad: 246242.6406  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 4.5561(4.5561) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 4.9472  avg_val_loss: 4.7185  time: 130s\n",
      "Epoch 2 - avg_train_Score: 4.9472 avgScore: 4.7185\n",
      "Epoch 2 - Save Best Score: 4.7185 Model\n",
      "Epoch 2 - Save Best Loss: 4.7185 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 4.5443(4.7185) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 12m 31s) Loss: 4.3050(4.3050) Grad: 644360.0000  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.5154(3.4121) Grad: 37229.4883  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.9393(3.2987) Grad: 33797.8594  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 2.2994(2.2994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 3.2987  avg_val_loss: 2.5296  time: 130s\n",
      "Epoch 3 - avg_train_Score: 3.2987 avgScore: 2.5296\n",
      "Epoch 3 - Save Best Score: 2.5296 Model\n",
      "Epoch 3 - Save Best Loss: 2.5296 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 2.4668(2.5296) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 12m 31s) Loss: 1.5724(1.5724) Grad: 473401.6875  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.3112(1.5320) Grad: 42017.1055  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0520(1.5304) Grad: 30839.7988  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.9010(0.9010) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.5304  avg_val_loss: 1.1076  time: 130s\n",
      "Epoch 4 - avg_train_Score: 1.5304 avgScore: 1.1076\n",
      "Epoch 4 - Save Best Score: 1.1076 Model\n",
      "Epoch 4 - Save Best Loss: 1.1076 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 1.1634(1.1076) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 13m 39s) Loss: 1.1464(1.1464) Grad: 276942.8750  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.2948(1.2979) Grad: 91604.2891  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.2483(1.2979) Grad: 65125.0078  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.7755(0.7755) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.2979  avg_val_loss: 0.9746  time: 131s\n",
      "Epoch 5 - avg_train_Score: 1.2979 avgScore: 0.9746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0306(0.9746) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Save Best Score: 0.9746 Model\n",
      "Epoch 5 - Save Best Loss: 0.9746 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 12m 18s) Loss: 1.1350(1.1350) Grad: 337598.0312  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2388(1.1723) Grad: 148773.6875  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.5313(1.1660) Grad: 216642.8906  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7592(0.7592) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 1.1660  avg_val_loss: 0.9259  time: 130s\n",
      "Epoch 6 - avg_train_Score: 1.1660 avgScore: 0.9259\n",
      "Epoch 6 - Save Best Score: 0.9259 Model\n",
      "Epoch 6 - Save Best Loss: 0.9259 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9560(0.9259) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 59s) Loss: 1.1607(1.1607) Grad: 568769.0000  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0130(1.1737) Grad: 50415.5117  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.3467(1.1768) Grad: 58833.6992  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7626(0.7626) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 1.1768  avg_val_loss: 0.9060  time: 130s\n",
      "Epoch 7 - avg_train_Score: 1.1768 avgScore: 0.9060\n",
      "Epoch 7 - Save Best Score: 0.9060 Model\n",
      "Epoch 7 - Save Best Loss: 0.9060 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9019(0.9060) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 1.0280(1.0280) Grad: 208681.1562  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8171(1.0773) Grad: 52513.0625  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9121(1.0790) Grad: 55052.4023  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7693(0.7693) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 1.0790  avg_val_loss: 0.8736  time: 130s\n",
      "Epoch 8 - avg_train_Score: 1.0790 avgScore: 0.8736\n",
      "Epoch 8 - Save Best Score: 0.8736 Model\n",
      "Epoch 8 - Save Best Loss: 0.8736 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8630(0.8736) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 0.9312(0.9312) Grad: 279093.0938  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9492(1.0188) Grad: 62610.4453  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9172(1.0253) Grad: 60475.4336  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.7278(0.7278) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 1.0253  avg_val_loss: 0.8565  time: 130s\n",
      "Epoch 9 - avg_train_Score: 1.0253 avgScore: 0.8565\n",
      "Epoch 9 - Save Best Score: 0.8565 Model\n",
      "Epoch 9 - Save Best Loss: 0.8565 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8194(0.8565) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 12m 42s) Loss: 0.8839(0.8839) Grad: 257541.1094  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8409(0.9578) Grad: 150298.5156  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9424(0.9573) Grad: 123747.2344  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7377(0.7377) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.9573  avg_val_loss: 0.8355  time: 130s\n",
      "Epoch 10 - avg_train_Score: 0.9573 avgScore: 0.8355\n",
      "Epoch 10 - Save Best Score: 0.8355 Model\n",
      "Epoch 10 - Save Best Loss: 0.8355 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8155(0.8355) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 12m 24s) Loss: 0.8217(0.8217) Grad: 319252.2812  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2768(0.9599) Grad: 64659.4531  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.3208(0.9625) Grad: 126547.7578  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7280(0.7280) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.9625  avg_val_loss: 0.8293  time: 130s\n",
      "Epoch 11 - avg_train_Score: 0.9625 avgScore: 0.8293\n",
      "Epoch 11 - Save Best Score: 0.8293 Model\n",
      "Epoch 11 - Save Best Loss: 0.8293 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7714(0.8293) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 1.1481(1.1481) Grad: 281147.2812  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8919(0.9152) Grad: 136679.6719  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9894(0.9158) Grad: 184055.0781  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 0.7010(0.7010) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.9158  avg_val_loss: 0.8190  time: 130s\n",
      "Epoch 12 - avg_train_Score: 0.9158 avgScore: 0.8190\n",
      "Epoch 12 - Save Best Score: 0.8190 Model\n",
      "Epoch 12 - Save Best Loss: 0.8190 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7586(0.8190) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 12m 4s) Loss: 1.0244(1.0244) Grad: 263844.3438  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2412(0.8778) Grad: 126763.2344  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0918(0.8766) Grad: 158742.5469  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.6933(0.6933) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.8766  avg_val_loss: 0.8060  time: 130s\n",
      "Epoch 13 - avg_train_Score: 0.8766 avgScore: 0.8060\n",
      "Epoch 13 - Save Best Score: 0.8060 Model\n",
      "Epoch 13 - Save Best Loss: 0.8060 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7319(0.8060) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 12m 6s) Loss: 0.9146(0.9146) Grad: 217645.2656  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8464(0.8965) Grad: 56972.4922  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8022(0.8963) Grad: 62309.5273  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7218(0.7218) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7561(0.8138) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.8963  avg_val_loss: 0.8138  time: 130s\n",
      "Epoch 14 - avg_train_Score: 0.8963 avgScore: 0.8138\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 12m 6s) Loss: 0.8362(0.8362) Grad: 243317.4531  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7462(0.8634) Grad: 123395.3672  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8275(0.8624) Grad: 130354.2266  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7232(0.7232) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.8624  avg_val_loss: 0.8055  time: 130s\n",
      "Epoch 15 - avg_train_Score: 0.8624 avgScore: 0.8055\n",
      "Epoch 15 - Save Best Score: 0.8055 Model\n",
      "Epoch 15 - Save Best Loss: 0.8055 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7682(0.8055) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 12m 17s) Loss: 0.7740(0.7740) Grad: 258720.5625  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7560(0.8354) Grad: 195701.6875  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7388(0.8345) Grad: 115767.2812  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7169(0.7169) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.8345  avg_val_loss: 0.7968  time: 130s\n",
      "Epoch 16 - avg_train_Score: 0.8345 avgScore: 0.7968\n",
      "Epoch 16 - Save Best Score: 0.7968 Model\n",
      "Epoch 16 - Save Best Loss: 0.7968 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7826(0.7968) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 12m 21s) Loss: 1.1095(1.1095) Grad: 213709.7812  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9532(0.8626) Grad: 100859.3828  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6202(0.8630) Grad: 30051.2090  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.6951(0.6951) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.8630  avg_val_loss: 0.8065  time: 130s\n",
      "Epoch 17 - avg_train_Score: 0.8630 avgScore: 0.8065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7879(0.8065) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 13m 13s) Loss: 0.8605(0.8605) Grad: 209440.6250  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7081(0.8246) Grad: 46897.8164  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7104(0.8226) Grad: 49244.6641  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7015(0.7015) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.8226  avg_val_loss: 0.7962  time: 130s\n",
      "Epoch 18 - avg_train_Score: 0.8226 avgScore: 0.7962\n",
      "Epoch 18 - Save Best Score: 0.7962 Model\n",
      "Epoch 18 - Save Best Loss: 0.7962 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7978(0.7962) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 12m 23s) Loss: 0.7292(0.7292) Grad: 202168.1562  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6584(0.8079) Grad: 54242.3008  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8552(0.8128) Grad: 68884.0078  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.6977(0.6977) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.8128  avg_val_loss: 0.7948  time: 130s\n",
      "Epoch 19 - avg_train_Score: 0.8128 avgScore: 0.7948\n",
      "Epoch 19 - Save Best Score: 0.7948 Model\n",
      "Epoch 19 - Save Best Loss: 0.7948 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7867(0.7948) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 13m 4s) Loss: 0.8571(0.8571) Grad: 290912.4375  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8636(0.8110) Grad: 21752.4023  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8650(0.8103) Grad: 25408.8164  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 0.6996(0.6996) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.8103  avg_val_loss: 0.7886  time: 130s\n",
      "Epoch 20 - avg_train_Score: 0.8103 avgScore: 0.7886\n",
      "Epoch 20 - Save Best Score: 0.7886 Model\n",
      "Epoch 20 - Save Best Loss: 0.7886 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7533(0.7886) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 12m 27s) Loss: 1.0313(1.0313) Grad: 198869.7344  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8104(0.7903) Grad: 43892.2617  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9299(0.7888) Grad: 57763.4453  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7083(0.7083) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.7888  avg_val_loss: 0.7875  time: 130s\n",
      "Epoch 21 - avg_train_Score: 0.7888 avgScore: 0.7875\n",
      "Epoch 21 - Save Best Score: 0.7875 Model\n",
      "Epoch 21 - Save Best Loss: 0.7875 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7812(0.7875) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 12m 14s) Loss: 1.3365(1.3365) Grad: 193810.8906  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7325(0.7669) Grad: 39342.1602  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6463(0.7660) Grad: 60622.0781  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.6986(0.6986) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.7660  avg_val_loss: 0.7825  time: 130s\n",
      "Epoch 22 - avg_train_Score: 0.7660 avgScore: 0.7825\n",
      "Epoch 22 - Save Best Score: 0.7825 Model\n",
      "Epoch 22 - Save Best Loss: 0.7825 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7879(0.7825) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.8348(0.8348) Grad: 217402.5781  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8585(0.7754) Grad: 36518.4609  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6294(0.7786) Grad: 21822.0020  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7121(0.7121) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.7786  avg_val_loss: 0.7799  time: 130s\n",
      "Epoch 23 - avg_train_Score: 0.7786 avgScore: 0.7799\n",
      "Epoch 23 - Save Best Score: 0.7799 Model\n",
      "Epoch 23 - Save Best Loss: 0.7799 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7773(0.7799) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 13m 20s) Loss: 0.8304(0.8304) Grad: 232515.5156  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7382(0.7726) Grad: 44573.5391  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9807(0.7731) Grad: 57863.6562  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 51s) Loss: 0.7025(0.7025) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.7731  avg_val_loss: 0.7782  time: 130s\n",
      "Epoch 24 - avg_train_Score: 0.7731 avgScore: 0.7782\n",
      "Epoch 24 - Save Best Score: 0.7782 Model\n",
      "Epoch 24 - Save Best Loss: 0.7782 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7643(0.7782) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 13m 12s) Loss: 0.6112(0.6112) Grad: 175212.9219  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7088(0.7415) Grad: 27007.5410  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7794(0.7446) Grad: 24224.6895  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7050(0.7050) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7510(0.7762) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.7446  avg_val_loss: 0.7762  time: 130s\n",
      "Epoch 25 - avg_train_Score: 0.7446 avgScore: 0.7762\n",
      "Epoch 25 - Save Best Score: 0.7762 Model\n",
      "Epoch 25 - Save Best Loss: 0.7762 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 0.9099(0.9099) Grad: 221809.9688  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6037(0.7456) Grad: 52802.1758  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6333(0.7446) Grad: 82159.4766  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7056(0.7056) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.7446  avg_val_loss: 0.7749  time: 130s\n",
      "Epoch 26 - avg_train_Score: 0.7446 avgScore: 0.7749\n",
      "Epoch 26 - Save Best Score: 0.7749 Model\n",
      "Epoch 26 - Save Best Loss: 0.7749 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7662(0.7749) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 0.7894(0.7894) Grad: 214473.4688  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9512(0.7459) Grad: 44154.7852  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6427(0.7434) Grad: 24915.2129  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 49s) Loss: 0.7108(0.7108) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.7434  avg_val_loss: 0.7743  time: 130s\n",
      "Epoch 27 - avg_train_Score: 0.7434 avgScore: 0.7743\n",
      "Epoch 27 - Save Best Score: 0.7743 Model\n",
      "Epoch 27 - Save Best Loss: 0.7743 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7768(0.7743) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 12m 26s) Loss: 0.7952(0.7952) Grad: 266153.8125  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 1.0128(0.7452) Grad: 54487.1016  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6061(0.7418) Grad: 20515.1074  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.6963(0.6963) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.7418  avg_val_loss: 0.7701  time: 129s\n",
      "Epoch 28 - avg_train_Score: 0.7418 avgScore: 0.7701\n",
      "Epoch 28 - Save Best Score: 0.7701 Model\n",
      "Epoch 28 - Save Best Loss: 0.7701 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7596(0.7701) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 12m 6s) Loss: 0.5592(0.5592) Grad: 192359.8125  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7930(0.7127) Grad: 24160.8340  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6685(0.7157) Grad: 24574.0977  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.6964(0.6964) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.7157  avg_val_loss: 0.7678  time: 130s\n",
      "Epoch 29 - avg_train_Score: 0.7157 avgScore: 0.7678\n",
      "Epoch 29 - Save Best Score: 0.7678 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7325(0.7678) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Save Best Loss: 0.7678 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 12m 44s) Loss: 0.6215(0.6215) Grad: 157133.6250  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.6074(0.7182) Grad: 20118.2715  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7127(0.7163) Grad: 42789.3633  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.6807(0.6807) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.7163  avg_val_loss: 0.7643  time: 129s\n",
      "Epoch 30 - avg_train_Score: 0.7163 avgScore: 0.7643\n",
      "Epoch 30 - Save Best Score: 0.7643 Model\n",
      "Epoch 30 - Save Best Loss: 0.7643 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7239(0.7643) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 12m 44s) Loss: 0.8240(0.8240) Grad: 234454.8125  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7912(0.7096) Grad: 31505.0996  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6950(0.7072) Grad: 20177.0762  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 50s) Loss: 0.6945(0.6945) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.7072  avg_val_loss: 0.7634  time: 130s\n",
      "Epoch 31 - avg_train_Score: 0.7072 avgScore: 0.7634\n",
      "Epoch 31 - Save Best Score: 0.7634 Model\n",
      "Epoch 31 - Save Best Loss: 0.7634 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7321(0.7634) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 0.8069(0.8069) Grad: 165006.7188  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7464(0.6966) Grad: 42963.6133  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6454(0.6939) Grad: 51149.4141  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6909(0.6909) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.6939  avg_val_loss: 0.7601  time: 130s\n",
      "Epoch 32 - avg_train_Score: 0.6939 avgScore: 0.7601\n",
      "Epoch 32 - Save Best Score: 0.7601 Model\n",
      "Epoch 32 - Save Best Loss: 0.7601 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7457(0.7601) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 12m 15s) Loss: 0.7731(0.7731) Grad: 232802.0781  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6132(0.6954) Grad: 36069.4180  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5524(0.6924) Grad: 41339.8320  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 50s) Loss: 0.6916(0.6916) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7482(0.7590) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.6924  avg_val_loss: 0.7590  time: 130s\n",
      "Epoch 33 - avg_train_Score: 0.6924 avgScore: 0.7590\n",
      "Epoch 33 - Save Best Score: 0.7590 Model\n",
      "Epoch 33 - Save Best Loss: 0.7590 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 12m 29s) Loss: 0.5432(0.5432) Grad: 173027.2344  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8141(0.6947) Grad: 26595.9785  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5905(0.6946) Grad: 40633.0312  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.6949(0.6949) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7420(0.7587) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.6946  avg_val_loss: 0.7587  time: 130s\n",
      "Epoch 34 - avg_train_Score: 0.6946 avgScore: 0.7587\n",
      "Epoch 34 - Save Best Score: 0.7587 Model\n",
      "Epoch 34 - Save Best Loss: 0.7587 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 12m 13s) Loss: 0.5343(0.5343) Grad: 184718.9531  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6286(0.6826) Grad: 22109.5801  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5720(0.6833) Grad: 23852.9844  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7110(0.7110) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.6833  avg_val_loss: 0.7607  time: 130s\n",
      "Epoch 35 - avg_train_Score: 0.6833 avgScore: 0.7607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7359(0.7607) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 12m 7s) Loss: 0.6301(0.6301) Grad: 282789.6250  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8838(0.6748) Grad: 46097.8555  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7423(0.6753) Grad: 41684.1445  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6972(0.6972) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.6753  avg_val_loss: 0.7569  time: 130s\n",
      "Epoch 36 - avg_train_Score: 0.6753 avgScore: 0.7569\n",
      "Epoch 36 - Save Best Score: 0.7569 Model\n",
      "Epoch 36 - Save Best Loss: 0.7569 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7291(0.7569) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 12m 8s) Loss: 0.7491(0.7491) Grad: 252019.1094  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6418(0.6749) Grad: 21426.5742  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7412(0.6781) Grad: 22655.9688  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.6935(0.6935) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.6781  avg_val_loss: 0.7563  time: 130s\n",
      "Epoch 37 - avg_train_Score: 0.6781 avgScore: 0.7563\n",
      "Epoch 37 - Save Best Score: 0.7563 Model\n",
      "Epoch 37 - Save Best Loss: 0.7563 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7304(0.7563) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 12m 49s) Loss: 0.6747(0.6747) Grad: 238898.3125  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7147(0.6879) Grad: 42688.2148  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6900(0.6870) Grad: 67911.4688  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.6984(0.6984) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.6870  avg_val_loss: 0.7573  time: 130s\n",
      "Epoch 38 - avg_train_Score: 0.6870 avgScore: 0.7573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7353(0.7573) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 12m 26s) Loss: 0.9063(0.9063) Grad: 184977.5938  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5618(0.6759) Grad: 22004.7031  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9981(0.6779) Grad: 29845.9766  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.6981(0.6981) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.6779  avg_val_loss: 0.7561  time: 130s\n",
      "Epoch 39 - avg_train_Score: 0.6779 avgScore: 0.7561\n",
      "Epoch 39 - Save Best Score: 0.7561 Model\n",
      "Epoch 39 - Save Best Loss: 0.7561 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7291(0.7561) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 12m 38s) Loss: 0.5418(0.5418) Grad: 192957.9062  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9661(0.6751) Grad: 10895.3262  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5663(0.6731) Grad: 10616.7529  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7030(0.7030) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.6731  avg_val_loss: 0.7555  time: 130s\n",
      "Epoch 40 - avg_train_Score: 0.6731 avgScore: 0.7555\n",
      "Epoch 40 - Save Best Score: 0.7555 Model\n",
      "Epoch 40 - Save Best Loss: 0.7555 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7277(0.7555) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 2 result ==========\n",
      "score: 0.7555\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 12m 14s) Loss: 6.5931(6.5931) Grad: inf  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 5.0145(5.3303) Grad: 75451.4766  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 5.2344(5.3169) Grad: 86166.6094  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 5.2788(5.2788) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.7227(5.2887) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 5.3169  avg_val_loss: 5.2887  time: 130s\n",
      "Epoch 1 - avg_train_Score: 5.3169 avgScore: 5.2887\n",
      "Epoch 1 - Save Best Score: 5.2887 Model\n",
      "Epoch 1 - Save Best Loss: 5.2887 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 5.3639(5.3639) Grad: 225967.7344  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 4.2736(4.7743) Grad: 77691.6797  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 4.4655(4.7396) Grad: 25641.6035  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 4.4649(4.4649) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.0206(4.5017) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 4.7396  avg_val_loss: 4.5017  time: 130s\n",
      "Epoch 2 - avg_train_Score: 4.7396 avgScore: 4.5017\n",
      "Epoch 2 - Save Best Score: 4.5017 Model\n",
      "Epoch 2 - Save Best Loss: 4.5017 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 4.8308(4.8308) Grad: 187834.0625  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.9454(2.7732) Grad: 6512.4053  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.8082(2.6896) Grad: 10867.0762  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 1.9546(1.9546) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 2.6896  avg_val_loss: 1.9009  time: 130s\n",
      "Epoch 3 - avg_train_Score: 2.6896 avgScore: 1.9009\n",
      "Epoch 3 - Save Best Score: 1.9009 Model\n",
      "Epoch 3 - Save Best Loss: 1.9009 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.6377(1.9009) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 12m 31s) Loss: 1.4979(1.4979) Grad: 754281.3750  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.5565(1.4467) Grad: 37572.8477  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.1954(1.4418) Grad: 32170.3906  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 1.0723(1.0723) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.4418  avg_val_loss: 1.1370  time: 130s\n",
      "Epoch 4 - avg_train_Score: 1.4418 avgScore: 1.1370\n",
      "Epoch 4 - Save Best Score: 1.1370 Model\n",
      "Epoch 4 - Save Best Loss: 1.1370 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0713(1.1370) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 11m 54s) Loss: 1.2748(1.2748) Grad: 287246.2500  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.4314(1.2467) Grad: 80040.4922  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.1233(1.2456) Grad: 98826.2969  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 0.9866(0.9866) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.2456  avg_val_loss: 1.0397  time: 130s\n",
      "Epoch 5 - avg_train_Score: 1.2456 avgScore: 1.0397\n",
      "Epoch 5 - Save Best Score: 1.0397 Model\n",
      "Epoch 5 - Save Best Loss: 1.0397 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0377(1.0397) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 12m 16s) Loss: 1.0106(1.0106) Grad: 479007.0312  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2289(1.1723) Grad: 70126.5000  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.3350(1.1774) Grad: 63075.1445  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.9074(0.9074) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 1.1774  avg_val_loss: 0.9984  time: 130s\n",
      "Epoch 6 - avg_train_Score: 1.1774 avgScore: 0.9984\n",
      "Epoch 6 - Save Best Score: 0.9984 Model\n",
      "Epoch 6 - Save Best Loss: 0.9984 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 1.0007(0.9984) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 57s) Loss: 1.3992(1.3992) Grad: 319793.9062  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.7073(1.1143) Grad: 61015.4727  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.1118(1.1174) Grad: 118177.0156  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8757(0.8757) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 1.1174  avg_val_loss: 0.9658  time: 130s\n",
      "Epoch 7 - avg_train_Score: 1.1174 avgScore: 0.9658\n",
      "Epoch 7 - Save Best Score: 0.9658 Model\n",
      "Epoch 7 - Save Best Loss: 0.9658 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.9940(0.9658) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 0.9259(0.9259) Grad: 271514.7812  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.3737(1.0934) Grad: 98887.8594  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.1209(1.0895) Grad: 108319.3594  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8739(0.8739) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.9450(0.9376) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 1.0895  avg_val_loss: 0.9376  time: 130s\n",
      "Epoch 8 - avg_train_Score: 1.0895 avgScore: 0.9376\n",
      "Epoch 8 - Save Best Score: 0.9376 Model\n",
      "Epoch 8 - Save Best Loss: 0.9376 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 0.8952(0.8952) Grad: 256727.6562  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8166(1.0411) Grad: 60905.1055  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9385(1.0396) Grad: 68507.0703  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8731(0.8731) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 1.0396  avg_val_loss: 0.9191  time: 130s\n",
      "Epoch 9 - avg_train_Score: 1.0396 avgScore: 0.9191\n",
      "Epoch 9 - Save Best Score: 0.9191 Model\n",
      "Epoch 9 - Save Best Loss: 0.9191 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.9325(0.9191) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 12m 25s) Loss: 0.9031(0.9031) Grad: 296320.8125  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.1078(1.0488) Grad: 44778.4492  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0146(1.0487) Grad: 30535.7305  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8456(0.8456) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 1.0487  avg_val_loss: 0.9093  time: 130s\n",
      "Epoch 10 - avg_train_Score: 1.0487 avgScore: 0.9093\n",
      "Epoch 10 - Save Best Score: 0.9093 Model\n",
      "Epoch 10 - Save Best Loss: 0.9093 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.9555(0.9093) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 1.0261(1.0261) Grad: 335302.0625  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7548(1.0490) Grad: 26900.4902  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7172(1.0515) Grad: 39946.9062  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8231(0.8231) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 1.0515  avg_val_loss: 0.8874  time: 130s\n",
      "Epoch 11 - avg_train_Score: 1.0515 avgScore: 0.8874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8989(0.8874) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Save Best Score: 0.8874 Model\n",
      "Epoch 11 - Save Best Loss: 0.8874 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 0.9911(0.9911) Grad: 404977.9062  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0647(0.9845) Grad: 28478.9961  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0070(0.9888) Grad: 32442.5938  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.8409(0.8409) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8909(0.8716) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.9888  avg_val_loss: 0.8716  time: 130s\n",
      "Epoch 12 - avg_train_Score: 0.9888 avgScore: 0.8716\n",
      "Epoch 12 - Save Best Score: 0.8716 Model\n",
      "Epoch 12 - Save Best Loss: 0.8716 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 13m 8s) Loss: 0.8423(0.8423) Grad: 292138.0938  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8381(0.9545) Grad: 32981.2227  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9207(0.9543) Grad: 32622.9668  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.8329(0.8329) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.9543  avg_val_loss: 0.8646  time: 130s\n",
      "Epoch 13 - avg_train_Score: 0.9543 avgScore: 0.8646\n",
      "Epoch 13 - Save Best Score: 0.8646 Model\n",
      "Epoch 13 - Save Best Loss: 0.8646 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9271(0.8646) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 12m 59s) Loss: 0.9319(0.9319) Grad: 226792.6719  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7755(0.8981) Grad: 64571.7344  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0932(0.8971) Grad: 127157.5781  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.8318(0.8318) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.8971  avg_val_loss: 0.8565  time: 130s\n",
      "Epoch 14 - avg_train_Score: 0.8971 avgScore: 0.8565\n",
      "Epoch 14 - Save Best Score: 0.8565 Model\n",
      "Epoch 14 - Save Best Loss: 0.8565 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8942(0.8565) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 12m 23s) Loss: 1.0825(1.0825) Grad: 234409.6406  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2801(0.8623) Grad: 90983.3281  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7469(0.8613) Grad: 144149.0781  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8169(0.8169) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.8613  avg_val_loss: 0.8439  time: 130s\n",
      "Epoch 15 - avg_train_Score: 0.8613 avgScore: 0.8439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8723(0.8439) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Save Best Score: 0.8439 Model\n",
      "Epoch 15 - Save Best Loss: 0.8439 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 0.9089(0.9089) Grad: 198994.0156  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8569(0.8846) Grad: 31875.1992  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2912(0.8887) Grad: 36081.4180  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 53s) Loss: 0.7999(0.7999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.8887  avg_val_loss: 0.8423  time: 130s\n",
      "Epoch 16 - avg_train_Score: 0.8887 avgScore: 0.8423\n",
      "Epoch 16 - Save Best Score: 0.8423 Model\n",
      "Epoch 16 - Save Best Loss: 0.8423 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8722(0.8423) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 16m 23s) Loss: 0.8738(0.8738) Grad: 343729.1250  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8896(0.8810) Grad: 54286.5938  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7586(0.8757) Grad: 55583.3477  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.7941(0.7941) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.8757  avg_val_loss: 0.8419  time: 130s\n",
      "Epoch 17 - avg_train_Score: 0.8757 avgScore: 0.8419\n",
      "Epoch 17 - Save Best Score: 0.8419 Model\n",
      "Epoch 17 - Save Best Loss: 0.8419 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8613(0.8419) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 12m 23s) Loss: 0.7475(0.7475) Grad: 182152.8281  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9433(0.8360) Grad: 59137.5859  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7246(0.8403) Grad: 29038.0957  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8121(0.8121) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.8403  avg_val_loss: 0.8335  time: 130s\n",
      "Epoch 18 - avg_train_Score: 0.8403 avgScore: 0.8335\n",
      "Epoch 18 - Save Best Score: 0.8335 Model\n",
      "Epoch 18 - Save Best Loss: 0.8335 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8415(0.8335) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 12m 18s) Loss: 0.7312(0.7312) Grad: 222236.6406  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9196(0.8477) Grad: 45938.3477  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9671(0.8476) Grad: 59341.6914  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8017(0.8017) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.8476  avg_val_loss: 0.8324  time: 130s\n",
      "Epoch 19 - avg_train_Score: 0.8476 avgScore: 0.8324\n",
      "Epoch 19 - Save Best Score: 0.8324 Model\n",
      "Epoch 19 - Save Best Loss: 0.8324 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8356(0.8324) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 54s) Loss: 0.9189(0.9189) Grad: 223188.3125  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9326(0.8301) Grad: 47936.9883  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7698(0.8267) Grad: 77790.5312  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7725(0.7725) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.8267  avg_val_loss: 0.8289  time: 130s\n",
      "Epoch 20 - avg_train_Score: 0.8267 avgScore: 0.8289\n",
      "Epoch 20 - Save Best Score: 0.8289 Model\n",
      "Epoch 20 - Save Best Loss: 0.8289 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8251(0.8289) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 12m 8s) Loss: 0.7769(0.7769) Grad: 200996.1406  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.6952(0.8123) Grad: 88557.3125  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8730(0.8124) Grad: 61637.4922  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7763(0.7763) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.8124  avg_val_loss: 0.8253  time: 130s\n",
      "Epoch 21 - avg_train_Score: 0.8124 avgScore: 0.8253\n",
      "Epoch 21 - Save Best Score: 0.8253 Model\n",
      "Epoch 21 - Save Best Loss: 0.8253 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8217(0.8253) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 12m 26s) Loss: 0.8270(0.8270) Grad: 390710.3438  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8762(0.7983) Grad: 48070.8164  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9454(0.8034) Grad: 77772.4453  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7870(0.7870) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.8034  avg_val_loss: 0.8224  time: 130s\n",
      "Epoch 22 - avg_train_Score: 0.8034 avgScore: 0.8224\n",
      "Epoch 22 - Save Best Score: 0.8224 Model\n",
      "Epoch 22 - Save Best Loss: 0.8224 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8022(0.8224) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 12m 18s) Loss: 0.8783(0.8783) Grad: 358218.5312  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6336(0.8393) Grad: 13904.4756  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6467(0.8397) Grad: 14018.4922  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7701(0.7701) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.8397  avg_val_loss: 0.8244  time: 130s\n",
      "Epoch 23 - avg_train_Score: 0.8397 avgScore: 0.8244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8306(0.8244) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 12m 3s) Loss: 0.9453(0.9453) Grad: 242084.0469  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.9221(0.8045) Grad: 94556.0859  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7161(0.8018) Grad: 62995.2656  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.7769(0.7769) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.8018  avg_val_loss: 0.8197  time: 130s\n",
      "Epoch 24 - avg_train_Score: 0.8018 avgScore: 0.8197\n",
      "Epoch 24 - Save Best Score: 0.8197 Model\n",
      "Epoch 24 - Save Best Loss: 0.8197 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8368(0.8197) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 12m 5s) Loss: 0.7975(0.7975) Grad: 353782.5938  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6994(0.7810) Grad: 81354.1953  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0858(0.7781) Grad: 48765.0508  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7966(0.7966) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.7781  avg_val_loss: 0.8145  time: 130s\n",
      "Epoch 25 - avg_train_Score: 0.7781 avgScore: 0.8145\n",
      "Epoch 25 - Save Best Score: 0.8145 Model\n",
      "Epoch 25 - Save Best Loss: 0.8145 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8245(0.8145) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 12m 52s) Loss: 0.6582(0.6582) Grad: 165199.7500  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6464(0.7746) Grad: 40856.3984  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6917(0.7731) Grad: 61163.6445  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7894(0.7894) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.7731  avg_val_loss: 0.8088  time: 130s\n",
      "Epoch 26 - avg_train_Score: 0.7731 avgScore: 0.8088\n",
      "Epoch 26 - Save Best Score: 0.8088 Model\n",
      "Epoch 26 - Save Best Loss: 0.8088 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8227(0.8088) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 14m 2s) Loss: 0.6648(0.6648) Grad: 221703.0000  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7203(0.7445) Grad: 44458.7266  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8122(0.7443) Grad: 88747.9922  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 0.7875(0.7875) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.7443  avg_val_loss: 0.8084  time: 130s\n",
      "Epoch 27 - avg_train_Score: 0.7443 avgScore: 0.8084\n",
      "Epoch 27 - Save Best Score: 0.8084 Model\n",
      "Epoch 27 - Save Best Loss: 0.8084 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8181(0.8084) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 12m 6s) Loss: 0.5892(0.5892) Grad: 186075.5000  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5639(0.7487) Grad: 106939.0312  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6912(0.7491) Grad: 163386.2656  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7803(0.7803) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.7491  avg_val_loss: 0.8058  time: 130s\n",
      "Epoch 28 - avg_train_Score: 0.7491 avgScore: 0.8058\n",
      "Epoch 28 - Save Best Score: 0.8058 Model\n",
      "Epoch 28 - Save Best Loss: 0.8058 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8297(0.8058) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 11m 56s) Loss: 0.6726(0.6726) Grad: 176605.5781  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.7513(0.7424) Grad: 26178.9434  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.4826(0.7423) Grad: 22287.6406  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 0.7936(0.7936) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8304(0.8048) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.7423  avg_val_loss: 0.8048  time: 130s\n",
      "Epoch 29 - avg_train_Score: 0.7423 avgScore: 0.8048\n",
      "Epoch 29 - Save Best Score: 0.8048 Model\n",
      "Epoch 29 - Save Best Loss: 0.8048 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 12m 9s) Loss: 0.7311(0.7311) Grad: 232195.0000  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5801(0.7363) Grad: 42521.6133  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8584(0.7360) Grad: 38960.5703  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.8001(0.8001) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.7360  avg_val_loss: 0.8116  time: 130s\n",
      "Epoch 30 - avg_train_Score: 0.7360 avgScore: 0.8116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8426(0.8116) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 12m 44s) Loss: 0.6645(0.6645) Grad: 197179.0625  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7512(0.7414) Grad: 73579.3281  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6451(0.7437) Grad: 29772.2227  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7930(0.7930) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.7437  avg_val_loss: 0.8088  time: 130s\n",
      "Epoch 31 - avg_train_Score: 0.7437 avgScore: 0.8088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8248(0.8088) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 12m 23s) Loss: 0.5680(0.5680) Grad: 217131.4062  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6850(0.7101) Grad: 166936.2969  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6098(0.7113) Grad: 104143.3516  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.7944(0.7944) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7995(0.8056) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.7113  avg_val_loss: 0.8056  time: 130s\n",
      "Epoch 32 - avg_train_Score: 0.7113 avgScore: 0.8056\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 12m 59s) Loss: 0.5943(0.5943) Grad: 226765.6875  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9623(0.7092) Grad: 93003.6406  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7118(0.7145) Grad: 29169.7324  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7984(0.7984) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8060(0.8062) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.7145  avg_val_loss: 0.8062  time: 130s\n",
      "Epoch 33 - avg_train_Score: 0.7145 avgScore: 0.8062\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 0.5691(0.5691) Grad: 163558.7344  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5684(0.7103) Grad: 43916.9922  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0384(0.7134) Grad: 92953.1016  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 53s) Loss: 0.8016(0.8016) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.7134  avg_val_loss: 0.8052  time: 131s\n",
      "Epoch 34 - avg_train_Score: 0.7134 avgScore: 0.8052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8055(0.8052) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 12m 32s) Loss: 0.7131(0.7131) Grad: 196148.4219  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5545(0.7201) Grad: 23553.6113  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5809(0.7191) Grad: 22020.0195  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7869(0.7869) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.7191  avg_val_loss: 0.8032  time: 130s\n",
      "Epoch 35 - avg_train_Score: 0.7191 avgScore: 0.8032\n",
      "Epoch 35 - Save Best Score: 0.8032 Model\n",
      "Epoch 35 - Save Best Loss: 0.8032 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8066(0.8032) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 12m 38s) Loss: 0.6663(0.6663) Grad: 327442.4062  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5730(0.7051) Grad: 28434.1289  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5768(0.7055) Grad: 27090.1602  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7978(0.7978) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.7055  avg_val_loss: 0.8017  time: 130s\n",
      "Epoch 36 - avg_train_Score: 0.7055 avgScore: 0.8017\n",
      "Epoch 36 - Save Best Score: 0.8017 Model\n",
      "Epoch 36 - Save Best Loss: 0.8017 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8072(0.8017) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 12m 8s) Loss: 0.7727(0.7727) Grad: 235615.8906  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6729(0.7102) Grad: 40402.5742  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6190(0.7096) Grad: 66438.3516  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7874(0.7874) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.7096  avg_val_loss: 0.8006  time: 130s\n",
      "Epoch 37 - avg_train_Score: 0.7096 avgScore: 0.8006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8143(0.8006) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Save Best Score: 0.8006 Model\n",
      "Epoch 37 - Save Best Loss: 0.8006 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 12m 27s) Loss: 0.6453(0.6453) Grad: 181909.6406  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5180(0.7089) Grad: 51384.3633  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0290(0.7052) Grad: 35878.9414  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7867(0.7867) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.7052  avg_val_loss: 0.8010  time: 130s\n",
      "Epoch 38 - avg_train_Score: 0.7052 avgScore: 0.8010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8120(0.8010) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 13m 10s) Loss: 0.5409(0.5409) Grad: 222683.4375  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7893(0.6984) Grad: 23830.8633  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5429(0.6955) Grad: 27692.3535  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7850(0.7850) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.6955  avg_val_loss: 0.8004  time: 130s\n",
      "Epoch 39 - avg_train_Score: 0.6955 avgScore: 0.8004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8136(0.8004) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Save Best Score: 0.8004 Model\n",
      "Epoch 39 - Save Best Loss: 0.8004 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 12m 30s) Loss: 1.0935(1.0935) Grad: nan  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.4969(0.6951) Grad: 31114.6133  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5581(0.6951) Grad: 22862.5859  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7793(0.7793) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.6951  avg_val_loss: 0.7987  time: 130s\n",
      "Epoch 40 - avg_train_Score: 0.6951 avgScore: 0.7987\n",
      "Epoch 40 - Save Best Score: 0.7987 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8142(0.7987) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Save Best Loss: 0.7987 Model\n",
      "/tmp/ipykernel_217129/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 3 result ==========\n",
      "score: 0.7987\n",
      "========== fold: 4 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 12m 21s) Loss: 5.1472(5.1472) Grad: 264683.0000  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 5.0373(5.3049) Grad: 42572.2969  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 5.4515(5.2795) Grad: 98528.7891  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 55s) Loss: 5.3973(5.3973) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 5.0773(5.2111) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 5.2795  avg_val_loss: 5.2111  time: 130s\n",
      "Epoch 1 - avg_train_Score: 5.2795 avgScore: 5.2111\n",
      "Epoch 1 - Save Best Score: 5.2111 Model\n",
      "Epoch 1 - Save Best Loss: 5.2111 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 5.0370(5.0370) Grad: 302268.8750  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 5.6715(4.9224) Grad: 62737.0195  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 4.3781(4.9009) Grad: 29072.3340  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 4.8174(4.8174) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 4.9009  avg_val_loss: 4.7180  time: 130s\n",
      "Epoch 2 - avg_train_Score: 4.9009 avgScore: 4.7180\n",
      "Epoch 2 - Save Best Score: 4.7180 Model\n",
      "Epoch 2 - Save Best Loss: 4.7180 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.5915(4.7180) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 12m 23s) Loss: 4.3076(4.3076) Grad: 617365.0000  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 1.6189(3.0474) Grad: 26683.8184  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.8831(2.9417) Grad: 35243.1172  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 1.9998(1.9998) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 2.9417  avg_val_loss: 2.1509  time: 130s\n",
      "Epoch 3 - avg_train_Score: 2.9418 avgScore: 2.1509\n",
      "Epoch 3 - Save Best Score: 2.1509 Model\n",
      "Epoch 3 - Save Best Loss: 2.1509 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 2.4324(2.1509) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 1.2848(1.2848) Grad: 418933.4688  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2672(1.4572) Grad: 40548.4492  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.7926(1.4502) Grad: 42240.2266  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 1.1452(1.1452) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.4502  avg_val_loss: 1.1228  time: 130s\n",
      "Epoch 4 - avg_train_Score: 1.4502 avgScore: 1.1228\n",
      "Epoch 4 - Save Best Score: 1.1228 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.3122(1.1228) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Save Best Loss: 1.1228 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 12m 31s) Loss: 1.0089(1.0089) Grad: 375396.0312  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.4012(1.2915) Grad: 39387.3008  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9120(1.2899) Grad: 32783.0742  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 1.0601(1.0601) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.2899  avg_val_loss: 1.0130  time: 130s\n",
      "Epoch 5 - avg_train_Score: 1.2899 avgScore: 1.0130\n",
      "Epoch 5 - Save Best Score: 1.0130 Model\n",
      "Epoch 5 - Save Best Loss: 1.0130 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 1.0793(1.0130) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 11m 56s) Loss: 1.2594(1.2594) Grad: 301164.6562  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2003(1.2096) Grad: 59391.2109  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0855(1.2035) Grad: 83125.0703  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.9974(0.9974) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 1.2035  avg_val_loss: 0.9594  time: 130s\n",
      "Epoch 6 - avg_train_Score: 1.2035 avgScore: 0.9594\n",
      "Epoch 6 - Save Best Score: 0.9594 Model\n",
      "Epoch 6 - Save Best Loss: 0.9594 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0224(0.9594) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 12m 56s) Loss: 1.8075(1.8075) Grad: 377146.1250  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.1398(1.1030) Grad: 106594.1016  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0576(1.1028) Grad: 55339.1875  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.8930(0.8930) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 1.1028  avg_val_loss: 0.9142  time: 130s\n",
      "Epoch 7 - avg_train_Score: 1.1028 avgScore: 0.9142\n",
      "Epoch 7 - Save Best Score: 0.9142 Model\n",
      "Epoch 7 - Save Best Loss: 0.9142 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0335(0.9142) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 1.2189(1.2189) Grad: 243462.4375  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.8100(1.0259) Grad: 136203.5312  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0672(1.0285) Grad: 50916.8906  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 0.8758(0.8758) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 1.0285  avg_val_loss: 0.8908  time: 130s\n",
      "Epoch 8 - avg_train_Score: 1.0285 avgScore: 0.8908\n",
      "Epoch 8 - Save Best Score: 0.8908 Model\n",
      "Epoch 8 - Save Best Loss: 0.8908 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9247(0.8908) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 12m 41s) Loss: 0.9439(0.9439) Grad: 302130.8750  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7943(0.9952) Grad: 216793.2500  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0954(0.9953) Grad: 59673.7852  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 0.8760(0.8760) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.9953  avg_val_loss: 0.8731  time: 130s\n",
      "Epoch 9 - avg_train_Score: 0.9953 avgScore: 0.8731\n",
      "Epoch 9 - Save Best Score: 0.8731 Model\n",
      "Epoch 9 - Save Best Loss: 0.8731 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9038(0.8731) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 13m 6s) Loss: 0.9371(0.9371) Grad: 229594.6406  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0400(0.9675) Grad: 93654.1797  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8471(0.9669) Grad: 78404.3359  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.8743(0.8743) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.9669  avg_val_loss: 0.8636  time: 130s\n",
      "Epoch 10 - avg_train_Score: 0.9669 avgScore: 0.8636\n",
      "Epoch 10 - Save Best Score: 0.8636 Model\n",
      "Epoch 10 - Save Best Loss: 0.8636 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8650(0.8636) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 12m 34s) Loss: 0.8228(0.8228) Grad: 282676.5625  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.4584(0.9997) Grad: 57670.0859  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9678(1.0022) Grad: 85996.7891  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8488(0.8488) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 1.0022  avg_val_loss: 0.8632  time: 130s\n",
      "Epoch 11 - avg_train_Score: 1.0022 avgScore: 0.8632\n",
      "Epoch 11 - Save Best Score: 0.8632 Model\n",
      "Epoch 11 - Save Best Loss: 0.8632 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8596(0.8632) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 12m 19s) Loss: 0.9129(0.9129) Grad: 209415.1562  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.5258(0.9787) Grad: 23350.1035  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6826(0.9807) Grad: 23012.0762  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.8842(0.8842) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.9807  avg_val_loss: 0.8560  time: 130s\n",
      "Epoch 12 - avg_train_Score: 0.9807 avgScore: 0.8560\n",
      "Epoch 12 - Save Best Score: 0.8560 Model\n",
      "Epoch 12 - Save Best Loss: 0.8560 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8722(0.8560) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 12m 16s) Loss: 1.1074(1.1074) Grad: 288925.6875  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.5129(0.8917) Grad: 113632.5469  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0721(0.8939) Grad: 203321.3438  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.8198(0.8198) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.8939  avg_val_loss: 0.8364  time: 130s\n",
      "Epoch 13 - avg_train_Score: 0.8939 avgScore: 0.8364\n",
      "Epoch 13 - Save Best Score: 0.8364 Model\n",
      "Epoch 13 - Save Best Loss: 0.8364 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8824(0.8364) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 57s) Loss: 0.8985(0.8985) Grad: 246810.8125  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9283(0.9014) Grad: 35859.5938  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9113(0.9042) Grad: 41324.8359  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.8209(0.8209) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.9042  avg_val_loss: 0.8359  time: 130s\n",
      "Epoch 14 - avg_train_Score: 0.9042 avgScore: 0.8359\n",
      "Epoch 14 - Save Best Score: 0.8359 Model\n",
      "Epoch 14 - Save Best Loss: 0.8359 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8744(0.8359) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 12m 27s) Loss: 0.8239(0.8239) Grad: 225209.6406  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9173(0.8905) Grad: 46819.8359  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8958(0.8877) Grad: 59623.3594  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8185(0.8185) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.8877  avg_val_loss: 0.8146  time: 130s\n",
      "Epoch 15 - avg_train_Score: 0.8877 avgScore: 0.8146\n",
      "Epoch 15 - Save Best Score: 0.8146 Model\n",
      "Epoch 15 - Save Best Loss: 0.8146 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8528(0.8146) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 1.3483(1.3483) Grad: 192003.5469  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7642(0.8246) Grad: 59381.1602  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8815(0.8244) Grad: 50111.7930  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.8097(0.8097) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.8244  avg_val_loss: 0.8055  time: 130s\n",
      "Epoch 16 - avg_train_Score: 0.8244 avgScore: 0.8055\n",
      "Epoch 16 - Save Best Score: 0.8055 Model\n",
      "Epoch 16 - Save Best Loss: 0.8055 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8485(0.8055) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 12m 7s) Loss: 1.1258(1.1258) Grad: 275963.6562  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7338(0.8344) Grad: 67051.6953  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7172(0.8343) Grad: 58826.0430  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7972(0.7972) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.8343  avg_val_loss: 0.8086  time: 130s\n",
      "Epoch 17 - avg_train_Score: 0.8343 avgScore: 0.8086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8559(0.8086) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 12m 58s) Loss: 0.6975(0.6975) Grad: 236934.7656  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6352(0.8330) Grad: 64106.0000  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7473(0.8328) Grad: 53952.2031  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.7936(0.7936) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.8328  avg_val_loss: 0.8037  time: 130s\n",
      "Epoch 18 - avg_train_Score: 0.8328 avgScore: 0.8037\n",
      "Epoch 18 - Save Best Score: 0.8037 Model\n",
      "Epoch 18 - Save Best Loss: 0.8037 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8471(0.8037) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 1.0961(1.0961) Grad: 217601.6250  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6972(0.8161) Grad: 49374.2383  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8178(0.8198) Grad: 49067.6133  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7965(0.7965) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.8198  avg_val_loss: 0.8027  time: 130s\n",
      "Epoch 19 - avg_train_Score: 0.8198 avgScore: 0.8027\n",
      "Epoch 19 - Save Best Score: 0.8027 Model\n",
      "Epoch 19 - Save Best Loss: 0.8027 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8036(0.8027) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 12m 16s) Loss: 0.5696(0.5696) Grad: 199394.6094  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7083(0.7870) Grad: 61231.2812  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6725(0.7887) Grad: 49637.2852  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 51s) Loss: 0.8072(0.8072) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.7887  avg_val_loss: 0.7979  time: 130s\n",
      "Epoch 20 - avg_train_Score: 0.7887 avgScore: 0.7979\n",
      "Epoch 20 - Save Best Score: 0.7979 Model\n",
      "Epoch 20 - Save Best Loss: 0.7979 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8134(0.7979) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 0.7223(0.7223) Grad: 549424.3125  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0687(0.7905) Grad: 53361.0859  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2136(0.7926) Grad: 44355.7070  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 0.8128(0.8128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.7926  avg_val_loss: 0.7961  time: 130s\n",
      "Epoch 21 - avg_train_Score: 0.7926 avgScore: 0.7961\n",
      "Epoch 21 - Save Best Score: 0.7961 Model\n",
      "Epoch 21 - Save Best Loss: 0.7961 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8227(0.7961) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 12m 17s) Loss: 0.7571(0.7571) Grad: 185511.4375  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0245(0.7795) Grad: 62064.7188  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7369(0.7804) Grad: 61376.0742  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7830(0.7830) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.7804  avg_val_loss: 0.7900  time: 130s\n",
      "Epoch 22 - avg_train_Score: 0.7804 avgScore: 0.7900\n",
      "Epoch 22 - Save Best Score: 0.7900 Model\n",
      "Epoch 22 - Save Best Loss: 0.7900 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8153(0.7900) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 12m 14s) Loss: 0.9657(0.9657) Grad: 214544.2656  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6291(0.7610) Grad: 56183.8320  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7158(0.7617) Grad: 45262.3242  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 53s) Loss: 0.7877(0.7877) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8421(0.7883) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.7617  avg_val_loss: 0.7883  time: 130s\n",
      "Epoch 23 - avg_train_Score: 0.7617 avgScore: 0.7883\n",
      "Epoch 23 - Save Best Score: 0.7883 Model\n",
      "Epoch 23 - Save Best Loss: 0.7883 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.8424(0.8424) Grad: 251707.8750  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8171(0.7496) Grad: 89136.3828  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6957(0.7497) Grad: 152597.8750  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7838(0.7838) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.7497  avg_val_loss: 0.7882  time: 130s\n",
      "Epoch 24 - avg_train_Score: 0.7497 avgScore: 0.7882\n",
      "Epoch 24 - Save Best Score: 0.7882 Model\n",
      "Epoch 24 - Save Best Loss: 0.7882 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8566(0.7882) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 12m 48s) Loss: 0.6162(0.6162) Grad: 161797.4531  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 41s (remain 0m 8s) Loss: 0.7500(0.7468) Grad: 90094.5781  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5767(0.7442) Grad: 87891.4375  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7998(0.7998) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.7442  avg_val_loss: 0.7846  time: 130s\n",
      "Epoch 25 - avg_train_Score: 0.7442 avgScore: 0.7846\n",
      "Epoch 25 - Save Best Score: 0.7846 Model\n",
      "Epoch 25 - Save Best Loss: 0.7846 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8714(0.7846) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 12m 22s) Loss: 1.2340(1.2340) Grad: 152282.7812  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5672(0.7118) Grad: 101391.5000  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6695(0.7143) Grad: 111145.7109  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 0.8091(0.8091) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8384(0.7843) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.7143  avg_val_loss: 0.7843  time: 130s\n",
      "Epoch 26 - avg_train_Score: 0.7143 avgScore: 0.7843\n",
      "Epoch 26 - Save Best Score: 0.7843 Model\n",
      "Epoch 26 - Save Best Loss: 0.7843 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 0.7041(0.7041) Grad: 193458.6719  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9900(0.7540) Grad: 60251.5586  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8596(0.7531) Grad: 41398.4883  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 0.7961(0.7961) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8458(0.7823) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.7531  avg_val_loss: 0.7823  time: 130s\n",
      "Epoch 27 - avg_train_Score: 0.7531 avgScore: 0.7823\n",
      "Epoch 27 - Save Best Score: 0.7823 Model\n",
      "Epoch 27 - Save Best Loss: 0.7823 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 0.6369(0.6369) Grad: 149696.0000  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7519(0.7399) Grad: 52264.9453  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0346(0.7372) Grad: 38862.7188  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.7769(0.7769) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.7372  avg_val_loss: 0.7831  time: 130s\n",
      "Epoch 28 - avg_train_Score: 0.7372 avgScore: 0.7831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8603(0.7831) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.7640(0.7640) Grad: 225341.2031  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6952(0.7276) Grad: 26266.4375  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0933(0.7264) Grad: 23020.2949  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7827(0.7827) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.7264  avg_val_loss: 0.7804  time: 130s\n",
      "Epoch 29 - avg_train_Score: 0.7264 avgScore: 0.7804\n",
      "Epoch 29 - Save Best Score: 0.7804 Model\n",
      "Epoch 29 - Save Best Loss: 0.7804 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8185(0.7804) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 12m 42s) Loss: 0.6262(0.6262) Grad: 192533.2969  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9210(0.7008) Grad: 87636.8047  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.4843(0.7023) Grad: 95307.1562  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.7887(0.7887) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.7023  avg_val_loss: 0.7794  time: 130s\n",
      "Epoch 30 - avg_train_Score: 0.7023 avgScore: 0.7794\n",
      "Epoch 30 - Save Best Score: 0.7794 Model\n",
      "Epoch 30 - Save Best Loss: 0.7794 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8181(0.7794) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 0.4912(0.4912) Grad: 173003.1719  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5232(0.6917) Grad: 90733.7734  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8103(0.6947) Grad: 83073.3203  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.7812(0.7812) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.6947  avg_val_loss: 0.7766  time: 130s\n",
      "Epoch 31 - avg_train_Score: 0.6947 avgScore: 0.7766\n",
      "Epoch 31 - Save Best Score: 0.7766 Model\n",
      "Epoch 31 - Save Best Loss: 0.7766 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8170(0.7766) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 0.8141(0.8141) Grad: 173418.4062  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6552(0.6843) Grad: 85444.3203  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6270(0.6843) Grad: 86937.5547  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.7789(0.7789) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.6843  avg_val_loss: 0.7745  time: 130s\n",
      "Epoch 32 - avg_train_Score: 0.6843 avgScore: 0.7745\n",
      "Epoch 32 - Save Best Score: 0.7745 Model\n",
      "Epoch 32 - Save Best Loss: 0.7745 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8136(0.7745) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 12m 38s) Loss: 0.7790(0.7790) Grad: 199671.8750  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7172(0.6863) Grad: 87611.3203  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7353(0.6853) Grad: 117422.3047  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7820(0.7820) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.6853  avg_val_loss: 0.7759  time: 130s\n",
      "Epoch 33 - avg_train_Score: 0.6853 avgScore: 0.7759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8093(0.7759) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.5529(0.5529) Grad: 192909.7500  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8899(0.6913) Grad: 64735.5859  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6231(0.6896) Grad: 50167.2852  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7719(0.7719) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.6896  avg_val_loss: 0.7746  time: 130s\n",
      "Epoch 34 - avg_train_Score: 0.6896 avgScore: 0.7746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8262(0.7746) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 13m 17s) Loss: 0.7792(0.7792) Grad: 185138.3438  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5615(0.6799) Grad: 46924.6992  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5399(0.6795) Grad: 43290.4531  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.7697(0.7697) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8347(0.7737) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.6795  avg_val_loss: 0.7737  time: 130s\n",
      "Epoch 35 - avg_train_Score: 0.6795 avgScore: 0.7737\n",
      "Epoch 35 - Save Best Score: 0.7737 Model\n",
      "Epoch 35 - Save Best Loss: 0.7737 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 12m 38s) Loss: 0.6645(0.6645) Grad: 213397.5469  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6246(0.6835) Grad: 103789.5625  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5339(0.6835) Grad: 212426.8594  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7770(0.7770) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8237(0.7740) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.6835  avg_val_loss: 0.7740  time: 130s\n",
      "Epoch 36 - avg_train_Score: 0.6835 avgScore: 0.7740\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 12m 18s) Loss: 0.5667(0.5667) Grad: 226639.3594  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5951(0.6742) Grad: 41469.7500  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7172(0.6767) Grad: 64191.7227  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 51s) Loss: 0.7788(0.7788) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8212(0.7714) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.6767  avg_val_loss: 0.7714  time: 130s\n",
      "Epoch 37 - avg_train_Score: 0.6767 avgScore: 0.7714\n",
      "Epoch 37 - Save Best Score: 0.7714 Model\n",
      "Epoch 37 - Save Best Loss: 0.7714 Model\n",
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 0.5583(0.5583) Grad: 161645.2812  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6499(0.6703) Grad: 105591.2109  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2705(0.6734) Grad: 83851.4531  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7726(0.7726) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.6734  avg_val_loss: 0.7704  time: 130s\n",
      "Epoch 38 - avg_train_Score: 0.6734 avgScore: 0.7704\n",
      "Epoch 38 - Save Best Score: 0.7704 Model\n",
      "Epoch 38 - Save Best Loss: 0.7704 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8130(0.7704) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 12m 34s) Loss: 0.5460(0.5460) Grad: 180636.7344  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5078(0.6768) Grad: 46745.6484  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5320(0.6758) Grad: 45843.2148  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.7762(0.7762) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.6758  avg_val_loss: 0.7717  time: 130s\n",
      "Epoch 39 - avg_train_Score: 0.6758 avgScore: 0.7717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8231(0.7717) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_217129/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 12m 29s) Loss: 0.5392(0.5392) Grad: 194859.7969  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6496(0.6731) Grad: 57425.9922  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5762(0.6745) Grad: 43695.7383  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 49s) Loss: 0.7750(0.7750) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.6745  avg_val_loss: 0.7702  time: 130s\n",
      "Epoch 40 - avg_train_Score: 0.6745 avgScore: 0.7702\n",
      "Epoch 40 - Save Best Score: 0.7702 Model\n",
      "Epoch 40 - Save Best Loss: 0.7702 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8177(0.7702) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 4 result ==========\n",
      "score: 0.7702\n",
      "========== CV ==========\n",
      "score: 0.7855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "            if CFG.objective_cv == 'binary':\n",
    "                outputs.append(torch.sigmoid(model(x)).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'multiclass':\n",
    "                outputs.append(torch.softmax(\n",
    "                    model(x), axis=1).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'regression':\n",
    "                outputs.append(model(x).to('cpu').numpy())\n",
    "\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "\n",
    "def test_fn(valid_loader, model, device):\n",
    "    preds = []\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        preds.append(y_preds)\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def inference():\n",
    "    test = pd.read_csv(CFG.comp_dataset_path +\n",
    "                       'test_features.csv')\n",
    "\n",
    "    test['base_path'] = CFG.comp_dataset_path + 'images/' + test['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in test['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    print(test.head(5))\n",
    "\n",
    "    valid_dataset = CustomDataset(\n",
    "        test, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = EnsembleModel()\n",
    "    folds = [0] if CFG.use_holdout else list(range(CFG.n_fold))\n",
    "    for fold in folds:\n",
    "        _model = CustomModel(CFG, pretrained=False)\n",
    "        _model.to(device)\n",
    "\n",
    "        model_path = CFG.model_dir + \\\n",
    "            f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth'\n",
    "        print('load', model_path)\n",
    "        state = torch.load(model_path)['model']\n",
    "        _model.load_state_dict(state)\n",
    "        _model.eval()\n",
    "\n",
    "        # _model = tta.ClassificationTTAWrapper(\n",
    "        #     _model, tta.aliases.five_crop_transform(256, 256))\n",
    "\n",
    "        model.add_model(_model)\n",
    "\n",
    "    preds = test_fn(valid_loader, model, device)\n",
    "\n",
    "    test[CFG.target_col] = preds\n",
    "    test.to_csv(CFG.submission_dir +\n",
    "                'submission_oof.csv', index=False)\n",
    "    test[CFG.target_col].to_csv(\n",
    "        CFG.submission_dir + f'submission_{CFG.exp_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-0.5.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355c6c20a0144e82b222ab718fcc661f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     ID      vEgo      aEgo  steeringAngleDeg  \\\n",
      "0  012baccc145d400c896cb82065a93d42_120  3.374273 -0.019360        -34.008415   \n",
      "1  012baccc145d400c896cb82065a93d42_220  2.441048 -0.022754        307.860077   \n",
      "2  012baccc145d400c896cb82065a93d42_320  3.604152 -0.286239         10.774388   \n",
      "3  012baccc145d400c896cb82065a93d42_420  2.048902 -0.537628         61.045235   \n",
      "4  01d738e799d260a10f6324f78023b38f_120  2.201528 -1.898600          5.740093   \n",
      "\n",
      "   steeringTorque  brake  brakePressed  gas  gasPressed gearShifter  \\\n",
      "0            17.0    0.0         False  0.0       False       drive   \n",
      "1           295.0    0.0          True  0.0       False       drive   \n",
      "2          -110.0    0.0          True  0.0       False       drive   \n",
      "3           189.0    0.0          True  0.0       False       drive   \n",
      "4           -41.0    0.0          True  0.0       False       drive   \n",
      "\n",
      "   leftBlinker  rightBlinker  \\\n",
      "0        False         False   \n",
      "1        False         False   \n",
      "2        False         False   \n",
      "3         True         False   \n",
      "4        False         False   \n",
      "\n",
      "                                           base_path  \n",
      "0  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "1  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "2  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "3  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "4  ../raw/atmacup_18_dataset/images/01d738e799d26...  \n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_small_2/atmacup_18-models/swin_small_patch4_window7_224_fold0_last.pth\n",
      "pretrained: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217129/610043316.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path)['model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_small_2/atmacup_18-models/swin_small_patch4_window7_224_fold1_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_small_2/atmacup_18-models/swin_small_patch4_window7_224_fold2_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_small_2/atmacup_18-models/swin_small_patch4_window7_224_fold3_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_small_2/atmacup_18-models/swin_small_patch4_window7_224_fold4_last.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dd05f60c56445e8ef3102f38023eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
