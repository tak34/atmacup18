{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from timm.utils.model_ema import ModelEmaV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set dataset path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'atmacup_18'  # comp名\n",
    "\n",
    "    comp_dataset_path = '../raw/atmacup_18_dataset/'\n",
    "\n",
    "    exp_name = 'atmacup_18_cnn_conv_tiny'\n",
    "\n",
    "    is_debug = False\n",
    "    use_gray_scale = False\n",
    "\n",
    "    model_in_chans = 9  # モデルの入力チャンネル数\n",
    "\n",
    "    # ============== file path =============\n",
    "    train_fold_dir = \"../proc/baseline/folds/\"\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = \"convnextv2_tiny\"\n",
    "\n",
    "    num_frames = 3  # model_in_chansの倍数\n",
    "    norm_in_chans = 1 if use_gray_scale else 3\n",
    "\n",
    "    use_torch_compile = False\n",
    "    use_ema = True\n",
    "    ema_decay = 0.995\n",
    "    # ============== training cfg =============\n",
    "    size = 224  # 224\n",
    "\n",
    "    batch_size = 64  # 32\n",
    "\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 40\n",
    "    if is_debug:\n",
    "        epochs = 2\n",
    "\n",
    "    # adamW warmupあり\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-4\n",
    "    if scheduler == 'GradualWarmupSchedulerV2':\n",
    "        lr /= warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    n_fold = 5\n",
    "    use_holdout = False\n",
    "    use_alldata = False\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    skf_col = 'class'\n",
    "    group_col = 'scene'\n",
    "    fold_type = 'gkf'\n",
    "\n",
    "    objective_cv = 'regression'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'minimize'  # 'maximize', 'minimize'\n",
    "    metrics = 'calc_mae_atmacup'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 18\n",
    "    target_col = ['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1', 'x_2', 'y_2',\n",
    "                  'z_2', 'x_3', 'y_3', 'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5']\n",
    "\n",
    "\n",
    "    # ============== ほぼ固定 =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'last'  # 'best'\n",
    "\n",
    "    min_lr = 5e-8\n",
    "    weight_decay = 1e-5\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== set dataset path =============\n",
    "    if exp_name is not None:\n",
    "        print('set dataset path')\n",
    "\n",
    "        outputs_path = f'../proc/baseline/outputs/{exp_name}/'\n",
    "\n",
    "        submission_dir = outputs_path + 'submissions/'\n",
    "        submission_path = submission_dir + f'submission_{exp_name}.csv'\n",
    "\n",
    "        model_dir = outputs_path + \\\n",
    "            f'{comp_name}-models/'\n",
    "\n",
    "        figures_dir = outputs_path + 'figures/'\n",
    "\n",
    "        log_dir = outputs_path + 'logs/'\n",
    "        log_path = log_dir + f'{exp_name}.txt'\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        A.Downscale(p=0.25),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.ShiftScaleRotate(p=0.5),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        # A.CoarseDropout(max_holes=1, max_height=int(\n",
    "        #     size * 0.3), max_width=int(size * 0.3), p=0.5),\n",
    "        A.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA が利用可能か: True\n",
      "利用可能な CUDA デバイス数: 1\n",
      "現在の CUDA デバイス: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA が利用可能か:\", torch.cuda.is_available())\n",
    "print(\"利用可能な CUDA デバイス数:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"現在の CUDA デバイス:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "def get_fold(train, cfg):\n",
    "    if cfg.fold_type == 'kf':\n",
    "        Fold = KFold(n_splits=cfg.n_fold,\n",
    "                     shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.target_col])\n",
    "    elif cfg.fold_type == 'skf':\n",
    "        Fold = StratifiedKFold(n_splits=cfg.n_fold,\n",
    "                               shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.skf_col])\n",
    "    elif cfg.fold_type == 'gkf':\n",
    "        Fold = GroupKFold(n_splits=cfg.n_fold)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.group_col], groups)\n",
    "    elif cfg.fold_type == 'sgkf':\n",
    "        Fold = StratifiedGroupKFold(n_splits=cfg.n_fold,\n",
    "                                    shuffle=True, random_state=cfg.seed)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.skf_col], groups)\n",
    "    # elif fold_type == 'mskf':\n",
    "    #     Fold = MultilabelStratifiedKFold(\n",
    "    #         n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    #     kf = Fold.split(train, train[cfg.skf_col])\n",
    "\n",
    "    for n, (train_index, val_index) in enumerate(kf):\n",
    "        train.loc[val_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "    print(train.groupby('fold').size())\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_folds():\n",
    "    train_df = pd.read_csv(CFG.comp_dataset_path + 'train_features.csv')\n",
    "\n",
    "    train_df['scene'] = train_df['ID'].str.split('_').str[0]\n",
    "\n",
    "    print('group', CFG.group_col)\n",
    "    print(f'train len: {len(train_df)}')\n",
    "\n",
    "    train_df = get_fold(train_df, CFG)\n",
    "\n",
    "    # print(train_df.groupby(['fold', CFG.target_col]).size())\n",
    "    print(train_df['fold'].value_counts())\n",
    "\n",
    "    os.makedirs(CFG.train_fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(CFG.train_fold_dir +\n",
    "                    'train_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group scene\n",
      "train len: 43371\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "dtype: int64\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "make_train_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数固定\n",
    "def set_seed(seed=None, cudnn_deterministic=True):\n",
    "    if seed is None:\n",
    "        seed = 42\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def make_dirs(cfg):\n",
    "    for dir in [cfg.model_dir, cfg.figures_dir, cfg.submission_dir, cfg.log_dir]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def cfg_init(cfg, mode='train'):\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if mode == 'train':\n",
    "        make_dirs(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_init(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common_utils.logger import init_logger, wandb_init, AverageMeter, timeSince\n",
    "# from common_utils.settings import cfg_init\n",
    "\n",
    "def init_logger(log_file):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- exp_info -----------------\n",
      "2024年11月23日 07:41:12\n"
     ]
    }
   ],
   "source": [
    "Logger = init_logger(log_file=CFG.log_path)\n",
    "\n",
    "Logger.info('\\n\\n-------- exp_info -----------------')\n",
    "Logger.info(datetime.datetime.now().strftime('%Y年%m月%d日 %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    # return roc_auc_score(y_true, y_pred)\n",
    "    eval_func = eval(CFG.metrics)\n",
    "    return eval_func(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calc_mae_atmacup(y_true, y_pred):\n",
    "    abs_diff = np.abs(y_true - y_pred)  # 各予測の差分の絶対値を計算して\n",
    "    mae = np.mean(abs_diff.reshape(-1, ))  # 予測の差分の絶対値の平均を計算\n",
    "\n",
    "    return mae\n",
    "\n",
    "def get_result(result_df):\n",
    "\n",
    "    # preds = result_df['preds'].values\n",
    "\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "    preds = result_df[pred_cols].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    Logger.info(f'score: {score:<.4f}')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_traffic_light(image, id):\n",
    "    path = f'./datasets/atmacup_18/traffic_lights/{id}.json'\n",
    "    traffic_lights = json.load(open(path))\n",
    "\n",
    "    traffic_class = ['green',\n",
    "                     'straight', 'left', 'right', 'empty', 'other', 'yellow', 'red']\n",
    "    class_to_idx = {\n",
    "        cls: idx for idx, cls in enumerate(traffic_class)\n",
    "    }\n",
    "\n",
    "    for traffic_light in traffic_lights:\n",
    "        bbox = traffic_light['bbox']\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # int\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        point1 = (x1, y1)\n",
    "        point2 = (x2, y2)\n",
    "\n",
    "        idx = class_to_idx[traffic_light['class']]\n",
    "        color = 255 - int(255*(idx/len(traffic_class)))\n",
    "\n",
    "        cv2.rectangle(image, point1, point2, color=color, thickness=1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_image_for_cache(path):\n",
    "    if CFG.use_gray_scale:\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # image = cv2.resize(image, (CFG.size, CFG.size))\n",
    "\n",
    "    # 効かない\n",
    "    # image = draw_traffic_light(image, path.split('/')[-2])\n",
    "    return (path, image)\n",
    "\n",
    "\n",
    "def make_video_cache(paths):\n",
    "    debug = []\n",
    "    for idx in range(9):\n",
    "        color = 255 - int(255*(idx/9))\n",
    "        debug.append(color)\n",
    "    print(debug)\n",
    "\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        res = pool.imap_unordered(read_image_for_cache, paths)\n",
    "        res = tqdm(res)\n",
    "        res = list(res)\n",
    "\n",
    "    return dict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import ReplayCompose\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        # aug = A.Compose(cfg.train_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        # aug = A.Compose(cfg.valid_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, cfg, labels=None, transform=None):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.base_paths = df['base_path'].values\n",
    "        # self.labels = df[self.cfg.target_col].values\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def read_image_multiframe(self, idx):\n",
    "        base_path = self.base_paths[idx]\n",
    "\n",
    "        images = []\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "\n",
    "            image = self.cfg.video_cache[path]\n",
    "\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.read_image_multiframe(idx)\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image=image)['image']\n",
    "            replay = None\n",
    "            images = []\n",
    "            for img in image:\n",
    "                if replay is None:\n",
    "                    sample = self.transform(image=img)\n",
    "                    replay = sample['replay']\n",
    "                else:\n",
    "                    sample = ReplayCompose.replay(replay, image=img)\n",
    "                images.append(sample['image'])\n",
    "\n",
    "            image = torch.concat(images, dim=0)\n",
    "\n",
    "        if self.labels is None:\n",
    "            return image\n",
    "\n",
    "        if self.cfg.objective_cv == 'multiclass':\n",
    "            label = torch.tensor(self.labels[idx]).long()\n",
    "        else:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aug_video(train, cfg, plot_count=1):\n",
    "    transform = CFG.train_aug_list\n",
    "    transform = A.ReplayCompose(transform)\n",
    "\n",
    "    dataset = CustomDataset(\n",
    "        train, CFG, transform=transform)\n",
    "\n",
    "    for i in range(plot_count):\n",
    "        image = dataset.read_image_multiframe(i)\n",
    "\n",
    "        if cfg.use_gray_scale:\n",
    "            image = np.stack(image, axis=2)\n",
    "        else:\n",
    "            image = np.concatenate(image, axis=2)\n",
    "\n",
    "        aug_image = dataset[i]\n",
    "        # torch to numpy\n",
    "        aug_image = aug_image.permute(1, 2, 0).numpy()*255\n",
    "\n",
    "        for frame in range(image.shape[-1]):\n",
    "            if frame % 3 != 0:\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            if cfg.use_gray_scale:\n",
    "                axes[0].imshow(image[..., frame], cmap=\"gray\")\n",
    "                axes[1].imshow(aug_image[..., frame], cmap=\"gray\")\n",
    "            else:\n",
    "                axes[0].imshow(image[..., frame:frame+3].astype(int))\n",
    "                axes[1].imshow(aug_image[..., frame:frame+3].astype(int))\n",
    "            plt.savefig(cfg.figures_dir +\n",
    "                        f'aug_{i}_frame{frame}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # xの次元数が4（バッチ、チャネル、高さ、幅）であることを確認\n",
    "        if x.dim() != 4:\n",
    "            raise ValueError(f'Expected 4D input (got {x.dim()}D input)')\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "        # self.cfg = cfg\n",
    "\n",
    "        if model_name is None:\n",
    "            model_name = cfg.model_name\n",
    "\n",
    "        print(f'pretrained: {pretrained}')\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=0,\n",
    "            in_chans=cfg.model_in_chans)\n",
    "\n",
    "        # モデルの出力サイズを取得\n",
    "        if hasattr(self.model, 'num_features'):\n",
    "            self.n_features = self.model.num_features  # num_featuresで取得するモデルが多い\n",
    "        elif hasattr(self.model, 'classifier') and hasattr(self.model.classifier, 'in_features'):\n",
    "            self.n_features = self.model.classifier.in_features  # classifierが存在する場合\n",
    "        elif hasattr(self.model, 'fc') and hasattr(self.model.fc, 'in_features'):\n",
    "            self.n_features = self.model.fc.in_features  # fcが存在する場合\n",
    "        else:\n",
    "            raise AttributeError(\"Could not find the output feature size.\")\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "\n",
    "        # モデルのヘッド部分をIdentityで置き換え\n",
    "        self.model.reset_classifier(0)  # timmで全結合層をリセットする簡単な方法\n",
    "\n",
    "        # カスタム層\n",
    "        self.pooling = GeM()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(self.n_features, self.target_size)\n",
    "\n",
    "    def forward(self, image):\n",
    "        features = self.model.forward_features(image)  # ここを変更\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        dropped_features = self.dropout(pooled_features)\n",
    "        output = self.linear(dropped_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(\n",
    "            optimizer, multiplier, total_epoch, after_scheduler)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [\n",
    "                        base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "def get_scheduler(cfg, optimizer):\n",
    "    if cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=cfg.factor, patience=cfg.patience, verbose=True, eps=cfg.eps)\n",
    "    elif cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer, T_max=cfg.epochs, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=cfg.T_0, T_mult=1, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'GradualWarmupSchedulerV2':\n",
    "        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, cfg.epochs, eta_min=1e-7)\n",
    "        scheduler = GradualWarmupSchedulerV2(\n",
    "            optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def scheduler_step(scheduler, avg_val_loss, epoch):\n",
    "    if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        scheduler.step(avg_val_loss)\n",
    "    elif isinstance(scheduler, CosineAnnealingLR):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, GradualWarmupSchedulerV2):\n",
    "        scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device,\n",
    "             model_ema=None):\n",
    "    \"\"\" 1epoch毎のtrain \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.use_amp)\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    preds_labels = []\n",
    "    start = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with autocast(CFG.use_amp):\n",
    "            y_preds = model(images)\n",
    "\n",
    "            if y_preds.size(1) == 1:\n",
    "                y_preds = y_preds.view(-1)\n",
    "\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.detach().to('cpu').numpy())\n",
    "\n",
    "        preds_labels.append(labels.detach().to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(preds_labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        if y_preds.size(1) == 1:\n",
    "            y_preds = y_preds.view(-1)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # binary\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(folds, fold):\n",
    "\n",
    "    Logger.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    if CFG.use_alldata:\n",
    "        train_folds = folds.copy().reset_index(drop=True)\n",
    "    else:\n",
    "        train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # train_folds = train_downsampling(train_folds)\n",
    "\n",
    "    train_labels = train_folds[CFG.target_col].values\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        train_folds, CFG, labels=train_labels, transform=get_transforms(data='train', cfg=CFG))\n",
    "    valid_dataset = CustomDataset(\n",
    "        valid_folds, CFG, labels=valid_labels, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                              )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "\n",
    "    model = CustomModel(CFG, pretrained=CFG.pretrained)\n",
    "    model.to(device)\n",
    "\n",
    "    if CFG.use_ema:\n",
    "        model_ema = ModelEmaV2(model, decay=CFG.ema_decay)\n",
    "    else:\n",
    "        model_ema = None\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr)\n",
    "    scheduler = get_scheduler(CFG, optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    if CFG.objective_cv == 'binary':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif CFG.objective_cv == 'multiclass':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif CFG.objective_cv == 'regression':\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "    if CFG.metric_direction == 'minimize':\n",
    "        best_score = np.inf\n",
    "    elif CFG.metric_direction == 'maximize':\n",
    "        best_score = -1\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    df_score = pd.DataFrame(columns=[\"train_loss\", 'train_score', 'val_loss', 'val_score'])\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss, train_preds, train_labels_epoch = train_fn(fold, train_loader, model,\n",
    "                                                             criterion, optimizer, epoch, scheduler, device, model_ema)\n",
    "        train_score = get_score(train_labels_epoch, train_preds)\n",
    "\n",
    "        # eval\n",
    "        if model_ema is not None:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model_ema.module, criterion, device)\n",
    "        else:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model, criterion, device)\n",
    "\n",
    "        scheduler_step(scheduler, avg_val_loss, epoch)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, valid_preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        # Logger.info(f'Epoch {epoch+1} - avgScore: {avg_score:.4f}')\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_Score: {train_score:.4f} avgScore: {score:.4f}')\n",
    "        \n",
    "        df_score.loc[epoch] = [avg_loss, train_score, avg_val_loss, score]\n",
    "\n",
    "        if CFG.metric_direction == 'minimize':\n",
    "            update_best = score < best_score\n",
    "        elif CFG.metric_direction == 'maximize':\n",
    "            update_best = score > best_score\n",
    "\n",
    "        if update_best:\n",
    "            best_loss = avg_val_loss\n",
    "            best_score = score\n",
    "\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "\n",
    "            if model_ema is not None:\n",
    "                torch.save({'model': model_ema.module.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "            else:\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save({'model': model.state_dict(),\n",
    "                'preds': valid_preds},\n",
    "               CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    \"\"\"\n",
    "    if model_ema is not None:\n",
    "        torch.save({'model': model_ema.module.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    else:\n",
    "        torch.save({'model': model.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "\n",
    "    check_point = torch.load(\n",
    "        CFG.model_dir + f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth', map_location=torch.device('cpu'))\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "\n",
    "    check_point_pred = check_point['preds']\n",
    "\n",
    "    # Columns must be same length as key 対策\n",
    "    if check_point_pred.ndim == 1:\n",
    "        check_point_pred = check_point_pred.reshape(-1, CFG.target_size)\n",
    "\n",
    "    print('check_point_pred shape', check_point_pred.shape)\n",
    "    valid_folds[pred_cols] = check_point_pred\n",
    "    return valid_folds, df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = pd.read_csv(CFG.train_fold_dir + 'train_folds.csv')\n",
    "    train['ori_idx'] = train.index\n",
    "\n",
    "    train['scene'] = train['ID'].str.split('_').str[0]\n",
    "\n",
    "    \"\"\"\n",
    "    if CFG.is_debug:\n",
    "        use_ids = train['scene'].unique()[:100]\n",
    "        train = train[train['scene'].isin(use_ids)].reset_index(drop=True)\n",
    "    \"\"\"\n",
    "\n",
    "    train['base_path'] = CFG.comp_dataset_path + 'images/' + train['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in train['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    # plot_aug_video(train, CFG, plot_count=10)\n",
    "\n",
    "    # train\n",
    "    oof_df = pd.DataFrame()\n",
    "    list_df_score = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold not in CFG.train_folds:\n",
    "            print(f'fold {fold} is skipped')\n",
    "            continue\n",
    "\n",
    "        _oof_df, _df_score = train_fold(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        list_df_score.append(_df_score)\n",
    "        Logger.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "\n",
    "        if CFG.use_holdout or CFG.use_alldata:\n",
    "            break\n",
    "\n",
    "    oof_df = oof_df.sort_values('ori_idx').reset_index(drop=True)\n",
    "\n",
    "    # CV result\n",
    "    Logger.info(\"========== CV ==========\")\n",
    "    score = get_result(oof_df)\n",
    "\n",
    "    # 学習曲線を可視化する\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.grid(alpha=0.1)\n",
    "    ax2.grid(alpha=0.1)\n",
    "    for i, df_score in enumerate(list_df_score):\n",
    "        ax1.plot(df_score['train_score'], label=f'fold {i}')\n",
    "        ax2.plot(df_score['val_score'], label=f'fold {i}')\n",
    "    ax1.set_title('Train Score')\n",
    "    ax2.set_title('Val Score') \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Score')\n",
    "    ax2.set_ylabel('Val Score')\n",
    "    ax1.set_ylim([0, 1.5])\n",
    "    ax2.set_ylim([0, 1.5])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CFG.figures_dir + f'learning_curve_{CFG.exp_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # save result\n",
    "    oof_df.to_csv(CFG.submission_dir + 'oof_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-0.5.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b2df8113c24385a2a5b920ddc2ae08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 2s (remain 21m 1s) Loss: 6.6007(6.6007) Grad: inf  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 1.7439(2.6871) Grad: 55118.6953  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 1.5001(2.6140) Grad: 43071.0391  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 1.4243(1.4243) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.6140  avg_val_loss: 1.6035  time: 122s\n",
      "Epoch 1 - avg_train_Score: 2.6140 avgScore: 1.6035\n",
      "Epoch 1 - Save Best Score: 1.6035 Model\n",
      "Epoch 1 - Save Best Loss: 1.6035 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.5456(1.6035) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 11m 34s) Loss: 1.5585(1.5585) Grad: inf  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 1.3796(1.5794) Grad: 35073.1992  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 1.5665(1.5737) Grad: 44824.8125  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 1.1646(1.1646) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.5737  avg_val_loss: 1.1712  time: 121s\n",
      "Epoch 2 - avg_train_Score: 1.5737 avgScore: 1.1712\n",
      "Epoch 2 - Save Best Score: 1.1712 Model\n",
      "Epoch 2 - Save Best Loss: 1.1712 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.1685(1.1712) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 11m 19s) Loss: 1.4235(1.4235) Grad: inf  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 1.1844(1.4818) Grad: 43498.3516  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 1.4154(1.4615) Grad: 69774.0312  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 1.0421(1.0421) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.4615  avg_val_loss: 1.0300  time: 121s\n",
      "Epoch 3 - avg_train_Score: 1.4615 avgScore: 1.0300\n",
      "Epoch 3 - Save Best Score: 1.0300 Model\n",
      "Epoch 3 - Save Best Loss: 1.0300 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9936(1.0300) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 13m 24s) Loss: 1.0247(1.0247) Grad: inf  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 1.1005(1.0305) Grad: 170802.2500  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 1.0283(1.0280) Grad: 108111.6406  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.9443(0.9443) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0280  avg_val_loss: 0.9054  time: 121s\n",
      "Epoch 4 - avg_train_Score: 1.0280 avgScore: 0.9054\n",
      "Epoch 4 - Save Best Score: 0.9054 Model\n",
      "Epoch 4 - Save Best Loss: 0.9054 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8799(0.9054) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 12m 40s) Loss: 0.9417(0.9417) Grad: inf  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.7802(0.8880) Grad: 258935.8906  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.7837(0.8872) Grad: 226214.3594  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.8575(0.8575) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.8872  avg_val_loss: 0.8527  time: 121s\n",
      "Epoch 5 - avg_train_Score: 0.8872 avgScore: 0.8527\n",
      "Epoch 5 - Save Best Score: 0.8527 Model\n",
      "Epoch 5 - Save Best Loss: 0.8527 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8277(0.8527) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 11m 46s) Loss: 0.7276(0.7276) Grad: inf  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.7408(0.8440) Grad: 138184.4375  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.8976(0.8450) Grad: 128264.0000  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.8289(0.8289) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8450  avg_val_loss: 0.8231  time: 121s\n",
      "Epoch 6 - avg_train_Score: 0.8450 avgScore: 0.8231\n",
      "Epoch 6 - Save Best Score: 0.8231 Model\n",
      "Epoch 6 - Save Best Loss: 0.8231 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8003(0.8231) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 55s) Loss: 0.9355(0.9355) Grad: inf  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.7121(0.7679) Grad: 133005.2969  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.7925(0.7708) Grad: 113339.1484  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8006(0.8006) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7708  avg_val_loss: 0.7949  time: 121s\n",
      "Epoch 7 - avg_train_Score: 0.7708 avgScore: 0.7949\n",
      "Epoch 7 - Save Best Score: 0.7949 Model\n",
      "Epoch 7 - Save Best Loss: 0.7949 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7830(0.7949) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 11m 28s) Loss: 0.8484(0.8484) Grad: inf  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.6394(0.7105) Grad: 191534.0625  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.6184(0.7091) Grad: 200180.1719  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7609(0.7609) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7091  avg_val_loss: 0.7749  time: 121s\n",
      "Epoch 8 - avg_train_Score: 0.7091 avgScore: 0.7749\n",
      "Epoch 8 - Save Best Score: 0.7749 Model\n",
      "Epoch 8 - Save Best Loss: 0.7749 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7402(0.7749) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 43s) Loss: 0.6049(0.6049) Grad: inf  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.7059(0.6634) Grad: 253046.3438  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.7373(0.6647) Grad: 249058.5938  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7524(0.7524) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6647  avg_val_loss: 0.7614  time: 121s\n",
      "Epoch 9 - avg_train_Score: 0.6647 avgScore: 0.7614\n",
      "Epoch 9 - Save Best Score: 0.7614 Model\n",
      "Epoch 9 - Save Best Loss: 0.7614 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7383(0.7614) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 12m 4s) Loss: 0.5282(0.5282) Grad: inf  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5397(0.6198) Grad: 162200.3438  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.6694(0.6205) Grad: 193049.4375  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7083(0.7083) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6205  avg_val_loss: 0.7455  time: 121s\n",
      "Epoch 10 - avg_train_Score: 0.6205 avgScore: 0.7455\n",
      "Epoch 10 - Save Best Score: 0.7455 Model\n",
      "Epoch 10 - Save Best Loss: 0.7455 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7409(0.7455) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 11m 44s) Loss: 0.6086(0.6086) Grad: 757458.6250  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.6881(0.5919) Grad: 204390.9375  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.6432(0.5945) Grad: 198866.9062  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7278(0.7278) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7498(0.7374) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5945  avg_val_loss: 0.7374  time: 121s\n",
      "Epoch 11 - avg_train_Score: 0.5945 avgScore: 0.7374\n",
      "Epoch 11 - Save Best Score: 0.7374 Model\n",
      "Epoch 11 - Save Best Loss: 0.7374 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 11m 21s) Loss: 0.5993(0.5993) Grad: inf  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.6746(0.6269) Grad: 80445.5781  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.5963(0.6280) Grad: 72173.3750  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7587(0.7587) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.6280  avg_val_loss: 0.7350  time: 121s\n",
      "Epoch 12 - avg_train_Score: 0.6280 avgScore: 0.7350\n",
      "Epoch 12 - Save Best Score: 0.7350 Model\n",
      "Epoch 12 - Save Best Loss: 0.7350 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7337(0.7350) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 12m 29s) Loss: 0.6224(0.6224) Grad: inf  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5749(0.5279) Grad: 336684.2188  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.5027(0.5255) Grad: 272915.0625  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7271(0.7271) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.5255  avg_val_loss: 0.7171  time: 121s\n",
      "Epoch 13 - avg_train_Score: 0.5255 avgScore: 0.7171\n",
      "Epoch 13 - Save Best Score: 0.7171 Model\n",
      "Epoch 13 - Save Best Loss: 0.7171 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7287(0.7171) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 0.4826(0.4826) Grad: 448968.8750  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5388(0.4917) Grad: 126462.8594  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.5121(0.4925) Grad: 149662.4844  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7054(0.7054) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4925  avg_val_loss: 0.7115  time: 121s\n",
      "Epoch 14 - avg_train_Score: 0.4925 avgScore: 0.7115\n",
      "Epoch 14 - Save Best Score: 0.7115 Model\n",
      "Epoch 14 - Save Best Loss: 0.7115 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7271(0.7115) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 12m 18s) Loss: 0.6028(0.6028) Grad: 643221.5625  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4914(0.4565) Grad: 254312.3594  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.4583(0.4564) Grad: 137027.2344  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7020(0.7020) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.4564  avg_val_loss: 0.7055  time: 121s\n",
      "Epoch 15 - avg_train_Score: 0.4564 avgScore: 0.7055\n",
      "Epoch 15 - Save Best Score: 0.7055 Model\n",
      "Epoch 15 - Save Best Loss: 0.7055 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7049(0.7055) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 0.4175(0.4175) Grad: inf  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4310(0.4298) Grad: 209388.1562  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.4459(0.4289) Grad: 254875.4531  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7151(0.7151) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4289  avg_val_loss: 0.7027  time: 120s\n",
      "Epoch 16 - avg_train_Score: 0.4289 avgScore: 0.7027\n",
      "Epoch 16 - Save Best Score: 0.7027 Model\n",
      "Epoch 16 - Save Best Loss: 0.7027 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7514(0.7027) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 11m 44s) Loss: 0.4224(0.4224) Grad: 727102.3125  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4199(0.4151) Grad: 215292.5938  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.4555(0.4162) Grad: 266995.5625  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6895(0.6895) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4162  avg_val_loss: 0.7030  time: 121s\n",
      "Epoch 17 - avg_train_Score: 0.4162 avgScore: 0.7030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7667(0.7030) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 53s) Loss: 0.4566(0.4566) Grad: 520768.0938  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4232(0.4072) Grad: 208841.0469  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.5110(0.4084) Grad: 253499.2812  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6949(0.6949) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7330(0.6986) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4084  avg_val_loss: 0.6986  time: 121s\n",
      "Epoch 18 - avg_train_Score: 0.4084 avgScore: 0.6986\n",
      "Epoch 18 - Save Best Score: 0.6986 Model\n",
      "Epoch 18 - Save Best Loss: 0.6986 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 11m 46s) Loss: 0.3130(0.3130) Grad: 388313.2812  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3869(0.3966) Grad: 283179.5312  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3768(0.3979) Grad: 291198.1562  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7125(0.7125) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.3979  avg_val_loss: 0.6979  time: 121s\n",
      "Epoch 19 - avg_train_Score: 0.3979 avgScore: 0.6979\n",
      "Epoch 19 - Save Best Score: 0.6979 Model\n",
      "Epoch 19 - Save Best Loss: 0.6979 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7142(0.6979) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 28s) Loss: 0.4262(0.4262) Grad: 629586.1875  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3563(0.3850) Grad: 226231.9531  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3544(0.3851) Grad: 196975.9844  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6894(0.6894) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.3851  avg_val_loss: 0.6940  time: 121s\n",
      "Epoch 20 - avg_train_Score: 0.3851 avgScore: 0.6940\n",
      "Epoch 20 - Save Best Score: 0.6940 Model\n",
      "Epoch 20 - Save Best Loss: 0.6940 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7155(0.6940) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 11m 25s) Loss: 0.3899(0.3899) Grad: inf  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3652(0.3715) Grad: 207380.2500  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3155(0.3717) Grad: 189379.6250  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6961(0.6961) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3717  avg_val_loss: 0.6924  time: 121s\n",
      "Epoch 21 - avg_train_Score: 0.3717 avgScore: 0.6924\n",
      "Epoch 21 - Save Best Score: 0.6924 Model\n",
      "Epoch 21 - Save Best Loss: 0.6924 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6993(0.6924) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 12m 29s) Loss: 0.4171(0.4171) Grad: 420889.0625  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3434(0.3593) Grad: 182445.2969  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3341(0.3591) Grad: 191936.8594  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6801(0.6801) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3591  avg_val_loss: 0.6879  time: 121s\n",
      "Epoch 22 - avg_train_Score: 0.3591 avgScore: 0.6879\n",
      "Epoch 22 - Save Best Score: 0.6879 Model\n",
      "Epoch 22 - Save Best Loss: 0.6879 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6977(0.6879) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 11m 51s) Loss: 0.3198(0.3198) Grad: 321359.2812  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3360(0.3466) Grad: 197591.6562  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3025(0.3470) Grad: 170737.2812  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6989(0.6989) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3470  avg_val_loss: 0.6882  time: 121s\n",
      "Epoch 23 - avg_train_Score: 0.3470 avgScore: 0.6882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7138(0.6882) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.3145(0.3145) Grad: 350011.4688  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3509(0.3331) Grad: 211132.3281  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3517(0.3337) Grad: 197108.2969  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.6914(0.6914) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3337  avg_val_loss: 0.6893  time: 121s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6732(0.6893) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 - avg_train_Score: 0.3337 avgScore: 0.6893\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 11m 27s) Loss: 0.3381(0.3381) Grad: 386724.2812  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2683(0.3194) Grad: 191376.3594  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3428(0.3199) Grad: 216756.9062  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6936(0.6936) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3199  avg_val_loss: 0.6871  time: 121s\n",
      "Epoch 25 - avg_train_Score: 0.3199 avgScore: 0.6871\n",
      "Epoch 25 - Save Best Score: 0.6871 Model\n",
      "Epoch 25 - Save Best Loss: 0.6871 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6772(0.6871) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 11m 35s) Loss: 0.3268(0.3268) Grad: 329955.6875  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3382(0.3115) Grad: 201172.8438  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3410(0.3116) Grad: 217764.8125  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6874(0.6874) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3116  avg_val_loss: 0.6866  time: 121s\n",
      "Epoch 26 - avg_train_Score: 0.3116 avgScore: 0.6866\n",
      "Epoch 26 - Save Best Score: 0.6866 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6933(0.6866) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Save Best Loss: 0.6866 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 11m 44s) Loss: 0.3243(0.3243) Grad: 357690.6250  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3226(0.2990) Grad: 263744.9062  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3709(0.2990) Grad: 224983.8125  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6884(0.6884) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.2990  avg_val_loss: 0.6843  time: 121s\n",
      "Epoch 27 - avg_train_Score: 0.2990 avgScore: 0.6843\n",
      "Epoch 27 - Save Best Score: 0.6843 Model\n",
      "Epoch 27 - Save Best Loss: 0.6843 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7023(0.6843) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 11m 37s) Loss: 0.2985(0.2985) Grad: 350002.4062  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2803(0.2907) Grad: 125554.5078  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2450(0.2908) Grad: 157831.0625  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6869(0.6869) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.2908  avg_val_loss: 0.6850  time: 121s\n",
      "Epoch 28 - avg_train_Score: 0.2908 avgScore: 0.6850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6979(0.6850) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 11m 33s) Loss: 0.2905(0.2905) Grad: 366548.5938  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2347(0.2728) Grad: 152553.9375  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2991(0.2736) Grad: 158583.9219  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6858(0.6858) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2736  avg_val_loss: 0.6865  time: 121s\n",
      "Epoch 29 - avg_train_Score: 0.2736 avgScore: 0.6865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6958(0.6865) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 11m 29s) Loss: 0.2492(0.2492) Grad: 369655.4688  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2409(0.2720) Grad: 162644.7500  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2702(0.2719) Grad: 281375.6250  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6973(0.6973) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2719  avg_val_loss: 0.6866  time: 120s\n",
      "Epoch 30 - avg_train_Score: 0.2719 avgScore: 0.6866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6873(0.6866) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 11m 40s) Loss: 0.2966(0.2966) Grad: 362387.2188  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2473(0.2614) Grad: 128332.8047  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2781(0.2620) Grad: 185078.6094  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6934(0.6934) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2620  avg_val_loss: 0.6864  time: 121s\n",
      "Epoch 31 - avg_train_Score: 0.2620 avgScore: 0.6864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6889(0.6864) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 11m 50s) Loss: 0.2471(0.2471) Grad: 333186.4688  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2164(0.2524) Grad: 133765.7031  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2565(0.2527) Grad: 149216.3594  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6949(0.6949) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2527  avg_val_loss: 0.6861  time: 121s\n",
      "Epoch 32 - avg_train_Score: 0.2527 avgScore: 0.6861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6912(0.6861) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 12m 32s) Loss: 0.2725(0.2725) Grad: 412726.0625  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2362(0.2461) Grad: 353971.5312  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2686(0.2463) Grad: 372813.5625  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.6844(0.6844) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2463  avg_val_loss: 0.6844  time: 121s\n",
      "Epoch 33 - avg_train_Score: 0.2463 avgScore: 0.6844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6807(0.6844) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 11m 50s) Loss: 0.2429(0.2429) Grad: 317506.7188  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2718(0.2415) Grad: 144353.8906  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2408(0.2420) Grad: 203820.5000  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.6862(0.6862) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2420  avg_val_loss: 0.6843  time: 121s\n",
      "Epoch 34 - avg_train_Score: 0.2420 avgScore: 0.6843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6791(0.6843) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 12m 38s) Loss: 0.2641(0.2641) Grad: 405403.7812  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2044(0.2391) Grad: 269164.9062  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2376(0.2391) Grad: 320696.6875  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6886(0.6886) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2391  avg_val_loss: 0.6850  time: 121s\n",
      "Epoch 35 - avg_train_Score: 0.2391 avgScore: 0.6850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6799(0.6850) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 11m 54s) Loss: 0.2260(0.2260) Grad: 425552.7500  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2939(0.2338) Grad: 365897.5000  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.1980(0.2332) Grad: 275984.0625  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6834(0.6834) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2332  avg_val_loss: 0.6839  time: 121s\n",
      "Epoch 36 - avg_train_Score: 0.2332 avgScore: 0.6839\n",
      "Epoch 36 - Save Best Score: 0.6839 Model\n",
      "Epoch 36 - Save Best Loss: 0.6839 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6847(0.6839) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 12m 6s) Loss: 0.2474(0.2474) Grad: 296809.6875  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2124(0.2322) Grad: 177299.9219  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.1937(0.2317) Grad: 166947.5938  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6809(0.6809) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2317  avg_val_loss: 0.6851  time: 121s\n",
      "Epoch 37 - avg_train_Score: 0.2317 avgScore: 0.6851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6852(0.6851) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 12m 26s) Loss: 0.2313(0.2313) Grad: 333111.0312  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2257(0.2295) Grad: 225173.8438  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2243(0.2292) Grad: 137600.7188  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6772(0.6772) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2292  avg_val_loss: 0.6837  time: 121s\n",
      "Epoch 38 - avg_train_Score: 0.2292 avgScore: 0.6837\n",
      "Epoch 38 - Save Best Score: 0.6837 Model\n",
      "Epoch 38 - Save Best Loss: 0.6837 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6845(0.6837) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 0.2272(0.2272) Grad: 303170.7188  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2365(0.2256) Grad: 278018.9375  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2430(0.2257) Grad: 335895.5938  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.6784(0.6784) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2257  avg_val_loss: 0.6830  time: 121s\n",
      "Epoch 39 - avg_train_Score: 0.2257 avgScore: 0.6830\n",
      "Epoch 39 - Save Best Score: 0.6830 Model\n",
      "Epoch 39 - Save Best Loss: 0.6830 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6840(0.6830) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 11m 50s) Loss: 0.1983(0.1983) Grad: 283249.5312  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2096(0.2235) Grad: 166190.6562  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2308(0.2236) Grad: 139634.3281  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6756(0.6756) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6834(0.6832) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2236  avg_val_loss: 0.6832  time: 121s\n",
      "Epoch 40 - avg_train_Score: 0.2236 avgScore: 0.6832\n",
      "/tmp/ipykernel_565/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 0 result ==========\n",
      "score: 0.6832\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8675, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 12m 51s) Loss: 6.4434(6.4434) Grad: inf  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 2.0583(2.5786) Grad: 226751.2188  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 1.7387(2.5169) Grad: 158390.8750  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 1.7163(1.7163) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.5169  avg_val_loss: 1.5342  time: 121s\n",
      "Epoch 1 - avg_train_Score: 2.5169 avgScore: 1.5342\n",
      "Epoch 1 - Save Best Score: 1.5342 Model\n",
      "Epoch 1 - Save Best Loss: 1.5342 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.3327(1.5342) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 1.7747(1.7747) Grad: inf  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 1.3971(1.6014) Grad: 67445.7422  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 1.5546(1.5940) Grad: 141318.9531  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 1.2530(1.2530) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.5940  avg_val_loss: 1.1416  time: 121s\n",
      "Epoch 2 - avg_train_Score: 1.5940 avgScore: 1.1416\n",
      "Epoch 2 - Save Best Score: 1.1416 Model\n",
      "Epoch 2 - Save Best Loss: 1.1416 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.0270(1.1416) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 12m 26s) Loss: 1.7420(1.7420) Grad: inf  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 1.1527(1.4628) Grad: 67565.9609  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 1.1493(1.4432) Grad: 48668.5586  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 1.0996(1.0996) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.4432  avg_val_loss: 0.9935  time: 121s\n",
      "Epoch 3 - avg_train_Score: 1.4432 avgScore: 0.9935\n",
      "Epoch 3 - Save Best Score: 0.9935 Model\n",
      "Epoch 3 - Save Best Loss: 0.9935 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9329(0.9935) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 12m 22s) Loss: 1.3203(1.3203) Grad: inf  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.9222(1.0335) Grad: 301594.2812  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.9741(1.0296) Grad: 245869.6562  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.9357(0.9357) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0296  avg_val_loss: 0.8786  time: 121s\n",
      "Epoch 4 - avg_train_Score: 1.0296 avgScore: 0.8786\n",
      "Epoch 4 - Save Best Score: 0.8786 Model\n",
      "Epoch 4 - Save Best Loss: 0.8786 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8442(0.8786) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 12m 38s) Loss: 0.9610(0.9610) Grad: inf  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 1.0276(0.9587) Grad: 129984.8594  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.9383(0.9570) Grad: 93334.8203  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.8822(0.8822) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9570  avg_val_loss: 0.8304  time: 121s\n",
      "Epoch 5 - avg_train_Score: 0.9570 avgScore: 0.8304\n",
      "Epoch 5 - Save Best Score: 0.8304 Model\n",
      "Epoch 5 - Save Best Loss: 0.8304 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7954(0.8304) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 12m 7s) Loss: 0.8966(0.8966) Grad: inf  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.7452(0.8060) Grad: 248022.5000  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.8344(0.8030) Grad: 294512.0312  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7856(0.7856) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8030  avg_val_loss: 0.7801  time: 121s\n",
      "Epoch 6 - avg_train_Score: 0.8030 avgScore: 0.7801\n",
      "Epoch 6 - Save Best Score: 0.7801 Model\n",
      "Epoch 6 - Save Best Loss: 0.7801 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7438(0.7801) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 12m 31s) Loss: 0.8082(0.8082) Grad: inf  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.9121(0.8054) Grad: 129584.5078  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.6320(0.8026) Grad: 88946.3203  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7616(0.7616) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8026  avg_val_loss: 0.7535  time: 121s\n",
      "Epoch 7 - avg_train_Score: 0.8026 avgScore: 0.7535\n",
      "Epoch 7 - Save Best Score: 0.7535 Model\n",
      "Epoch 7 - Save Best Loss: 0.7535 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7089(0.7535) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 12m 49s) Loss: 0.7021(0.7021) Grad: 651152.0625  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.7519(0.7250) Grad: 71988.7266  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.6889(0.7258) Grad: 127700.2891  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7477(0.7477) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7258  avg_val_loss: 0.7212  time: 121s\n",
      "Epoch 8 - avg_train_Score: 0.7258 avgScore: 0.7212\n",
      "Epoch 8 - Save Best Score: 0.7212 Model\n",
      "Epoch 8 - Save Best Loss: 0.7212 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7148(0.7212) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 12m 30s) Loss: 0.6795(0.6795) Grad: 477807.1875  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5992(0.6118) Grad: 184456.6094  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.5897(0.6120) Grad: 126865.0312  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7430(0.7430) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6120  avg_val_loss: 0.6968  time: 121s\n",
      "Epoch 9 - avg_train_Score: 0.6120 avgScore: 0.6968\n",
      "Epoch 9 - Save Best Score: 0.6968 Model\n",
      "Epoch 9 - Save Best Loss: 0.6968 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6952(0.6968) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 0.5917(0.5917) Grad: 510551.0000  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5114(0.5523) Grad: 135427.7969  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.5096(0.5529) Grad: 116725.4062  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7226(0.7226) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5529  avg_val_loss: 0.6894  time: 121s\n",
      "Epoch 10 - avg_train_Score: 0.5529 avgScore: 0.6894\n",
      "Epoch 10 - Save Best Score: 0.6894 Model\n",
      "Epoch 10 - Save Best Loss: 0.6894 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6730(0.6894) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 11m 54s) Loss: 0.5695(0.5695) Grad: 512993.1250  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.6228(0.4952) Grad: 219978.9531  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.5033(0.4985) Grad: 143394.2812  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7140(0.7140) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.4985  avg_val_loss: 0.6804  time: 121s\n",
      "Epoch 11 - avg_train_Score: 0.4985 avgScore: 0.6804\n",
      "Epoch 11 - Save Best Score: 0.6804 Model\n",
      "Epoch 11 - Save Best Loss: 0.6804 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6601(0.6804) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 13m 13s) Loss: 0.6523(0.6523) Grad: inf  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5319(0.4714) Grad: 149502.3438  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.5117(0.4756) Grad: 187247.1406  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7334(0.7334) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4756  avg_val_loss: 0.6781  time: 121s\n",
      "Epoch 12 - avg_train_Score: 0.4756 avgScore: 0.6781\n",
      "Epoch 12 - Save Best Score: 0.6781 Model\n",
      "Epoch 12 - Save Best Loss: 0.6781 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6602(0.6781) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 12m 24s) Loss: 0.5812(0.5812) Grad: 499513.3438  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4444(0.4615) Grad: 267956.8438  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.5124(0.4610) Grad: 249833.2031  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7206(0.7206) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4610  avg_val_loss: 0.6736  time: 121s\n",
      "Epoch 13 - avg_train_Score: 0.4610 avgScore: 0.6736\n",
      "Epoch 13 - Save Best Score: 0.6736 Model\n",
      "Epoch 13 - Save Best Loss: 0.6736 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6424(0.6736) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 13m 4s) Loss: 0.4670(0.4670) Grad: 662837.1875  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5079(0.4450) Grad: 137743.0156  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.5684(0.4489) Grad: 147977.8438  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.7294(0.7294) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4489  avg_val_loss: 0.6702  time: 121s\n",
      "Epoch 14 - avg_train_Score: 0.4489 avgScore: 0.6702\n",
      "Epoch 14 - Save Best Score: 0.6702 Model\n",
      "Epoch 14 - Save Best Loss: 0.6702 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6285(0.6702) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 12m 44s) Loss: 0.4982(0.4982) Grad: 587417.3125  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4833(0.4607) Grad: 114543.9844  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.4606(0.4636) Grad: 166412.5156  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7064(0.7064) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.4636  avg_val_loss: 0.6647  time: 121s\n",
      "Epoch 15 - avg_train_Score: 0.4636 avgScore: 0.6647\n",
      "Epoch 15 - Save Best Score: 0.6647 Model\n",
      "Epoch 15 - Save Best Loss: 0.6647 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6300(0.6647) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 12m 41s) Loss: 0.5346(0.5346) Grad: 701198.1875  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4025(0.4373) Grad: 199788.6719  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3877(0.4369) Grad: 227749.8594  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.7097(0.7097) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4369  avg_val_loss: 0.6592  time: 121s\n",
      "Epoch 16 - avg_train_Score: 0.4369 avgScore: 0.6592\n",
      "Epoch 16 - Save Best Score: 0.6592 Model\n",
      "Epoch 16 - Save Best Loss: 0.6592 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6385(0.6592) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 12m 39s) Loss: 0.4094(0.4094) Grad: inf  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4688(0.4068) Grad: 272204.8438  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3563(0.4081) Grad: 170625.6562  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7298(0.7298) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4081  avg_val_loss: 0.6550  time: 121s\n",
      "Epoch 17 - avg_train_Score: 0.4081 avgScore: 0.6550\n",
      "Epoch 17 - Save Best Score: 0.6550 Model\n",
      "Epoch 17 - Save Best Loss: 0.6550 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6326(0.6550) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 58s) Loss: 0.4882(0.4882) Grad: 457817.7500  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3526(0.3894) Grad: 203123.1719  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3128(0.3895) Grad: 217128.1875  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7441(0.7441) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.3895  avg_val_loss: 0.6520  time: 121s\n",
      "Epoch 18 - avg_train_Score: 0.3895 avgScore: 0.6520\n",
      "Epoch 18 - Save Best Score: 0.6520 Model\n",
      "Epoch 18 - Save Best Loss: 0.6520 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6082(0.6520) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 12m 8s) Loss: 0.3926(0.3926) Grad: 405494.2812  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3477(0.3820) Grad: 176074.0312  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3868(0.3813) Grad: 224162.1875  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7352(0.7352) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.3813  avg_val_loss: 0.6532  time: 121s\n",
      "Epoch 19 - avg_train_Score: 0.3813 avgScore: 0.6532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6277(0.6532) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 12m 1s) Loss: 0.3472(0.3472) Grad: 365597.1250  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3808(0.3686) Grad: 196087.4531  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3323(0.3684) Grad: 181608.0781  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7238(0.7238) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.3684  avg_val_loss: 0.6502  time: 121s\n",
      "Epoch 20 - avg_train_Score: 0.3684 avgScore: 0.6502\n",
      "Epoch 20 - Save Best Score: 0.6502 Model\n",
      "Epoch 20 - Save Best Loss: 0.6502 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6285(0.6502) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 11m 57s) Loss: 0.3423(0.3423) Grad: 411768.0625  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3079(0.3556) Grad: 174903.8594  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3539(0.3550) Grad: 210164.4062  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7130(0.7130) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3550  avg_val_loss: 0.6462  time: 121s\n",
      "Epoch 21 - avg_train_Score: 0.3550 avgScore: 0.6462\n",
      "Epoch 21 - Save Best Score: 0.6462 Model\n",
      "Epoch 21 - Save Best Loss: 0.6462 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6459(0.6462) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 12m 19s) Loss: 0.3353(0.3353) Grad: 413220.7500  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2947(0.3444) Grad: 168225.0469  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3378(0.3441) Grad: 171711.4062  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7170(0.7170) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3441  avg_val_loss: 0.6445  time: 121s\n",
      "Epoch 22 - avg_train_Score: 0.3441 avgScore: 0.6445\n",
      "Epoch 22 - Save Best Score: 0.6445 Model\n",
      "Epoch 22 - Save Best Loss: 0.6445 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6216(0.6445) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 12m 4s) Loss: 0.2910(0.2910) Grad: 316146.5000  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3331(0.3331) Grad: 217932.2812  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3034(0.3327) Grad: 186319.2969  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7182(0.7182) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3327  avg_val_loss: 0.6427  time: 121s\n",
      "Epoch 23 - avg_train_Score: 0.3327 avgScore: 0.6427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6293(0.6427) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Save Best Score: 0.6427 Model\n",
      "Epoch 23 - Save Best Loss: 0.6427 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 0.3290(0.3290) Grad: 292285.5312  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3074(0.3207) Grad: 176870.8281  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3066(0.3205) Grad: 165854.3594  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7310(0.7310) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3205  avg_val_loss: 0.6435  time: 121s\n",
      "Epoch 24 - avg_train_Score: 0.3205 avgScore: 0.6435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6299(0.6435) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 11m 59s) Loss: 0.3189(0.3189) Grad: 333243.8125  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3153(0.3051) Grad: 146580.6562  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3646(0.3057) Grad: 162069.7969  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7328(0.7328) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3057  avg_val_loss: 0.6422  time: 121s\n",
      "Epoch 25 - avg_train_Score: 0.3057 avgScore: 0.6422\n",
      "Epoch 25 - Save Best Score: 0.6422 Model\n",
      "Epoch 25 - Save Best Loss: 0.6422 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6322(0.6422) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 12m 26s) Loss: 0.3312(0.3312) Grad: 274046.2812  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2705(0.3012) Grad: 183429.6250  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2787(0.3014) Grad: 140280.7344  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7142(0.7142) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3014  avg_val_loss: 0.6401  time: 121s\n",
      "Epoch 26 - avg_train_Score: 0.3014 avgScore: 0.6401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6218(0.6401) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Save Best Score: 0.6401 Model\n",
      "Epoch 26 - Save Best Loss: 0.6401 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 12m 19s) Loss: 0.2651(0.2651) Grad: 346012.1562  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2572(0.2911) Grad: 149288.8750  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2659(0.2914) Grad: 150443.7656  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7168(0.7168) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.2914  avg_val_loss: 0.6399  time: 121s\n",
      "Epoch 27 - avg_train_Score: 0.2914 avgScore: 0.6399\n",
      "Epoch 27 - Save Best Score: 0.6399 Model\n",
      "Epoch 27 - Save Best Loss: 0.6399 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6203(0.6399) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 0.2417(0.2417) Grad: 299370.6250  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2228(0.2734) Grad: 278695.3125  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2868(0.2731) Grad: 321333.3750  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7199(0.7199) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.2731  avg_val_loss: 0.6406  time: 121s\n",
      "Epoch 28 - avg_train_Score: 0.2731 avgScore: 0.6406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6159(0.6406) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 12m 14s) Loss: 0.2114(0.2114) Grad: 245316.4062  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2620(0.2690) Grad: 194875.4219  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3030(0.2695) Grad: 152125.1562  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7139(0.7139) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2695  avg_val_loss: 0.6415  time: 121s\n",
      "Epoch 29 - avg_train_Score: 0.2695 avgScore: 0.6415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6197(0.6415) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 12m 30s) Loss: 0.2647(0.2647) Grad: 349369.3438  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2751(0.2644) Grad: 166771.2969  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2387(0.2646) Grad: 121254.4688  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7099(0.7099) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2646  avg_val_loss: 0.6406  time: 121s\n",
      "Epoch 30 - avg_train_Score: 0.2646 avgScore: 0.6406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6309(0.6406) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 12m 6s) Loss: 0.2413(0.2413) Grad: 243230.0312  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2518(0.2548) Grad: 307855.7812  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2929(0.2551) Grad: 321773.0312  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7125(0.7125) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2551  avg_val_loss: 0.6404  time: 121s\n",
      "Epoch 31 - avg_train_Score: 0.2551 avgScore: 0.6404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6312(0.6404) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 12m 49s) Loss: 0.2755(0.2755) Grad: 386651.2812  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2464(0.2523) Grad: 173588.3125  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2587(0.2517) Grad: 143760.1094  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7107(0.7107) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2517  avg_val_loss: 0.6403  time: 121s\n",
      "Epoch 32 - avg_train_Score: 0.2517 avgScore: 0.6403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6354(0.6403) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 12m 36s) Loss: 0.2413(0.2413) Grad: 293167.6875  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2390(0.2434) Grad: 325001.0000  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2271(0.2433) Grad: 254936.3438  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7145(0.7145) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2433  avg_val_loss: 0.6399  time: 121s\n",
      "Epoch 33 - avg_train_Score: 0.2433 avgScore: 0.6399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6372(0.6399) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 12m 19s) Loss: 0.2401(0.2401) Grad: 314833.5625  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2135(0.2428) Grad: 129812.2109  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2135(0.2423) Grad: 133087.4844  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7057(0.7057) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2423  avg_val_loss: 0.6400  time: 121s\n",
      "Epoch 34 - avg_train_Score: 0.2423 avgScore: 0.6400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6382(0.6400) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 0.2530(0.2530) Grad: 312363.8750  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2141(0.2345) Grad: 290968.2188  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2262(0.2347) Grad: 282817.4688  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7015(0.7015) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2347  avg_val_loss: 0.6408  time: 121s\n",
      "Epoch 35 - avg_train_Score: 0.2347 avgScore: 0.6408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6400(0.6408) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 12m 35s) Loss: 0.2291(0.2291) Grad: 266546.3125  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2241(0.2322) Grad: 156563.6094  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2252(0.2326) Grad: 134253.8906  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7079(0.7079) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2326  avg_val_loss: 0.6392  time: 121s\n",
      "Epoch 36 - avg_train_Score: 0.2326 avgScore: 0.6392\n",
      "Epoch 36 - Save Best Score: 0.6392 Model\n",
      "Epoch 36 - Save Best Loss: 0.6392 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6352(0.6392) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 12m 9s) Loss: 0.2502(0.2502) Grad: 319007.2500  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2275(0.2287) Grad: 290729.5938  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2288(0.2288) Grad: 122876.6797  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7123(0.7123) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2288  avg_val_loss: 0.6398  time: 121s\n",
      "Epoch 37 - avg_train_Score: 0.2288 avgScore: 0.6398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6286(0.6398) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 12m 8s) Loss: 0.2090(0.2090) Grad: 256356.2500  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2199(0.2253) Grad: 141498.9062  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2387(0.2252) Grad: 137318.8906  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7100(0.7100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2252  avg_val_loss: 0.6391  time: 121s\n",
      "Epoch 38 - avg_train_Score: 0.2252 avgScore: 0.6391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6285(0.6391) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Save Best Score: 0.6391 Model\n",
      "Epoch 38 - Save Best Loss: 0.6391 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 12m 17s) Loss: 0.1883(0.1883) Grad: 240259.3125  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2278(0.2229) Grad: 282762.5938  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2123(0.2232) Grad: 234241.0156  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7128(0.7128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2232  avg_val_loss: 0.6395  time: 121s\n",
      "Epoch 39 - avg_train_Score: 0.2232 avgScore: 0.6395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6298(0.6395) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 11m 55s) Loss: 0.1941(0.1941) Grad: 278133.4688  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2072(0.2218) Grad: 275032.4375  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2283(0.2223) Grad: 262498.3438  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7113(0.7113) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2223  avg_val_loss: 0.6394  time: 121s\n",
      "Epoch 40 - avg_train_Score: 0.2223 avgScore: 0.6394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6297(0.6394) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 1 result ==========\n",
      "score: 0.6394\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 6.3453(6.3453) Grad: inf  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 1.7823(2.6641) Grad: 305328.7812  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 1.8450(2.6042) Grad: 88524.9766  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 1.2796(1.2796) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.6042  avg_val_loss: 1.5756  time: 121s\n",
      "Epoch 1 - avg_train_Score: 2.6042 avgScore: 1.5756\n",
      "Epoch 1 - Save Best Score: 1.5756 Model\n",
      "Epoch 1 - Save Best Loss: 1.5756 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.5620(1.5756) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 1.8075(1.8075) Grad: inf  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 1.2178(1.6340) Grad: 74538.6797  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 1.6417(1.6221) Grad: 84962.5156  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.8457(0.8457) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.6221  avg_val_loss: 1.1087  time: 121s\n",
      "Epoch 2 - avg_train_Score: 1.6221 avgScore: 1.1087\n",
      "Epoch 2 - Save Best Score: 1.1087 Model\n",
      "Epoch 2 - Save Best Loss: 1.1087 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.0975(1.1087) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 13m 0s) Loss: 1.6079(1.6079) Grad: inf  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 2.3311(2.8634) Grad: 15379.5664  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 1.7613(2.8023) Grad: 9344.7588  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 3.1821(3.1821) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 2.8023  avg_val_loss: 3.1049  time: 121s\n",
      "Epoch 3 - avg_train_Score: 2.8023 avgScore: 3.1049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 2.6926(3.1049) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 12m 15s) Loss: 1.5481(1.5481) Grad: inf  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 1.2366(1.3099) Grad: 93974.6875  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 1.1416(1.2958) Grad: 135053.2812  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.8933(0.8933) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.2958  avg_val_loss: 1.0843  time: 121s\n",
      "Epoch 4 - avg_train_Score: 1.2958 avgScore: 1.0843\n",
      "Epoch 4 - Save Best Score: 1.0843 Model\n",
      "Epoch 4 - Save Best Loss: 1.0843 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.1357(1.0843) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 11m 51s) Loss: 1.0831(1.0831) Grad: inf  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 1.1779(1.0941) Grad: 127885.8516  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.8748(1.0914) Grad: 63309.2891  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8066(0.8066) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.0914  avg_val_loss: 0.9526  time: 121s\n",
      "Epoch 5 - avg_train_Score: 1.0914 avgScore: 0.9526\n",
      "Epoch 5 - Save Best Score: 0.9526 Model\n",
      "Epoch 5 - Save Best Loss: 0.9526 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.0124(0.9526) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 12m 17s) Loss: 1.0100(1.0100) Grad: inf  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.9161(0.9797) Grad: 81616.2500  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.9582(0.9827) Grad: 82629.1094  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7346(0.7346) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.9827  avg_val_loss: 0.8866  time: 121s\n",
      "Epoch 6 - avg_train_Score: 0.9827 avgScore: 0.8866\n",
      "Epoch 6 - Save Best Score: 0.8866 Model\n",
      "Epoch 6 - Save Best Loss: 0.8866 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9201(0.8866) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 12m 55s) Loss: 0.9684(0.9684) Grad: inf  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.8352(0.8719) Grad: 157268.7656  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.8809(0.8717) Grad: 82210.4297  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.6868(0.6868) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8717  avg_val_loss: 0.8366  time: 121s\n",
      "Epoch 7 - avg_train_Score: 0.8717 avgScore: 0.8366\n",
      "Epoch 7 - Save Best Score: 0.8366 Model\n",
      "Epoch 7 - Save Best Loss: 0.8366 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8385(0.8366) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 11m 49s) Loss: 0.9018(0.9018) Grad: inf  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.7599(0.7918) Grad: 221572.5781  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.8382(0.7906) Grad: 157134.0469  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6630(0.6630) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7906  avg_val_loss: 0.8062  time: 121s\n",
      "Epoch 8 - avg_train_Score: 0.7906 avgScore: 0.8062\n",
      "Epoch 8 - Save Best Score: 0.8062 Model\n",
      "Epoch 8 - Save Best Loss: 0.8062 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7976(0.8062) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 0.7159(0.7159) Grad: inf  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.6073(0.7324) Grad: 154715.2188  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.6594(0.7329) Grad: 141981.9062  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6492(0.6492) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.7329  avg_val_loss: 0.7870  time: 121s\n",
      "Epoch 9 - avg_train_Score: 0.7329 avgScore: 0.7870\n",
      "Epoch 9 - Save Best Score: 0.7870 Model\n",
      "Epoch 9 - Save Best Loss: 0.7870 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7668(0.7870) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 48s) Loss: 0.7624(0.7624) Grad: inf  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.7875(0.6859) Grad: 249046.0781  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.6125(0.6870) Grad: 214358.9688  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6487(0.6487) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7763(0.7716) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6870  avg_val_loss: 0.7716  time: 121s\n",
      "Epoch 10 - avg_train_Score: 0.6870 avgScore: 0.7716\n",
      "Epoch 10 - Save Best Score: 0.7716 Model\n",
      "Epoch 10 - Save Best Loss: 0.7716 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 11m 56s) Loss: 0.6215(0.6215) Grad: 795135.4375  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5442(0.6520) Grad: 167537.2969  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.6297(0.6529) Grad: 159258.0000  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.6386(0.6386) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.6529  avg_val_loss: 0.7612  time: 121s\n",
      "Epoch 11 - avg_train_Score: 0.6529 avgScore: 0.7612\n",
      "Epoch 11 - Save Best Score: 0.7612 Model\n",
      "Epoch 11 - Save Best Loss: 0.7612 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7853(0.7612) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 0.6880(0.6880) Grad: inf  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.7264(0.6169) Grad: 175150.3906  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.6382(0.6168) Grad: 173865.9375  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6284(0.6284) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.6168  avg_val_loss: 0.7519  time: 121s\n",
      "Epoch 12 - avg_train_Score: 0.6168 avgScore: 0.7519\n",
      "Epoch 12 - Save Best Score: 0.7519 Model\n",
      "Epoch 12 - Save Best Loss: 0.7519 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7286(0.7519) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 0.5686(0.5686) Grad: inf  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5919(0.6327) Grad: 70079.5547  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.6302(0.6342) Grad: 69240.2812  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6423(0.6423) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.6342  avg_val_loss: 0.7444  time: 121s\n",
      "Epoch 13 - avg_train_Score: 0.6342 avgScore: 0.7444\n",
      "Epoch 13 - Save Best Score: 0.7444 Model\n",
      "Epoch 13 - Save Best Loss: 0.7444 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7301(0.7444) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 0.5981(0.5981) Grad: 499508.8750  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4303(0.5622) Grad: 111552.3672  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.7123(0.5625) Grad: 150790.9375  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6200(0.6200) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5625  avg_val_loss: 0.7316  time: 121s\n",
      "Epoch 14 - avg_train_Score: 0.5625 avgScore: 0.7316\n",
      "Epoch 14 - Save Best Score: 0.7316 Model\n",
      "Epoch 14 - Save Best Loss: 0.7316 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7021(0.7316) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 12m 21s) Loss: 0.5285(0.5285) Grad: 657281.5625  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4880(0.4783) Grad: 245021.6406  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.4203(0.4783) Grad: 259218.0938  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.6231(0.6231) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.4783  avg_val_loss: 0.7251  time: 121s\n",
      "Epoch 15 - avg_train_Score: 0.4783 avgScore: 0.7251\n",
      "Epoch 15 - Save Best Score: 0.7251 Model\n",
      "Epoch 15 - Save Best Loss: 0.7251 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7064(0.7251) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 12m 27s) Loss: 0.4145(0.4145) Grad: 375797.3125  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5223(0.4473) Grad: 282475.2188  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3949(0.4469) Grad: 228172.6094  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6233(0.6233) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4469  avg_val_loss: 0.7242  time: 121s\n",
      "Epoch 16 - avg_train_Score: 0.4469 avgScore: 0.7242\n",
      "Epoch 16 - Save Best Score: 0.7242 Model\n",
      "Epoch 16 - Save Best Loss: 0.7242 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7341(0.7242) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 12m 50s) Loss: 0.3634(0.3634) Grad: inf  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4397(0.4352) Grad: 236827.6562  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.4562(0.4353) Grad: 208853.3906  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.6145(0.6145) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4353  avg_val_loss: 0.7203  time: 121s\n",
      "Epoch 17 - avg_train_Score: 0.4353 avgScore: 0.7203\n",
      "Epoch 17 - Save Best Score: 0.7203 Model\n",
      "Epoch 17 - Save Best Loss: 0.7203 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7135(0.7203) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 12m 27s) Loss: 0.3919(0.3919) Grad: 432533.6250  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4075(0.4212) Grad: 234745.7656  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3609(0.4211) Grad: 320564.9688  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.6095(0.6095) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4211  avg_val_loss: 0.7181  time: 121s\n",
      "Epoch 18 - avg_train_Score: 0.4211 avgScore: 0.7181\n",
      "Epoch 18 - Save Best Score: 0.7181 Model\n",
      "Epoch 18 - Save Best Loss: 0.7181 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7008(0.7181) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.4229(0.4229) Grad: 609780.1250  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.4268(0.4366) Grad: 113115.8438  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3679(0.4377) Grad: 125028.6719  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6122(0.6122) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4377  avg_val_loss: 0.7193  time: 121s\n",
      "Epoch 19 - avg_train_Score: 0.4377 avgScore: 0.7193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6752(0.7193) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 12m 16s) Loss: 0.3999(0.3999) Grad: 392153.7812  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.4248(0.4153) Grad: 230505.9219  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3561(0.4144) Grad: 205988.1719  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6112(0.6112) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.4144  avg_val_loss: 0.7157  time: 121s\n",
      "Epoch 20 - avg_train_Score: 0.4144 avgScore: 0.7157\n",
      "Epoch 20 - Save Best Score: 0.7157 Model\n",
      "Epoch 20 - Save Best Loss: 0.7157 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7008(0.7157) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 12m 29s) Loss: 0.4397(0.4397) Grad: 537009.3750  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3254(0.3811) Grad: 193483.4062  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3587(0.3815) Grad: 198990.7812  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6151(0.6151) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3815  avg_val_loss: 0.7128  time: 121s\n",
      "Epoch 21 - avg_train_Score: 0.3815 avgScore: 0.7128\n",
      "Epoch 21 - Save Best Score: 0.7128 Model\n",
      "Epoch 21 - Save Best Loss: 0.7128 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7133(0.7128) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 12m 42s) Loss: 0.3398(0.3398) Grad: 389101.1875  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3227(0.3657) Grad: 177950.2344  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3504(0.3656) Grad: 207273.4062  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6193(0.6193) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3656  avg_val_loss: 0.7091  time: 121s\n",
      "Epoch 22 - avg_train_Score: 0.3656 avgScore: 0.7091\n",
      "Epoch 22 - Save Best Score: 0.7091 Model\n",
      "Epoch 22 - Save Best Loss: 0.7091 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7048(0.7091) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 11m 51s) Loss: 0.4160(0.4160) Grad: inf  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3280(0.3568) Grad: 185328.6406  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3023(0.3570) Grad: 233640.2656  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.6160(0.6160) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3570  avg_val_loss: 0.7085  time: 121s\n",
      "Epoch 23 - avg_train_Score: 0.3570 avgScore: 0.7085\n",
      "Epoch 23 - Save Best Score: 0.7085 Model\n",
      "Epoch 23 - Save Best Loss: 0.7085 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6910(0.7085) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 14m 49s) Loss: 0.4107(0.4107) Grad: 391323.3750  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3626(0.3400) Grad: 186663.1250  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3447(0.3406) Grad: 170738.9688  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.6107(0.6107) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3406  avg_val_loss: 0.7093  time: 121s\n",
      "Epoch 24 - avg_train_Score: 0.3406 avgScore: 0.7093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7071(0.7093) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 12m 8s) Loss: 0.3514(0.3514) Grad: 467748.4375  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3766(0.3326) Grad: 159943.7344  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3228(0.3326) Grad: 151376.9688  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6135(0.6135) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7098(0.7069) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3326  avg_val_loss: 0.7069  time: 121s\n",
      "Epoch 25 - avg_train_Score: 0.3326 avgScore: 0.7069\n",
      "Epoch 25 - Save Best Score: 0.7069 Model\n",
      "Epoch 25 - Save Best Loss: 0.7069 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 11m 58s) Loss: 0.3606(0.3606) Grad: 423706.1875  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3305(0.3256) Grad: 233162.0625  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3036(0.3260) Grad: 150085.7656  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.6168(0.6168) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3260  avg_val_loss: 0.7073  time: 121s\n",
      "Epoch 26 - avg_train_Score: 0.3260 avgScore: 0.7073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7051(0.7073) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 11m 47s) Loss: 0.3180(0.3180) Grad: 464563.7812  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2862(0.3121) Grad: 158584.7500  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2791(0.3120) Grad: 167830.0312  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6098(0.6098) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.3120  avg_val_loss: 0.7053  time: 121s\n",
      "Epoch 27 - avg_train_Score: 0.3120 avgScore: 0.7053\n",
      "Epoch 27 - Save Best Score: 0.7053 Model\n",
      "Epoch 27 - Save Best Loss: 0.7053 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6982(0.7053) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 12m 3s) Loss: 0.3135(0.3135) Grad: 378071.4688  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2683(0.3031) Grad: 188960.2969  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3029(0.3028) Grad: 184028.0156  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.6138(0.6138) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.3028  avg_val_loss: 0.7055  time: 121s\n",
      "Epoch 28 - avg_train_Score: 0.3028 avgScore: 0.7055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6911(0.7055) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 12m 1s) Loss: 0.2988(0.2988) Grad: 400521.6250  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3012(0.2900) Grad: 160193.6719  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3211(0.2904) Grad: 159784.5156  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.6104(0.6104) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2904  avg_val_loss: 0.7036  time: 121s\n",
      "Epoch 29 - avg_train_Score: 0.2904 avgScore: 0.7036\n",
      "Epoch 29 - Save Best Score: 0.7036 Model\n",
      "Epoch 29 - Save Best Loss: 0.7036 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6978(0.7036) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 11m 47s) Loss: 0.3019(0.3019) Grad: 365375.5312  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2712(0.2832) Grad: 167929.0469  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2872(0.2830) Grad: 175741.7031  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6084(0.6084) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6974(0.7044) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2830  avg_val_loss: 0.7044  time: 121s\n",
      "Epoch 30 - avg_train_Score: 0.2830 avgScore: 0.7044\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 14m 1s) Loss: 0.2905(0.2905) Grad: 384077.6562  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2147(0.2754) Grad: 175792.1562  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2568(0.2756) Grad: 132889.1250  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6068(0.6068) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2756  avg_val_loss: 0.7028  time: 121s\n",
      "Epoch 31 - avg_train_Score: 0.2756 avgScore: 0.7028\n",
      "Epoch 31 - Save Best Score: 0.7028 Model\n",
      "Epoch 31 - Save Best Loss: 0.7028 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6969(0.7028) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 13m 0s) Loss: 0.2753(0.2753) Grad: 331161.8750  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2579(0.2694) Grad: 211552.6406  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2215(0.2689) Grad: 140688.3438  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.6091(0.6091) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7057(0.7025) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2689  avg_val_loss: 0.7025  time: 121s\n",
      "Epoch 32 - avg_train_Score: 0.2689 avgScore: 0.7025\n",
      "Epoch 32 - Save Best Score: 0.7025 Model\n",
      "Epoch 32 - Save Best Loss: 0.7025 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 0.2621(0.2621) Grad: 276307.4375  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2361(0.2579) Grad: 147678.7500  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2535(0.2578) Grad: 162729.0781  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6056(0.6056) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2578  avg_val_loss: 0.7021  time: 121s\n",
      "Epoch 33 - avg_train_Score: 0.2578 avgScore: 0.7021\n",
      "Epoch 33 - Save Best Score: 0.7021 Model\n",
      "Epoch 33 - Save Best Loss: 0.7021 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6902(0.7021) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 0.2481(0.2481) Grad: 372653.1875  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2086(0.2563) Grad: 161362.0312  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2838(0.2561) Grad: 147420.8438  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6076(0.6076) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2561  avg_val_loss: 0.7019  time: 121s\n",
      "Epoch 34 - avg_train_Score: 0.2561 avgScore: 0.7019\n",
      "Epoch 34 - Save Best Score: 0.7019 Model\n",
      "Epoch 34 - Save Best Loss: 0.7019 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6901(0.7019) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 13m 26s) Loss: 0.2745(0.2745) Grad: 328455.7500  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2510(0.2507) Grad: 167907.9219  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2630(0.2506) Grad: 216771.4844  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6112(0.6112) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2506  avg_val_loss: 0.7027  time: 121s\n",
      "Epoch 35 - avg_train_Score: 0.2506 avgScore: 0.7027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6904(0.7027) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.2322(0.2322) Grad: 329824.9375  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2769(0.2442) Grad: 201726.0000  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2354(0.2444) Grad: 150475.1406  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6053(0.6053) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2444  avg_val_loss: 0.7028  time: 121s\n",
      "Epoch 36 - avg_train_Score: 0.2444 avgScore: 0.7028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6942(0.7028) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 13m 3s) Loss: 0.3033(0.3033) Grad: 322793.5312  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2348(0.2414) Grad: 154732.6875  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2422(0.2411) Grad: 174664.7812  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.6040(0.6040) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2411  avg_val_loss: 0.7007  time: 122s\n",
      "Epoch 37 - avg_train_Score: 0.2411 avgScore: 0.7007\n",
      "Epoch 37 - Save Best Score: 0.7007 Model\n",
      "Epoch 37 - Save Best Loss: 0.7007 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6918(0.7007) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 0.2381(0.2381) Grad: 281448.1562  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2661(0.2372) Grad: 154352.5312  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2144(0.2378) Grad: 145338.0312  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6038(0.6038) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2378  avg_val_loss: 0.7000  time: 121s\n",
      "Epoch 38 - avg_train_Score: 0.2378 avgScore: 0.7000\n",
      "Epoch 38 - Save Best Score: 0.7000 Model\n",
      "Epoch 38 - Save Best Loss: 0.7000 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6923(0.7000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 11m 52s) Loss: 0.2120(0.2120) Grad: 286909.8125  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2138(0.2355) Grad: 252627.4531  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2774(0.2356) Grad: 340083.6562  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6038(0.6038) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2356  avg_val_loss: 0.7003  time: 121s\n",
      "Epoch 39 - avg_train_Score: 0.2356 avgScore: 0.7003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6931(0.7003) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 12m 13s) Loss: 0.2693(0.2693) Grad: 342364.5625  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2423(0.2337) Grad: 179886.9531  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2024(0.2338) Grad: 137224.8750  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6046(0.6046) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2338  avg_val_loss: 0.7003  time: 121s\n",
      "Epoch 40 - avg_train_Score: 0.2338 avgScore: 0.7003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6920(0.7003) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 2 result ==========\n",
      "score: 0.7003\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 11m 57s) Loss: 7.3748(7.3748) Grad: inf  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 1.7316(2.5565) Grad: 121213.6797  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 1.5344(2.4946) Grad: 112743.2812  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 1.4838(1.4838) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.4946  avg_val_loss: 1.5853  time: 121s\n",
      "Epoch 1 - avg_train_Score: 2.4946 avgScore: 1.5853\n",
      "Epoch 1 - Save Best Score: 1.5853 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.4306(1.5853) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Save Best Loss: 1.5853 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 14m 9s) Loss: 1.6606(1.6606) Grad: inf  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 1.4589(1.5872) Grad: 177380.7188  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 1.4404(1.5802) Grad: 199028.2344  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 1.0493(1.0493) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.5802  avg_val_loss: 1.2027  time: 121s\n",
      "Epoch 2 - avg_train_Score: 1.5802 avgScore: 1.2027\n",
      "Epoch 2 - Save Best Score: 1.2027 Model\n",
      "Epoch 2 - Save Best Loss: 1.2027 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.1633(1.2027) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 12m 40s) Loss: 1.5338(1.5338) Grad: inf  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 1.1893(1.4673) Grad: 42387.9219  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 1.1406(1.4492) Grad: 86415.8125  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.9209(0.9209) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.4492  avg_val_loss: 1.0324  time: 121s\n",
      "Epoch 3 - avg_train_Score: 1.4492 avgScore: 1.0324\n",
      "Epoch 3 - Save Best Score: 1.0324 Model\n",
      "Epoch 3 - Save Best Loss: 1.0324 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.0675(1.0324) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 12m 7s) Loss: 1.2326(1.2326) Grad: inf  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.9748(1.0151) Grad: 159656.0781  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 1.0491(1.0095) Grad: 179420.5781  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.8745(0.8745) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0095  avg_val_loss: 0.9041  time: 121s\n",
      "Epoch 4 - avg_train_Score: 1.0095 avgScore: 0.9041\n",
      "Epoch 4 - Save Best Score: 0.9041 Model\n",
      "Epoch 4 - Save Best Loss: 0.9041 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9887(0.9041) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 11m 55s) Loss: 0.8640(0.8640) Grad: inf  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.8137(0.8758) Grad: 244502.4219  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.7378(0.8747) Grad: 181925.2969  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.8573(0.8573) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.8747  avg_val_loss: 0.8541  time: 121s\n",
      "Epoch 5 - avg_train_Score: 0.8747 avgScore: 0.8541\n",
      "Epoch 5 - Save Best Score: 0.8541 Model\n",
      "Epoch 5 - Save Best Loss: 0.8541 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9338(0.8541) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 12m 15s) Loss: 0.9023(0.9023) Grad: inf  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.9053(0.7938) Grad: 356007.0312  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.6415(0.7942) Grad: 177629.5938  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.8027(0.8027) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.7942  avg_val_loss: 0.8223  time: 121s\n",
      "Epoch 6 - avg_train_Score: 0.7942 avgScore: 0.8223\n",
      "Epoch 6 - Save Best Score: 0.8223 Model\n",
      "Epoch 6 - Save Best Loss: 0.8223 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9028(0.8223) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 15m 11s) Loss: 0.7660(0.7660) Grad: inf  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.7409(0.7325) Grad: 186627.0312  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.7628(0.7316) Grad: 138584.7031  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7804(0.7804) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7316  avg_val_loss: 0.7950  time: 121s\n",
      "Epoch 7 - avg_train_Score: 0.7316 avgScore: 0.7950\n",
      "Epoch 7 - Save Best Score: 0.7950 Model\n",
      "Epoch 7 - Save Best Loss: 0.7950 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8835(0.7950) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 12m 46s) Loss: 0.6193(0.6193) Grad: inf  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.7277(0.6870) Grad: 250131.5312  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.6246(0.6854) Grad: 167114.7656  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7934(0.7934) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.6854  avg_val_loss: 0.7769  time: 121s\n",
      "Epoch 8 - avg_train_Score: 0.6854 avgScore: 0.7769\n",
      "Epoch 8 - Save Best Score: 0.7769 Model\n",
      "Epoch 8 - Save Best Loss: 0.7769 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8591(0.7769) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 57s) Loss: 0.7065(0.7065) Grad: inf  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.6413(0.6376) Grad: 167497.4375  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.5634(0.6362) Grad: 127688.8203  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7726(0.7726) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6362  avg_val_loss: 0.7637  time: 121s\n",
      "Epoch 9 - avg_train_Score: 0.6362 avgScore: 0.7637\n",
      "Epoch 9 - Save Best Score: 0.7637 Model\n",
      "Epoch 9 - Save Best Loss: 0.7637 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8564(0.7637) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 54s) Loss: 0.5136(0.5136) Grad: inf  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5765(0.5439) Grad: 304077.0938  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.5307(0.5442) Grad: 309662.5625  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7450(0.7450) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5442  avg_val_loss: 0.7458  time: 121s\n",
      "Epoch 10 - avg_train_Score: 0.5442 avgScore: 0.7458\n",
      "Epoch 10 - Save Best Score: 0.7458 Model\n",
      "Epoch 10 - Save Best Loss: 0.7458 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8344(0.7458) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 12m 26s) Loss: 0.6129(0.6129) Grad: inf  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.5902(0.5097) Grad: 155392.7812  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.6840(0.5162) Grad: 200577.5938  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7366(0.7366) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5162  avg_val_loss: 0.7403  time: 121s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8475(0.7403) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 - avg_train_Score: 0.5162 avgScore: 0.7403\n",
      "Epoch 11 - Save Best Score: 0.7403 Model\n",
      "Epoch 11 - Save Best Loss: 0.7403 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 14m 49s) Loss: 0.4484(0.4484) Grad: 522248.6562  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.5094(0.4990) Grad: 300816.1562  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.5537(0.4993) Grad: 459226.4688  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7317(0.7317) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4993  avg_val_loss: 0.7385  time: 122s\n",
      "Epoch 12 - avg_train_Score: 0.4993 avgScore: 0.7385\n",
      "Epoch 12 - Save Best Score: 0.7385 Model\n",
      "Epoch 12 - Save Best Loss: 0.7385 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8346(0.7385) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 12m 33s) Loss: 0.4595(0.4595) Grad: 609296.2500  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5249(0.4704) Grad: 181343.6406  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.6089(0.4733) Grad: 249841.7344  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7174(0.7174) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4733  avg_val_loss: 0.7309  time: 121s\n",
      "Epoch 13 - avg_train_Score: 0.4733 avgScore: 0.7309\n",
      "Epoch 13 - Save Best Score: 0.7309 Model\n",
      "Epoch 13 - Save Best Loss: 0.7309 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8482(0.7309) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 12m 14s) Loss: 0.5238(0.5238) Grad: 849259.2500  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4263(0.4645) Grad: 273987.6562  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.4471(0.4638) Grad: 256538.8906  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.6912(0.6912) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4638  avg_val_loss: 0.7219  time: 121s\n",
      "Epoch 14 - avg_train_Score: 0.4638 avgScore: 0.7219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8595(0.7219) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Save Best Score: 0.7219 Model\n",
      "Epoch 14 - Save Best Loss: 0.7219 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 12m 17s) Loss: 0.5007(0.5007) Grad: inf  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3630(0.4417) Grad: 242695.3281  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.4413(0.4407) Grad: 217160.7656  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.6889(0.6889) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.4407  avg_val_loss: 0.7190  time: 121s\n",
      "Epoch 15 - avg_train_Score: 0.4407 avgScore: 0.7190\n",
      "Epoch 15 - Save Best Score: 0.7190 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8367(0.7190) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Save Best Loss: 0.7190 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 12m 31s) Loss: 0.4426(0.4426) Grad: 451187.2812  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4969(0.4290) Grad: 239552.0938  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.4397(0.4287) Grad: 216160.5938  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7054(0.7054) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8442(0.7158) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4287  avg_val_loss: 0.7158  time: 121s\n",
      "Epoch 16 - avg_train_Score: 0.4287 avgScore: 0.7158\n",
      "Epoch 16 - Save Best Score: 0.7158 Model\n",
      "Epoch 16 - Save Best Loss: 0.7158 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 12m 18s) Loss: 0.4081(0.4081) Grad: 498153.9375  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3754(0.4145) Grad: 227594.0625  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.4544(0.4135) Grad: 229073.6875  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7157(0.7157) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4135  avg_val_loss: 0.7143  time: 121s\n",
      "Epoch 17 - avg_train_Score: 0.4135 avgScore: 0.7143\n",
      "Epoch 17 - Save Best Score: 0.7143 Model\n",
      "Epoch 17 - Save Best Loss: 0.7143 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8207(0.7143) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 56s) Loss: 0.3783(0.3783) Grad: 387066.3750  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3609(0.3975) Grad: 181398.6875  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.5007(0.3974) Grad: 229451.6562  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7213(0.7213) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.3974  avg_val_loss: 0.7106  time: 121s\n",
      "Epoch 18 - avg_train_Score: 0.3974 avgScore: 0.7106\n",
      "Epoch 18 - Save Best Score: 0.7106 Model\n",
      "Epoch 18 - Save Best Loss: 0.7106 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8250(0.7106) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 12m 17s) Loss: 0.3492(0.3492) Grad: 332007.7812  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3679(0.3849) Grad: 264747.0625  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3346(0.3840) Grad: 178855.5000  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.6958(0.6958) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.3840  avg_val_loss: 0.7062  time: 121s\n",
      "Epoch 19 - avg_train_Score: 0.3840 avgScore: 0.7062\n",
      "Epoch 19 - Save Best Score: 0.7062 Model\n",
      "Epoch 19 - Save Best Loss: 0.7062 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8250(0.7062) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 56s) Loss: 0.3229(0.3229) Grad: 402584.9688  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3281(0.3620) Grad: 142211.5781  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.4315(0.3618) Grad: 191542.1094  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.6908(0.6908) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.3618  avg_val_loss: 0.7026  time: 121s\n",
      "Epoch 20 - avg_train_Score: 0.3618 avgScore: 0.7026\n",
      "Epoch 20 - Save Best Score: 0.7026 Model\n",
      "Epoch 20 - Save Best Loss: 0.7026 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8154(0.7026) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 12m 4s) Loss: 0.3704(0.3704) Grad: 381448.0000  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.4074(0.3537) Grad: 186309.7500  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3278(0.3536) Grad: 183421.0312  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6882(0.6882) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3536  avg_val_loss: 0.7004  time: 121s\n",
      "Epoch 21 - avg_train_Score: 0.3536 avgScore: 0.7004\n",
      "Epoch 21 - Save Best Score: 0.7004 Model\n",
      "Epoch 21 - Save Best Loss: 0.7004 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8070(0.7004) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 12m 16s) Loss: 0.3193(0.3193) Grad: 353577.7188  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2835(0.3424) Grad: 159376.6094  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3351(0.3429) Grad: 193445.7188  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6908(0.6908) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8183(0.6993) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3429  avg_val_loss: 0.6993  time: 121s\n",
      "Epoch 22 - avg_train_Score: 0.3429 avgScore: 0.6993\n",
      "Epoch 22 - Save Best Score: 0.6993 Model\n",
      "Epoch 22 - Save Best Loss: 0.6993 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 12m 50s) Loss: 0.3443(0.3443) Grad: 385298.4688  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2948(0.3162) Grad: 358200.2812  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.3140(0.3160) Grad: 267994.3125  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.6779(0.6779) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3160  avg_val_loss: 0.6964  time: 121s\n",
      "Epoch 23 - avg_train_Score: 0.3160 avgScore: 0.6964\n",
      "Epoch 23 - Save Best Score: 0.6964 Model\n",
      "Epoch 23 - Save Best Loss: 0.6964 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7924(0.6964) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 12m 31s) Loss: 0.2901(0.2901) Grad: 276319.6562  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3245(0.3117) Grad: 200553.7188  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2869(0.3121) Grad: 256796.9219  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.6614(0.6614) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3121  avg_val_loss: 0.6959  time: 121s\n",
      "Epoch 24 - avg_train_Score: 0.3121 avgScore: 0.6959\n",
      "Epoch 24 - Save Best Score: 0.6959 Model\n",
      "Epoch 24 - Save Best Loss: 0.6959 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8267(0.6959) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 12m 18s) Loss: 0.3218(0.3218) Grad: 358799.7188  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3077(0.3010) Grad: 332940.3750  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3149(0.3005) Grad: 320748.8750  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6809(0.6809) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3005  avg_val_loss: 0.6944  time: 121s\n",
      "Epoch 25 - avg_train_Score: 0.3005 avgScore: 0.6944\n",
      "Epoch 25 - Save Best Score: 0.6944 Model\n",
      "Epoch 25 - Save Best Loss: 0.6944 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8236(0.6944) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 12m 25s) Loss: 0.2784(0.2784) Grad: 291518.5312  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2987(0.2877) Grad: 134002.0312  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3033(0.2884) Grad: 171215.6250  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6758(0.6758) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.2884  avg_val_loss: 0.6937  time: 121s\n",
      "Epoch 26 - avg_train_Score: 0.2884 avgScore: 0.6937\n",
      "Epoch 26 - Save Best Score: 0.6937 Model\n",
      "Epoch 26 - Save Best Loss: 0.6937 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8153(0.6937) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 0.3232(0.3232) Grad: 364009.9688  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2920(0.2843) Grad: 341941.3438  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2747(0.2844) Grad: 326124.4375  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6799(0.6799) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.2844  avg_val_loss: 0.6929  time: 121s\n",
      "Epoch 27 - avg_train_Score: 0.2844 avgScore: 0.6929\n",
      "Epoch 27 - Save Best Score: 0.6929 Model\n",
      "Epoch 27 - Save Best Loss: 0.6929 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8143(0.6929) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 12m 13s) Loss: 0.2462(0.2462) Grad: 396136.3750  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2782(0.2753) Grad: 140318.8125  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2866(0.2762) Grad: 231073.0312  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.6775(0.6775) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.2762  avg_val_loss: 0.6922  time: 121s\n",
      "Epoch 28 - avg_train_Score: 0.2762 avgScore: 0.6922\n",
      "Epoch 28 - Save Best Score: 0.6922 Model\n",
      "Epoch 28 - Save Best Loss: 0.6922 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8013(0.6922) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 12m 50s) Loss: 0.3001(0.3001) Grad: 386957.6875  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2980(0.2722) Grad: 284136.2188  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2815(0.2720) Grad: 335800.3125  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6788(0.6788) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2720  avg_val_loss: 0.6948  time: 121s\n",
      "Epoch 29 - avg_train_Score: 0.2720 avgScore: 0.6948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8111(0.6948) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.2339(0.2339) Grad: 316914.6250  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2593(0.2588) Grad: 327359.7500  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2600(0.2581) Grad: 291663.9062  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6721(0.6721) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2581  avg_val_loss: 0.6936  time: 121s\n",
      "Epoch 30 - avg_train_Score: 0.2581 avgScore: 0.6936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8021(0.6936) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 0.2256(0.2256) Grad: 262089.1250  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2332(0.2531) Grad: 310793.5312  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2662(0.2532) Grad: 325740.5312  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6795(0.6795) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2532  avg_val_loss: 0.6931  time: 121s\n",
      "Epoch 31 - avg_train_Score: 0.2532 avgScore: 0.6931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7987(0.6931) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 12m 21s) Loss: 0.2401(0.2401) Grad: 290433.4688  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2410(0.2471) Grad: 420581.5000  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2374(0.2479) Grad: 381383.4688  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.6793(0.6793) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2479  avg_val_loss: 0.6926  time: 121s\n",
      "Epoch 32 - avg_train_Score: 0.2479 avgScore: 0.6926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7933(0.6926) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 14m 41s) Loss: 0.2474(0.2474) Grad: 319678.0625  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2079(0.2416) Grad: 276915.0625  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2564(0.2415) Grad: 401344.1250  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 0.6758(0.6758) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2415  avg_val_loss: 0.6925  time: 122s\n",
      "Epoch 33 - avg_train_Score: 0.2415 avgScore: 0.6925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7960(0.6925) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 15m 59s) Loss: 0.2347(0.2347) Grad: 267153.2188  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2220(0.2377) Grad: 296371.0938  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2875(0.2380) Grad: 280770.9375  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.6775(0.6775) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2380  avg_val_loss: 0.6923  time: 122s\n",
      "Epoch 34 - avg_train_Score: 0.2380 avgScore: 0.6923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7971(0.6923) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 0.2558(0.2558) Grad: 404119.2500  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2746(0.2322) Grad: 260724.7969  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2109(0.2325) Grad: 276148.9688  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6692(0.6692) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2325  avg_val_loss: 0.6934  time: 121s\n",
      "Epoch 35 - avg_train_Score: 0.2325 avgScore: 0.6934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7929(0.6934) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 12m 1s) Loss: 0.2041(0.2041) Grad: 309769.7188  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2200(0.2292) Grad: 342746.9688  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2477(0.2291) Grad: 243860.8750  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6707(0.6707) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2291  avg_val_loss: 0.6917  time: 121s\n",
      "Epoch 36 - avg_train_Score: 0.2291 avgScore: 0.6917\n",
      "Epoch 36 - Save Best Score: 0.6917 Model\n",
      "Epoch 36 - Save Best Loss: 0.6917 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8001(0.6917) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 11m 50s) Loss: 0.2163(0.2163) Grad: 233345.7969  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2224(0.2269) Grad: 315834.4688  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2373(0.2273) Grad: 218421.7969  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.6768(0.6768) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2273  avg_val_loss: 0.6922  time: 121s\n",
      "Epoch 37 - avg_train_Score: 0.2273 avgScore: 0.6922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7949(0.6922) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 0.2181(0.2181) Grad: 275693.9062  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2175(0.2236) Grad: 292226.5938  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2522(0.2241) Grad: 298519.5938  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.6754(0.6754) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2241  avg_val_loss: 0.6926  time: 121s\n",
      "Epoch 38 - avg_train_Score: 0.2241 avgScore: 0.6926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7958(0.6926) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 12m 50s) Loss: 0.2295(0.2295) Grad: 241945.5312  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2031(0.2214) Grad: 249248.4531  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2008(0.2218) Grad: 249091.7188  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6751(0.6751) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2218  avg_val_loss: 0.6922  time: 121s\n",
      "Epoch 39 - avg_train_Score: 0.2218 avgScore: 0.6922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7959(0.6922) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 12m 25s) Loss: 0.2555(0.2555) Grad: 300589.9375  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.1989(0.2215) Grad: 253750.8125  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2255(0.2209) Grad: 330701.7500  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.6736(0.6736) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2209  avg_val_loss: 0.6920  time: 121s\n",
      "Epoch 40 - avg_train_Score: 0.2209 avgScore: 0.6920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7951(0.6920) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 3 result ==========\n",
      "score: 0.6920\n",
      "========== fold: 4 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 12m 23s) Loss: 6.8964(6.8964) Grad: inf  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 2.3804(2.6153) Grad: 226541.8438  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 1.7695(2.5570) Grad: 133074.0156  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 1.4267(1.4267) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.5570  avg_val_loss: 1.5490  time: 121s\n",
      "Epoch 1 - avg_train_Score: 2.5570 avgScore: 1.5490\n",
      "Epoch 1 - Save Best Score: 1.5490 Model\n",
      "Epoch 1 - Save Best Loss: 1.5490 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.8292(1.5490) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 11m 59s) Loss: 1.7072(1.7072) Grad: inf  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 1.5572(1.6582) Grad: 96343.6016  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 1.5035(1.6464) Grad: 77115.6250  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 1.0903(1.0903) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.6464  avg_val_loss: 1.1478  time: 121s\n",
      "Epoch 2 - avg_train_Score: 1.6464 avgScore: 1.1478\n",
      "Epoch 2 - Save Best Score: 1.1478 Model\n",
      "Epoch 2 - Save Best Loss: 1.1478 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.4585(1.1478) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 1.4264(1.4264) Grad: inf  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 1.4104(1.4321) Grad: 23730.8516  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 1.2534(1.4119) Grad: 42143.0703  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.9982(0.9982) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.4119  avg_val_loss: 0.9848  time: 121s\n",
      "Epoch 3 - avg_train_Score: 1.4119 avgScore: 0.9848\n",
      "Epoch 3 - Save Best Score: 0.9848 Model\n",
      "Epoch 3 - Save Best Loss: 0.9848 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.1776(0.9848) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 1.2998(1.2998) Grad: inf  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.8955(0.9629) Grad: 105106.0078  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.6794(0.9559) Grad: 89086.7109  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.8212(0.8212) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 0.9559  avg_val_loss: 0.8586  time: 121s\n",
      "Epoch 4 - avg_train_Score: 0.9559 avgScore: 0.8586\n",
      "Epoch 4 - Save Best Score: 0.8586 Model\n",
      "Epoch 4 - Save Best Loss: 0.8586 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9448(0.8586) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 12m 8s) Loss: 0.8565(0.8565) Grad: inf  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.8825(0.7981) Grad: 211457.4375  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.8312(0.7967) Grad: 165329.6406  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7609(0.7609) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.7967  avg_val_loss: 0.8115  time: 121s\n",
      "Epoch 5 - avg_train_Score: 0.7967 avgScore: 0.8115\n",
      "Epoch 5 - Save Best Score: 0.8115 Model\n",
      "Epoch 5 - Save Best Loss: 0.8115 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8613(0.8115) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 12m 40s) Loss: 0.7251(0.7251) Grad: inf  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.7863(0.7273) Grad: 193274.6875  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.6876(0.7285) Grad: 135190.3438  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7121(0.7121) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.7285  avg_val_loss: 0.7853  time: 121s\n",
      "Epoch 6 - avg_train_Score: 0.7285 avgScore: 0.7853\n",
      "Epoch 6 - Save Best Score: 0.7853 Model\n",
      "Epoch 6 - Save Best Loss: 0.7853 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8450(0.7853) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.7280(0.7280) Grad: 776693.5000  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.7253(0.6622) Grad: 172721.8594  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.7775(0.6631) Grad: 226409.1875  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7027(0.7027) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.6631  avg_val_loss: 0.7663  time: 121s\n",
      "Epoch 7 - avg_train_Score: 0.6631 avgScore: 0.7663\n",
      "Epoch 7 - Save Best Score: 0.7663 Model\n",
      "Epoch 7 - Save Best Loss: 0.7663 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8240(0.7663) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 11m 53s) Loss: 0.5826(0.5826) Grad: inf  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.6187(0.6479) Grad: 148450.9688  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.6262(0.6479) Grad: 196592.8125  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6994(0.6994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.6479  avg_val_loss: 0.7492  time: 121s\n",
      "Epoch 8 - avg_train_Score: 0.6479 avgScore: 0.7492\n",
      "Epoch 8 - Save Best Score: 0.7492 Model\n",
      "Epoch 8 - Save Best Loss: 0.7492 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8043(0.7492) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 47s) Loss: 0.6141(0.6141) Grad: inf  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.7048(0.6122) Grad: 194762.4688  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.5358(0.6122) Grad: 132621.7344  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6846(0.6846) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6122  avg_val_loss: 0.7307  time: 121s\n",
      "Epoch 9 - avg_train_Score: 0.6122 avgScore: 0.7307\n",
      "Epoch 9 - Save Best Score: 0.7307 Model\n",
      "Epoch 9 - Save Best Loss: 0.7307 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7746(0.7307) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 39s) Loss: 0.5663(0.5663) Grad: inf  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.5542(0.5814) Grad: 166323.4375  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.5730(0.5825) Grad: 172145.8438  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6799(0.6799) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5825  avg_val_loss: 0.7149  time: 121s\n",
      "Epoch 10 - avg_train_Score: 0.5825 avgScore: 0.7149\n",
      "Epoch 10 - Save Best Score: 0.7149 Model\n",
      "Epoch 10 - Save Best Loss: 0.7149 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7675(0.7149) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 0.5231(0.5231) Grad: inf  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.4846(0.5516) Grad: 142491.4531  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.5340(0.5518) Grad: 172377.4688  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6795(0.6795) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5518  avg_val_loss: 0.7048  time: 121s\n",
      "Epoch 11 - avg_train_Score: 0.5518 avgScore: 0.7048\n",
      "Epoch 11 - Save Best Score: 0.7048 Model\n",
      "Epoch 11 - Save Best Loss: 0.7048 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7546(0.7048) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 12m 28s) Loss: 0.5247(0.5247) Grad: 484257.1562  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.5925(0.4822) Grad: 172512.3125  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.5275(0.4861) Grad: 165201.6250  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.6773(0.6773) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4861  avg_val_loss: 0.6972  time: 121s\n",
      "Epoch 12 - avg_train_Score: 0.4861 avgScore: 0.6972\n",
      "Epoch 12 - Save Best Score: 0.6972 Model\n",
      "Epoch 12 - Save Best Loss: 0.6972 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7620(0.6972) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 11m 53s) Loss: 0.5683(0.5683) Grad: 631084.0000  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4334(0.4595) Grad: 187941.5625  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.4325(0.4591) Grad: 330298.2500  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.6823(0.6823) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4591  avg_val_loss: 0.6893  time: 121s\n",
      "Epoch 13 - avg_train_Score: 0.4591 avgScore: 0.6893\n",
      "Epoch 13 - Save Best Score: 0.6893 Model\n",
      "Epoch 13 - Save Best Loss: 0.6893 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7707(0.6893) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 57s) Loss: 0.4157(0.4157) Grad: inf  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4313(0.4335) Grad: 254373.5781  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.4095(0.4332) Grad: 306452.1562  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.6818(0.6818) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4332  avg_val_loss: 0.6861  time: 121s\n",
      "Epoch 14 - avg_train_Score: 0.4332 avgScore: 0.6861\n",
      "Epoch 14 - Save Best Score: 0.6861 Model\n",
      "Epoch 14 - Save Best Loss: 0.6861 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7512(0.6861) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 12m 18s) Loss: 0.4499(0.4499) Grad: 446732.0938  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.4125(0.4224) Grad: 212851.3906  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.4488(0.4231) Grad: 177514.5938  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.6685(0.6685) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.4231  avg_val_loss: 0.6844  time: 121s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7504(0.6844) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 - avg_train_Score: 0.4231 avgScore: 0.6844\n",
      "Epoch 15 - Save Best Score: 0.6844 Model\n",
      "Epoch 15 - Save Best Loss: 0.6844 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 13m 0s) Loss: 0.3762(0.3762) Grad: 416740.5625  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.5298(0.4136) Grad: 280325.3750  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3871(0.4133) Grad: 237211.6875  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.6529(0.6529) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4133  avg_val_loss: 0.6790  time: 122s\n",
      "Epoch 16 - avg_train_Score: 0.4133 avgScore: 0.6790\n",
      "Epoch 16 - Save Best Score: 0.6790 Model\n",
      "Epoch 16 - Save Best Loss: 0.6790 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7618(0.6790) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 12m 26s) Loss: 0.3992(0.3992) Grad: 518888.4062  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3452(0.4061) Grad: 225934.9531  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.4218(0.4062) Grad: 189831.4531  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6338(0.6338) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4062  avg_val_loss: 0.6737  time: 121s\n",
      "Epoch 17 - avg_train_Score: 0.4062 avgScore: 0.6737\n",
      "Epoch 17 - Save Best Score: 0.6737 Model\n",
      "Epoch 17 - Save Best Loss: 0.6737 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7272(0.6737) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 12m 50s) Loss: 0.3675(0.3675) Grad: 474690.5000  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3666(0.3874) Grad: 227837.1562  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3786(0.3881) Grad: 215867.2344  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6386(0.6386) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.3881  avg_val_loss: 0.6696  time: 121s\n",
      "Epoch 18 - avg_train_Score: 0.3881 avgScore: 0.6696\n",
      "Epoch 18 - Save Best Score: 0.6696 Model\n",
      "Epoch 18 - Save Best Loss: 0.6696 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7269(0.6696) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 12m 3s) Loss: 0.3657(0.3657) Grad: 480076.7500  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3690(0.3783) Grad: 174878.6250  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3575(0.3782) Grad: 304918.0312  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6520(0.6520) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.3782  avg_val_loss: 0.6677  time: 121s\n",
      "Epoch 19 - avg_train_Score: 0.3782 avgScore: 0.6677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7464(0.6677) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Save Best Score: 0.6677 Model\n",
      "Epoch 19 - Save Best Loss: 0.6677 Model\n",
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 12m 30s) Loss: 0.3798(0.3798) Grad: 569951.1875  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.4149(0.3671) Grad: 202273.1875  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3755(0.3669) Grad: 187495.3125  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.6333(0.6333) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.3669  avg_val_loss: 0.6646  time: 122s\n",
      "Epoch 20 - avg_train_Score: 0.3669 avgScore: 0.6646\n",
      "Epoch 20 - Save Best Score: 0.6646 Model\n",
      "Epoch 20 - Save Best Loss: 0.6646 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7310(0.6646) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 12m 48s) Loss: 0.2914(0.2914) Grad: 413164.5000  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3286(0.3429) Grad: 172670.2500  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3791(0.3451) Grad: 200281.9219  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.6253(0.6253) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3451  avg_val_loss: 0.6650  time: 121s\n",
      "Epoch 21 - avg_train_Score: 0.3451 avgScore: 0.6650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7214(0.6650) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 11m 52s) Loss: 0.3043(0.3043) Grad: 312101.7812  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3063(0.3202) Grad: 371704.5938  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3294(0.3197) Grad: 282513.6562  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6474(0.6474) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3197  avg_val_loss: 0.6638  time: 121s\n",
      "Epoch 22 - avg_train_Score: 0.3197 avgScore: 0.6638\n",
      "Epoch 22 - Save Best Score: 0.6638 Model\n",
      "Epoch 22 - Save Best Loss: 0.6638 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7296(0.6638) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 12m 19s) Loss: 0.2785(0.2785) Grad: 316169.0000  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3655(0.3203) Grad: 183453.4062  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3631(0.3216) Grad: 233029.0938  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6386(0.6386) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3216  avg_val_loss: 0.6624  time: 121s\n",
      "Epoch 23 - avg_train_Score: 0.3216 avgScore: 0.6624\n",
      "Epoch 23 - Save Best Score: 0.6624 Model\n",
      "Epoch 23 - Save Best Loss: 0.6624 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7378(0.6624) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.3271(0.3271) Grad: 314797.9375  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3383(0.3143) Grad: 161056.0781  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3809(0.3158) Grad: 200837.2656  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6390(0.6390) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3158  avg_val_loss: 0.6598  time: 121s\n",
      "Epoch 24 - avg_train_Score: 0.3158 avgScore: 0.6598\n",
      "Epoch 24 - Save Best Score: 0.6598 Model\n",
      "Epoch 24 - Save Best Loss: 0.6598 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7446(0.6598) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 12m 10s) Loss: 0.3642(0.3642) Grad: 359772.1250  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3760(0.3061) Grad: 205528.3125  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3221(0.3066) Grad: 155878.5000  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.6439(0.6439) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3066  avg_val_loss: 0.6578  time: 122s\n",
      "Epoch 25 - avg_train_Score: 0.3066 avgScore: 0.6578\n",
      "Epoch 25 - Save Best Score: 0.6578 Model\n",
      "Epoch 25 - Save Best Loss: 0.6578 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7505(0.6578) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 12m 15s) Loss: 0.3372(0.3372) Grad: 327837.5938  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.3511(0.2924) Grad: 419541.4062  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2995(0.2918) Grad: 349738.8750  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6364(0.6364) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.2918  avg_val_loss: 0.6567  time: 121s\n",
      "Epoch 26 - avg_train_Score: 0.2918 avgScore: 0.6567\n",
      "Epoch 26 - Save Best Score: 0.6567 Model\n",
      "Epoch 26 - Save Best Loss: 0.6567 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7384(0.6567) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 11m 43s) Loss: 0.2832(0.2832) Grad: 324249.7188  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.3450(0.2767) Grad: 184294.5625  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.3061(0.2781) Grad: 149040.5781  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.6327(0.6327) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.2781  avg_val_loss: 0.6563  time: 121s\n",
      "Epoch 27 - avg_train_Score: 0.2781 avgScore: 0.6563\n",
      "Epoch 27 - Save Best Score: 0.6563 Model\n",
      "Epoch 27 - Save Best Loss: 0.6563 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7287(0.6563) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 13m 17s) Loss: 0.2663(0.2663) Grad: 312027.8125  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2806(0.2847) Grad: 153708.8438  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2955(0.2847) Grad: 156180.1562  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6441(0.6441) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.2847  avg_val_loss: 0.6549  time: 121s\n",
      "Epoch 28 - avg_train_Score: 0.2847 avgScore: 0.6549\n",
      "Epoch 28 - Save Best Score: 0.6549 Model\n",
      "Epoch 28 - Save Best Loss: 0.6549 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7231(0.6549) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 12m 6s) Loss: 0.2947(0.2947) Grad: 298195.4375  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2455(0.2702) Grad: 292456.6250  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2780(0.2700) Grad: 392175.6562  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6466(0.6466) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2700  avg_val_loss: 0.6543  time: 121s\n",
      "Epoch 29 - avg_train_Score: 0.2700 avgScore: 0.6543\n",
      "Epoch 29 - Save Best Score: 0.6543 Model\n",
      "Epoch 29 - Save Best Loss: 0.6543 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7182(0.6543) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 12m 12s) Loss: 0.2470(0.2470) Grad: 306693.0625  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2585(0.2555) Grad: 274549.8750  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2491(0.2554) Grad: 307961.5938  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6426(0.6426) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2554  avg_val_loss: 0.6546  time: 121s\n",
      "Epoch 30 - avg_train_Score: 0.2554 avgScore: 0.6546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7328(0.6546) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 11m 57s) Loss: 0.2334(0.2334) Grad: 285304.9688  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2414(0.2474) Grad: 387898.2812  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2541(0.2474) Grad: 295961.2812  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.6417(0.6417) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2474  avg_val_loss: 0.6564  time: 121s\n",
      "Epoch 31 - avg_train_Score: 0.2474 avgScore: 0.6564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7303(0.6564) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 11m 41s) Loss: 0.2633(0.2633) Grad: 459426.1875  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2725(0.2430) Grad: 271557.7500  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.1966(0.2432) Grad: 263742.2188  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6455(0.6455) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2432  avg_val_loss: 0.6541  time: 121s\n",
      "Epoch 32 - avg_train_Score: 0.2432 avgScore: 0.6541\n",
      "Epoch 32 - Save Best Score: 0.6541 Model\n",
      "Epoch 32 - Save Best Loss: 0.6541 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7375(0.6541) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 0.2181(0.2181) Grad: 307236.3438  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2327(0.2381) Grad: 372193.2812  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 47s (remain 0m 0s) Loss: 0.2121(0.2379) Grad: 135680.6875  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6430(0.6430) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2379  avg_val_loss: 0.6538  time: 121s\n",
      "Epoch 33 - avg_train_Score: 0.2379 avgScore: 0.6538\n",
      "Epoch 33 - Save Best Score: 0.6538 Model\n",
      "Epoch 33 - Save Best Loss: 0.6538 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7315(0.6538) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 11m 58s) Loss: 0.2213(0.2213) Grad: 379232.4062  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2686(0.2363) Grad: 140546.3281  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2473(0.2368) Grad: 125710.4062  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.6435(0.6435) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2368  avg_val_loss: 0.6544  time: 121s\n",
      "Epoch 34 - avg_train_Score: 0.2368 avgScore: 0.6544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7383(0.6544) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 11m 52s) Loss: 0.2442(0.2442) Grad: 529400.8125  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 39s (remain 0m 8s) Loss: 0.2328(0.2316) Grad: 301110.5625  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.1843(0.2315) Grad: 271109.0625  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6402(0.6402) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2315  avg_val_loss: 0.6551  time: 121s\n",
      "Epoch 35 - avg_train_Score: 0.2315 avgScore: 0.6551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7367(0.6551) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 0.1997(0.1997) Grad: 308637.0625  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2412(0.2258) Grad: 265163.0312  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2401(0.2254) Grad: 280400.5938  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6402(0.6402) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2254  avg_val_loss: 0.6557  time: 121s\n",
      "Epoch 36 - avg_train_Score: 0.2254 avgScore: 0.6557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7426(0.6557) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 11m 48s) Loss: 0.2295(0.2295) Grad: 298906.0625  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2298(0.2228) Grad: 282995.9062  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.1873(0.2224) Grad: 218551.0625  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.6393(0.6393) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2224  avg_val_loss: 0.6557  time: 121s\n",
      "Epoch 37 - avg_train_Score: 0.2224 avgScore: 0.6557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7402(0.6557) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 12m 32s) Loss: 0.2301(0.2301) Grad: 244153.0156  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2433(0.2184) Grad: 295824.5312  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2492(0.2186) Grad: 301843.2500  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6366(0.6366) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2186  avg_val_loss: 0.6560  time: 121s\n",
      "Epoch 38 - avg_train_Score: 0.2186 avgScore: 0.6560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7387(0.6560) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 11m 49s) Loss: 0.2419(0.2419) Grad: 286485.0938  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2381(0.2175) Grad: 263479.8125  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2345(0.2178) Grad: 285432.1875  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6393(0.6393) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2178  avg_val_loss: 0.6555  time: 121s\n",
      "Epoch 39 - avg_train_Score: 0.2178 avgScore: 0.6555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7371(0.6555) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_565/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 11m 44s) Loss: 0.2370(0.2370) Grad: 274279.4688  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 40s (remain 0m 8s) Loss: 0.2497(0.2168) Grad: 293985.9688  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 48s (remain 0m 0s) Loss: 0.2315(0.2170) Grad: 264978.9375  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6401(0.6401) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2170  avg_val_loss: 0.6553  time: 121s\n",
      "Epoch 40 - avg_train_Score: 0.2170 avgScore: 0.6553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7368(0.6553) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 4 result ==========\n",
      "score: 0.6553\n",
      "========== CV ==========\n",
      "score: 0.6740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3xb1fnH8c/Vlrx3lrP3IAkhQAghYSWEPcoooVBayigUKPArowXKhtIW6KDMljICYZW9AhQIMyRk7z3seMR7SbKk+/tDlhM3TuIh2bH1fffll6yre889j2U3h0fnPMcwTdNERERERERERESkA1k6uwMiIiIiIiIiIhJ/lJQSEREREREREZEOp6SUiIiIiIiIiIh0OCWlRERERERERESkwykpJSIiIiIiIiIiHU5JKRERERERERER6XBKSomIiIiIiIiISIdTUkpERERERERERDqcklIiIiIiIiIiItLhlJQSkS7tpz/9Kf379+/sboiIiIh0SZs3b8YwDJ555pnO7oqIxCElpUQkJgzDaNHXZ5991tld3cPmzZu5+OKLGTRoEC6Xix49enDUUUdx++23d3bXREREJI6deuqpeDweqqqq9nrOrFmzcDgclJSURP3+GiOJSLQZpmmand0JEel+nn/++SbPn332WebOnctzzz3X5Pjxxx9PTk5Om+9TX19PKBTC6XS2uY3drV+/nokTJ+J2u/nZz35G//792bFjBz/88APvv/8+Xq83KvcRERERaa05c+Zw3nnn8e9//5sLL7xwj9dra2vJzs7mmGOO4a233mpRm5s3b2bAgAH861//4qc//elez9MYSURiwdbZHRCR7umCCy5o8vzbb79l7ty5exz/X7W1tXg8nhbfx263t6l/e/PQQw9RXV3N4sWL6devX5PXioqKonqv/ampqSEhIaFD7ykiIiIHrlNPPZWkpCRmz57dbFLqzTffpKamhlmzZkX93hojiUgsaPmeiHSaadOmMXr0aBYuXMhRRx2Fx+PhlltuAcKDqpNOOolevXrhdDoZNGgQd911F8FgsEkb/1tTKlIX4Y9//CNPPPEEgwYNwul0MnHiRL7//vv99mnDhg306dNnj8EWQHZ29h7H3n//faZOnUpSUhLJyclMnDiR2bNnNznnlVdeYcKECbjdbjIzM7ngggvIy8vbI47ExEQ2bNjAiSeeSFJSUuOAMhQK8fDDDzNq1ChcLhc5OTlcdtlllJWV7TceERER6T7cbjdnnnkmn3zySbOJoNmzZ5OUlMSpp55KaWkpN9xwA2PGjCExMZHk5GRmzpzJkiVL2nRvjZFEJBaUlBKRTlVSUsLMmTMZN24cDz/8MEcffTQAzzzzDImJiVx33XU88sgjTJgwgdtuu42bbrqpRe3Onj2bBx98kMsuu4y7776bzZs3c+aZZ1JfX7/P6/r168e2bdv49NNP93uPZ555hpNOOonS0lJuvvlm7r//fsaNG8cHH3zQ5JxzzjkHq9XKfffdxy9+8Qtef/11jjzySMrLy5u0FwgEmDFjBtnZ2fzxj3/krLPOAuCyyy7j//7v/5g8eTKPPPIIF198MS+88AIzZszYbzwiIiLSvcyaNYtAIMDLL7/c5HhpaSkffvghZ5xxBm63m40bN/LGG29w8skn8+c//5n/+7//Y9myZUydOpX8/PxW31djJBGJCVNEpANceeWV5v/+X87UqVNNwHzsscf2OL+2tnaPY5dddpnp8XhMr9fbeOyiiy4y+/Xr1/h806ZNJmBmZGSYpaWljcfffPNNEzDffvvtffZz+fLlptvtNgFz3Lhx5jXXXGO+8cYbZk1NTZPzysvLzaSkJPOwww4z6+rqmrwWCoVM0zRNv99vZmdnm6NHj25yzjvvvGMC5m233dYkDsC86aabmrQ1b948EzBfeOGFJsc/+OCDZo+LiIhI9xYIBMyePXuakyZNanL8scceMwHzww8/NE3TNL1erxkMBpucs2nTJtPpdJp33nlnk2OA+a9//Wuf99UYSURiQTOlRKRTOZ1OLr744j2Ou93uxu+rqqrYuXMnU6ZMoba2ltWrV++33XPPPZe0tLTG51OmTAFg48aN+7xu1KhRLF68mAsuuIDNmzfzyCOPcPrpp5OTk8OTTz7ZeN7cuXOpqqripptuwuVyNWnDMAwAFixYQFFREb/85S+bnHPSSScxfPhw3n333T3uf8UVVzR5/sorr5CSksLxxx/Pzp07G78mTJhAYmIi//3vf/f7sxAREZHuw2q1ct555/HNN9+wefPmxuOzZ88mJyeHY489FgiPsSyW8H/uBYNBSkpKSExMZNiwYfzwww+tvq/GSCISC0pKiUin6t27Nw6HY4/jK1as4IwzziAlJYXk5GSysrIai6RXVFTst92+ffs2eR5JULWkxsDQoUN57rnn2LlzJ0uXLuXee+/FZrNx6aWX8vHHHwPhugoAo0eP3ms7W7ZsAWDYsGF7vDZ8+PDG1yNsNht9+vRpcmzdunVUVFSQnZ1NVlZWk6/q6uoOLywqIiIinS9SUylSo2n79u3MmzeP8847D6vVCoTrLT300EMMGTIEp9NJZmYmWVlZLF26tEVjqeZojCQi0abd90SkU+0+IyqivLycqVOnkpyczJ133smgQYNwuVz88MMP3HjjjYRCof22GxmQ/S/TNFvcN6vVypgxYxgzZgyTJk3i6KOP5oUXXuC4445rcRutsfsnmhGhUIjs7GxeeOGFZq/JysqKSV9ERETkwDVhwgSGDx/Oiy++yC233MKLL76IaZpNdt279957ufXWW/nZz37GXXfdRXp6OhaLhWuvvbZFY6l90RhJRKJFSSkROeB89tlnlJSU8Prrr3PUUUc1Ht+0aVOn9emQQw4BYMeOHQAMGjQIgOXLlzN48OBmr4nsTrNmzRqOOeaYJq+tWbOm2d1r/tegQYP4+OOPmTx5crMJPBEREYlPs2bN4tZbb2Xp0qXMnj2bIUOGMHHixMbXX331VY4++miefvrpJteVl5eTmZkZtX5ojCQi7aHleyJywInMctp9VpPf7+fRRx+N+b3nzZvX7G4t7733HrBrmvn06dNJSkrivvvuw+v1Njk30u9DDjmE7OxsHnvsMXw+X+Pr77//PqtWreKkk07ab3/OOeccgsEgd9111x6vBQKBPXanERERkfgQmRV12223sXjx4iazpCA8nvrfGeKvvPIKeXl5bbqfxkgiEguaKSUiB5wjjjiCtLQ0LrroIq6++moMw+C5555r1dK7tnrggQdYuHAhZ555JgcddBAAP/zwA88++yzp6elce+21ACQnJ/PQQw9xySWXMHHiRM4//3zS0tJYsmQJtbW1/Pvf/8Zut/PAAw9w8cUXM3XqVH784x9TWFjII488Qv/+/fn1r3+93/5MnTqVyy67jPvuu4/Fixczffp07HY769at45VXXuGRRx7hRz/6USx/JCIiInIAGjBgAEcccQRvvvkmwB5JqZNPPpk777yTiy++mCOOOIJly5bxwgsvMHDgwDbdT2MkEYkFJaVE5ICTkZHBO++8w/XXX8/vfvc70tLSuOCCCzj22GOZMWNGTO99yy23MHv2bD7//HNeeOEFamtr6dmzJ+eddx633norAwYMaDz35z//OdnZ2dx///3cdddd2O12hg8f3mQg9dOf/hSPx8P999/PjTfeSEJCAmeccQYPPPAAqampLerTY489xoQJE3j88ce55ZZbsNls9O/fnwsuuIDJkydH+0cgIiIiXcSsWbP4+uuvOfTQQ/dYKnfLLbdQU1PD7NmzmTNnDgcffDDvvvsuN910U5vupTGSiMSCYXbE1AMREREREREREZHdqKaUiIiIiIiIiIh0OCWlRERERERERESkwykpJSIiIiIiIiIiHU5JKRERERERERER6XBKSomIiIiIiIiISIdTUkpERERERERERDqcrbM70NFCoRD5+fkkJSVhGEZnd0dEREQOcKZpUlVVRa9evbBY4vfzPI2hREREpKVaOn6Ku6RUfn4+ubm5nd0NERER6WK2bdtGnz59OrsbnUZjKBEREWmt/Y2f4i4plZSUBIR/MMnJyVFv3zRNvF4vLpcrKp8ivnbZ05S4B5NR/SGH/eF39En3tPjam764ic+2f8ZvJv6GM4ec2e6+7Eu04+4KFHN8xAzxGbdijo+YIT7jbm3MlZWV5ObmNo4h4lXMx1AlG5jzhw+o9I1m6NqXyE+xcP6Lf9/vddd/dj1f5X/Fbw/7LacMOqVdfaitr+WYV44B4L/n/Be3zd2u9vZHf3/xETPEZ9yKOT5ihviMWzFHb/wUd0mpyA8vOTk5Zkkph8MRtV/OAQNTqC1JIDWUzTury/nNCT1afG1SchJWtxWr2xqTWHcX7bi7AsUcHzFDfMatmOMjZojPuNsac7z8fPYm5mMo1zByPK9SbyZgS8ghs2ZDi+5juIzGsU57++UOurG6reHvE90kOzR+irZ4jBniM27FHB8xQ3zGrZijN36K38IIXUSvY48AoMqZy+aP3sYfCLX4WofVAYAv6ItJ30RERESixu4m3VENgNeZRmpVCfXB/Y976kP14cut9nZ3wWbZ9XltfbC+3e2JiIjIvikpdYDLGdMXgOqEXhy/4RM+WlnQ4mudViegQZWIiIh0DekJ4SSUz5VGdl05hRV1+72mMSllaX9SyjAMbIatSbsiIiISO0pKHeCSM93YLfWYFju9SoK88M2WFl+rmVIiIiLSlSSkhscuXmca9lCQwi079ntNNJNSsGvGVSAUiEp7IiIisndKSh3gDMMgKzcBgGpbH+oXfsWG4uoWXRuZKaWklIiIiHQFCRnhDV28rlQAyjdt3e81keTR7kvv2iPSjmZKiYiIxJ6SUl1A1uAcAKoTczkn77+8+N3+B2gADkv400Z/0B+zvomIiIhES0JWOgAhq5uA1UXV1u37vSbqM6Ua2lFSSkREJPaUlOoCsvqGt1CsSsplRN5mXluwFW99cL/XRZbv+UNKSomIiMiBz5beE9PSUOzclUZ9fv5+r4nUzoxWUkozpURERDqOklJdQFZuOClVndgbq9dk4LYVvLds/zUWtHxPREREuhIzuTcWeykAPmcaZsH+N3iJ5u57sCu5pZpSIiIisaekVBeQ2sODzW4haHVR687irO2f859Fefu9rnGmlJbviYiISBcQSu6Nw1oCNBQ7Lyna7zUxW76n3YtFRERiTkmpLsBiMcjokwiEl/CNyd9AfnHFfq/T7nsiIiLSpSRkk9iQlPI500goK97vJbHafU/L90RERGJPSakuIlJXqia1D7b6ED3XLsY0zX1eE1m+p0/6REREpEuwWEl11QHhmlKp1aX7He9Effc9QzWlREREOoqSUl1EpK5UbUYfAGZs/ZqKun0PljRTSkRERLqazPCQB58zjSR/LeUllfs8P1YzpVRTSkREJPaUlOoiIjOlKlwDMIGhldsoqPTu8xqHRUkpERER6VqyU90A1LpSAShcv3mv55qm2Zg8inpNKc2UEhERiTklpbqI9J4JWKwGftON15VOYl0dBRX7TkpFlu+p0LmIiIh0FclZyQD4nWmYQNmmrXs9d/fZTNHefU9JKRERkdhTUqqLsNotpPdKAKA6MRdLvUnRzn0XO2/cfS+kpJSIiIh0DYk9MgEwLQ7q7QnUbN2+13N3TxxFa6ZUpDaVanKKiIjEnpJSXUikrlRVcriuVMXWvH2eH5kppeV7IiIi0lVY0/sQsIXrSPmcafjyd+z13FgkpTRTSkREpOMoKdWFROpKVSfnAhDatn6f5zfOlNLyPREREekqUnpj2koB8DrToLBlSSmrYY3K7SNJKRU6FxERiT0lpbqQSFKqKiGclHIUbNjn+dp9T0RERLqclD7YbSVAeKaUfWfxXk/dvci5YRhRuX2kNpVmSomIiMSeklJdSEbvRDDAZ0/F50jGXdKy5XuqiSAiIiJdhjMFt70cAK8rjcTyvSelImOcaC3d270tJaVERERiT0mpLsTutJKW4wGgKjGXxIq9D9IAHJbwTKmAGdAUdBEREekaDIMkdx0QnimVXFOOGWh+HBNJHEVr5z3YrdC5klIiIiIxp6RUF9NYVyqxD4k1FfgDob2eG1m+B6orJSIiIl1HRmL4sc6VhtUMUbejoNnzGpNSMZgppQ/0REREYq9Tk1JffPEFp5xyCr169cIwDN54440WX/vVV19hs9kYN25czPp3IMqM7MCX1JekulqKqrx7PVdJKRERke6pu4+hstPdANS5UgEo3rC12fNimZTSTCkREZHY69SkVE1NDWPHjuXvf/97q64rLy/nwgsv5Nhjj41Rzw5c6b0SAKhzZ+L01lNYufci5jaLDZsRnoLuDykpJSIi0l109zFU76xMAOodqZgYlG3a1ux5kdlMkSV30dCYlFJNThERkZiL3r/gbTBz5kxmzpzZ6usuv/xyzj//fKxWa6s+GewOElLCxct9jhSsdUEKK+qAtL2eb7faCQQC2oFPRESkG+nuY6iUnj0IEcJi2PA7kqjZtr3Z82IxU0o1pURERDpOpyal2uJf//oXGzdu5Pnnn+fuu+/e7/k+nw+fb1dCprKyEgDTNDFNM+r9i7Qbi7YBPCkNn945kgiFbBTnF2OO6bnX851WJ3WBOnwBX8z6BLGP+0CkmONHPMatmONHPMbd2pi7y8+mK42hjLTe+O1FuOrT8TnT8OflN3vPSHkCu8UetT7tXlMq1u+9/v7iRzzGrZjjRzzGrZhbdn5LdKmk1Lp167jpppuYN28eNlvLun7fffdxxx137HHc6/XicDiauaL9fD4fhmHEpG2sJharQSho4nckUbdxJV7v0L2eHtmBr6quCq9r7/WnoiGmcR+gFHP8iMe4FXP8iMe4WxOz1xvbfz87QlcbQxnOLIL2NVCfjteZhlmwo9n3ocZbA4DVsEbvfWrYQ8Zb7+2Q915/f/EjHuNWzPEjHuNWzPvW0n9Du0xSKhgMcv7553PHHXcwdOjekzD/6+abb+a6665rfF5ZWUlubi4ulwuXyxX1fkYyh06nM2a/oJ4UB9WlPnyOVJwFm/YZR6TYuWEzYhJvREfEfaBRzPERM8Rn3Io5PmKG+Iy7tTH7/V27LmOXHENlDcBiexMYhM+VRkLpumbvaVjD75/T6oxan9yOcJH1EKGYjp1Af3/xEjPEZ9yKOT5ihviMWzFHb/zUZZJSVVVVLFiwgEWLFnHVVVcBEAqFME0Tm83GRx99xDHHHLPHdU6nE6fTucdxwzBi9ssTaTtW7SekOKku9eF3puAs3rrP+zit4dj9QX/M/1hiHfeBSDHHj3iMWzHHj3iMuzUxd/WfS5ccQ9ldOJ3h5YJeZxrZhcWNr+8uYIYLndut9qj1yW61N7bdEe+9/v7iRzzGrZjjRzzGrZj3f25LdJmkVHJyMsuWLWty7NFHH+XTTz/l1VdfZcCAAZ3Us463q9h5MgllBfs8NzJTSoXORURE4lNXHUMluLzUAT5nGs56H6HKSqwpKU3Oienueyp0LiIiEnOdmpSqrq5m/fr1jc83bdrE4sWLSU9Pp2/fvtx8883k5eXx7LPPYrFYGD16dJPrs7Ozcblcexzv7hJSGhJNzhQSqtaEC4LuJQu5+0wpERER6R7iYQyVmmRSB9S4w7sM+/PycP9PUioWu+8pKSUiItJxLJ158wULFjB+/HjGjx8PwHXXXcf48eO57bbbANixYwdbt27tzC4ekDwNM6X8jhSS6qqp9Ab2em5kppQ/pKSUiIhIdxEPY6jM1HA9J68rFYDKLdv3OKc+GLukVCC49/GViIiIREenzpSaNm3aPrcJfOaZZ/Z5/e9//3t+//vfR7dTXUBCasNMKUcKnjofhZVeUtzND8a0fE9ERKT7iYcxVM/sDJYBQVsKIcNC2aZt5PzPObGYKRVZCqiZUiIiIrHXqTOlpG0aZ0o5U7B5AxRU7H2rRS3fExERka6od69+BI0AGBZ8jlSqtzUzUyqSlLJq+Z6IiEhXpKRUF7Sr0HkKeKGgtHqv52qmlIiIiHRFnoyB1DrKAfC50qjP37HHOTGpKRXZfS+k5XsiIiKxpqRUFxQpdF7vSCKEldot6/d6rsPSUFNKM6VERESkK0nuQ729DACvMxUK90xKafc9ERGRrk1JqS7IlWDHYg3vtud3JMO21Xs9V8v3REREpEtKyMK0lwLgc6bhKCne4xTtviciItK1KSnVBRkWA09ywwwoRwqOgk17PVfL90RERKRLsliwOauAcFLKU1VGyN/0Q7ZY7L7XWOg8qKSUiIhIrCkp1UUlpDbUlXKm4Cndczp7hJJSIiIi0lW5XbUA1LrTAAgUFDR5PTKbKRbL91RTSkREJPaUlOqidi927qnYudfzIsv3NAVdREREupqkRBOAWnc6wB7FzrX7noiISNempFQX5UnZtXwvsaaC+mCo2fM0U0pERES6qvTUcILI70wFwLt9e5PXY7n7npJSIiIisaekVBfVOFPKmUJCXR3FVc0nnSIzpZSUEhERka4mJyu8bC9kTSJosVGxpWlSKrLELiY1pZSUEhERiTklpbqo3WdKObwBCiq9zZ7nsDScp933REREpIvpkdOfekt4DONzplGztfmZUqopJSIi0jUpKdVF7V7o3FIXorC8rtnzIsv3lJQSERGRriYrazg1jjIAfM7UPWtKxWD3vUhbQTNIMBSMWrsiIiKyJyWluqiEhplSPkcyIb+FsoK8Zs+LLN9TUkpERES6mqSMIdQ6ygHwOtOgaC+FzmOQlAIImJotJSIiEktKSnVRkZpS9Y5kQoaF4OYVzZ6nQuciIiLSVRmuZAKNM6XScJQUY5pm4+ux2H1v96WAkZlYIiIiEhtKSnVRrgQ7FosBgN+RjG3HxmbPU1JKREREujKLoxKAOlca1kA9wZKSxtdiPlNKdaVERERiSkmpLsqwGE2KnbuKtjV7XmT5nnaQERERka7I4a4FoNaTDkD9jl1L+GKx+57VYsVihIfIGj+JiIjElpJSXZinYQmfz5GCp7yo2XMiSSnNlBIREZGuKCEhnHiqc6UBNCl2Hovd92BXkktJKRERkdhSUqoLayx27kzBU1XWpMZCRKTGgpJSIiIi0hWlpDQkiByp4cf8XZu7xGL5HuxKcikpJSIiEltKSnVhCakNO+s5Ukisq6HKt2fdA6elYfmeCnWKiIhIF5SZlQyAafEQsDqp2bpbUioYm6RU40wpjZ9ERERiSkmpLmzXTKlkXHV+Ciu8e5yj5XsiIiLSlWVn5uKzhutK+Zyp1Gzb3vhaLHbfg11JqYCpQuciIiKxpKRUFxapKeV3pGL1Bimo3DMpFdl9zx/0d2jfRERERKIhK2MI1Y5yALzOtCaFzmO1fE8zpURERDqGklJdWEJjofNkQnUWSnbu3OOcSFIqYAYIhoId2j8RERGR9srMGEGNsxwAnzMNo7Cg8bVY7L4HqiklIiLSUZSU6sISUncVOjeDBr5ta/c4x2P3YGAAUOot7dD+iYiIiLRXekI2NY4yALyuNGzVlYRqw8v5tPueiIhI16akVBfmSW4oYm5PJGRYYNvqPc5xWp0MSh0EwNKdSzu0fyIiIiLtZTEshBwVANS6MwCoLwjPlorVTKlIjapI+yIiIhIbSkp1Ye5EOxaLAYYFvyMZe9HWZs8bmzUWgCXFSzqyeyIiIiJRYXPXAFDrTgOgPi8//Bjr3fc0U0pERCSmlJTqwgyLgadhBz6/Ixl3SX6z5zUmpYqUlBIREZGux+UJb9jic6UCUL8jn5AZatwdL1a77ykpJSIiEltKSnVxnsZi56kkVJY0e04kKbWyZKUGVyIiItLlJCWHh6wBWyomUJ+f32RpXcwKnWv3PRERkZhSUqqLS4jMlHImk1BTRSAY2uOc/in9SXIk4Q16WVu6ZzF0ERERkQNZekYiAKbFScDmxpe3I6ZJqUh7kZlYIiIiEhtKSnVxCY0zpVLw1HkprvbtcY7FsHBQ1kEALC5e3JHdExEREWm3rLQ+1NmqAfA506jbvr3J7O+Y7b6nmVIiIiIxpaRUF5eQGp4p5XOmYK8LUFi5Z1IKVOxcREREuq7M9MFUO8sA8DrTqM/f0ZiUMjCwGtao3i9So0plD0RERGJLSakuLlJTyu9IwfQaFJeUNnteJCm1tHhph/VNREREJBqyUgdQ7SgHwOdKw9hZhN/vBcKzmgzDiOr9bEZDTSklpURERGJKSakubvflewGvheqCTc2ed1DmQRgY5FXnsbNuZ0d2UURERKRdsjzZ1DjCM6XqnOkYwSD+okIg+jvv7d7m7nWrREREJPqUlOriPI2FzlPANAhuW9PseYmORAalDgJgSZGW8ImIiEjXkeHKoNpZDkBNQgYA9Tt2ANEvcr57m5opJSIiEltKSnVxkZlSfnsiIcOCLX/jXs9VXSkRERHpiuxWO6azCoA6dxoAwfzYJaUihdOVlBIREYktJaW6OHeiHYvFAMNCvT0JZ8n2vZ6rpJSIiIh0VXZPuIaU35EKQLAgvHwv2jvvgXbfExER6ShKSnVxhsVoXMLnc6bgKS/e67ljs8NJqRUlKzTIEhERkS7FkxR+DNhSMDEINSSltHxPRESk61JSqhvwJDckpRwpeKor9npe/+T+JDuS8QV9rClrvvaUiIiIyIEoNc2FSQgMG/X2RMyC8AdxsVy+p0LnIiIisaWkVDeQkNpQV8qZQkJtLdW+5gdQFsPCQVkHAVrCJyIiIl1LZkpvau3hulJeVxrmjoakVAx339NMKRERkdhSUqob8DQUO/c5UnB66ymo8O713Ma6UtqBT0RERLqQrJT+VDvLAPA507AWlQJaviciItKVKSnVDSSk7Fq+Z9SZFJeW7/VcFTsXERGRrigzqRfVjnIAvM40rHVePF5TSSkREZEuTEmpbmD35Xv1dVYqirbs9dwxmWMwMMivyae4du9F0UVEREQOJFnuLGqc5QBUezIByKxUTSkREZGuTEmpbmBXofNkQn4L3oKNez030ZHI4LTBgGZLiYiISNeR5c6i2hFevlfnSQcgs8JsTCBFU+NMKe1WLCIiElNKSnUDu2ZKpQJgbF+/z/O1hE9ERES6mkx3JtUNM6V8rlQAsmI0U0rL90RERDqGklLdQEJDoXO/PZGQYcFRtHWf50eSUkuLl8a8byIiIiLR4LF7CDrDu+/VO1IAyKw0Y7L7XmT2lZJSIiIisdWpSakvvviCU045hV69emEYBm+88cY+z3/99dc5/vjjycrKIjk5mUmTJvHhhx92TGcPYO5EO4bFAMNCvT0JT2nBPs+PJKVWlKzQtHQREZEuKF7HUK7EcI2ngCUJE4PMCmKzfK8h0aWaUiIiIrHVqUmpmpoaxo4dy9///vcWnf/FF19w/PHH895777Fw4UKOPvpoTjnlFBYtWhTjnh7YDIuxq66UMwVPZek+z++f3J8UZwq+oI81ZWs6oosiIiISRfE6hkpKcRE0gmBY8TlTwjOltHxPRESky4r+R0utMHPmTGbOnNni8x9++OEmz++9917efPNN3n77bcaPHx/l3nUtCSkOasp9+BwpZNTmEQyZWC1Gs+cahsFBmQcxL28eS4qXMDpzdAf3VkRERNojXsdQWYk9qHWUk+TLwOdMI6uiXEkpERGRLqxTk1LtFQqFqKqqIj09fa/n+Hw+fD5f4/PKykoATNPENM2o9ynSbiza3hdPihOowu9IweX1UlzlJSfZtdfzx2aNZV7ePBYXLeb84ee3+/6dFXdnUszxIx7jVszxIx7jbm3M3fFn01XHUJlJfahsSEp5nWlkFW/C5Q1FvT82o6GmVLA+pu+//v7iRzzGrZjjRzzGrZhbdn5LdOmk1B//+Eeqq6s555xz9nrOfffdxx133LHHca/Xi8PhiEm/fD4fhtH8LKVYcSVaw/d2pmCtCbGtYCcpjsy9nj88ZTgAS4qW4PV6o9KHzoi7synm+BGPcSvm+BGPcbcm5mj9O3kg6apjqFRHOvmOcgB2pmaSUwzZq4rwTorue2QGwgNpf9Af8/dff3/xIx7jVszxIx7jVsz71tJ/P7tsUmr27NnccccdvPnmm2RnZ+/1vJtvvpnrrruu8XllZSW5ubm4XC5crr3PJGqrSObQ6XR26C9oUrobAJ8jhcBOK3XlBbiG9tnr+RN6TcBiWNhRu4OqUBVZnqx23b+z4u5Mijk+Yob4jFsxx0fMEJ9xtzZmv9/fAb3qOF15DNUzuSfznZsBKE4Lf/jWa3Vx1PuT4E4AIGgGYxJrhP7+4iNmiM+4FXN8xAzxGbdijt74qUsmpV566SUuueQSXnnlFY477rh9nut0OnE6nXscNwwjZr88kbY78pczMTU8YPI7kwnUWanduRXDmLj38x2JDE4dzNqytSzduZTj+u3759gSnRF3Z1PM8SMe41bM8SMe425NzN3p59LVx1BZniyqG2ZK1SSmApC5Ij/q/Ynsvlcfqo/5+6+/v/gRj3Er5vgRj3Er5v2f2xKduvteW7z44otcfPHFvPjii5x00kmd3Z0DhielYfc9Rwpm0CCUv2G/14zNGgvAkuIlMe2biIiIdL7uMIbKdGVS4ywDIGRPIWRAQn459Tt2RPU+jTWlVOhcREQkpjo1KVVdXc3ixYtZvHgxAJs2bWLx4sVs3boVCE8bv/DCCxvPnz17NhdeeCF/+tOfOOywwygoKKCgoICKiorO6P4BJSEl/Emm35kCgK1g036vUVJKRESka4rXMdTuM6XswWTW9wwfr/n6m6jeZ/eZUiIiIhI7nZqUWrBgAePHj2/civi6665j/Pjx3HbbbQDs2LGjcXAF8MQTTxAIBLjyyivp2bNn49c111zTKf0/kCSkNiSl7EmEDAvOkvz9XhNJSq3YuYL6oAZdIiIiXUW8jqGSHcn4XVUA2AOJLOsfntFU802Uk1KWcFIqEApEtV0RERFpqlNrSk2bNm2f2wQ+88wzTZ5/9tlnse1QF+ZOtGNYDMyQhXp7Eu6Knfu9pl9yP1KdqZT7ylldupoxWWM6oKciIiLSXvE6hjIMg8QEOwHDj810sDY3DSim5ptvMEMhDEt0Pm+NJKU0U0pERCS2ulxNKWmeYTHwJDfUlXKm4Kmu3P81hsFBWQcBWsInIiIiXUNmYg41zvCyw6K0NEJOO8GSEnzr1kXtHjZL+HPbQCiwz+SfiIiItI+SUt1IQkOxc78jGU9tHTW+/U85V10pERER6UqyEnpQ7QgXO/cE0igZ1heAmq++jto9IjOlQEv4REREYklJqW7E01Ds3OdIxe4NUlC2/9lSSkqJiIhIV5LpzqTaWQ5Agj+NFX36ANGtK7V7UkpL+ERERGJHSaluJDJTyudMJlBnobxg236vGZM5BothYUfNDopqi2LdRREREZF2yXJnNc6USvSl8nFCJgC1CxYQ8vujco/I7nugpJSIiEgsKSnVjTTuwOdIIVBnpXrnlv1e47F7GJI6BNBsKRERETnwZXmyGmdKJfpTWeJIwkxLx6yro27R4qjcw2bs2gtISSkREZHYUVKqG9m90HnAa8FXvHU/V4Q1LuErUlJKREREDmyZ7kxqHOUAJPrSMLGTPzC8g3DNN9GpK2UYRpNi5yIiIhIbSkp1I5GZUj5HCpgGloKNLbpubLbqSomIiEjXsPvyvQR/KphWPk3sB0DN19GvK6WZUiIiIrGjpFQ3ktBQ6NzvSgXAUbS9RddFZkqtLFlJfVADLxERETlwZXmyqHKVEiKEO5BIasjNR67wDnze5csJVlRE5T5KSomIiMSeklLdiKeh0LnflkjIsOAqa1nh8r5JfUlzpuEP+VlVuiqWXRQRERFplzRnGkFrPUWJ4dqZk91Z7HSnUpPTB0IhaubPj8p9GpNS+sBOREQkZpSU6kbcSQ4MiwGGhXp7EgnVZS26zjAMDso6CNASPhERETmwWS1WMtzpbEtdDcBIb3g4uyg7vHFLzdfRqSulmlIiIiKxp6RUN2KxGHiSwp/q+RzJuGtrCYbMFl3bWOxcSSkRERE5wGW6s9iWGp7dbRQ7sRkGH3v6A1AbpbpSWr4nIiISe0pKdTORYud+ZwrOWj8lVTUtuk5JKREREekqsjxZFCduxWutIRBwcGx2CssyB2JaLPi3bKE+L6/d97BblZQSERGJNSWluhlPyq4d+IJeCyUFLSt2PjpzNBbDQkFNAYU1hbHsooiIiEi7ZLmzMA2T7alrADjC4qfW7mZrzgAAar5p/2wpzZQSERGJPSWlupmEhmLnPmcKgTorVUVbWnSdx+5haNpQQLOlRERE5MCW4c4AaKwr5SqsBuDL5IEA1ERhCZ9qSomIiMSeklLdTGSmlN+RQn2dlbqSrS2+Vkv4REREpCvIcmcBsD0lnJQqKXVzaK8UFmU1FDv/5hvMUKhd99DueyIiIrGnpFQ30zhTypFCyG8hVLS5xdcqKSUiIiJdQSQpVeOsINW2BRMLM5KsrE7vh8/uIlhWhm/NmnbdozEpZSopJSIiEitKSnUzCZGaUq4UAOwtXL4Hu5JSK0tW4g/6o985ERERkSjI9GQ2fp+btg2AnjuLCVqsLMloqCv19dftuodmSomIiMSeklLdzK7d91IBcJa0vGh5blIuac406kP1rCpdFYvuiYiIiLRbtju78fu+I9MAKNpmMKpnEj9khWtktreulGpKiYiIxJ6SUt2Mp2H5nt+WiImBp7KkxdcahrFrCV+RlvCJiIjIgalHQg9OHngy5w07jz6Tj8RmeKnxJ3Bi7+TGulK1CxcS8vnafA/tviciIhJ7Skp1M+4kB4YBGBb8jiQ8NVWtun5stupKiYiIyIHNMAzum3Ifvz38t9hyx9Hbsx6AURX5bE3KodSVjOn1UrdoUZvvYbcqKSUiIhJrSkp1MxaLgSd5V7FzV62POm/L60Op2LmIiIh0KYZB3wEGABUbKxmUncgPkV342rGELzJTSsv3REREYkdJqW7IkxKpK5VCyGuhuHB7i68dlTEKq2GlsLaQgpqCWHVRREREJGr6HjoSgPySNGYOy2JxY1Kq7cXOIzWlNFNKREQkdpSU6oYixc59jhTq6yxUFGxu8bUeu4dh6cMA+GL7F7HonoiIiEhUpYyfTJK1mBB2JgeKGutKeVesIFhe3qY2tfueiIhI7Ckp1Q01Fjt3pBCotVK7c2urrj9pwEkAvLTmJUzTjHr/RERERKLJsLvo26MUgPp1eXh69WBLUg6YJjXfzW9Tmyp0LiIiEntKSnVDCQ3L93zOFAJeK/Vl21p1/WmDT8NldbGubB0/FP0Qiy6KiIiIRFXfMT0B2LrNwQmjejTOlmrrEr4EewIA+TX50emgiIiI7EFJqW4ooXGmVDJm0MBS1LqZUinOFE4a2DBbavVLUe+fiIiISLT1mXokFgJU+DOZnlrLouyhAFS3MSk1pc8UAD7b9hnegDda3RQREZHdKCnVDUVmSnldaQA4d+a1uo3zhp8HwMdbPqa4tjh6nRMRERGJAUdaJj2Swpu0uNaupqD/CAKGhcC2bfi3t3zTl4ixWWPpkdCDmvoavsz7MtrdFREREZSU6pYaa0q5UgBwVbQ+qTQ8fTjjssYRMAO8uu7VqPZPREREJBb6Dgl/MLdtbS1Tx/VndXo/oG1L+CyGhZn9ZwLw3qb3otdJERERaaSkVDcU2X3Pb03ExCChuqJN7URmS7265lUV+RQREZEDXt8jxgOQV9GbEwc6GutKVX/9TZvaO2HACUB4R+Ka+prodFJEREQaKSnVDbmTHBgGYFjwO5Jw19QRCgZb3c70ftPJcGVQVFfEp1s/jX5HRURERKIoc+QQ3LZq6k03fbYuYX3uCAAqv/4GMxRqdXsj0kfQP7k/vqBPYyEREZEYUFKqG7JYDNzJkWLnKYS8BqXFrd85xm61c9bQswAVPBcREZEDn2ExyO1dB8D2pXn0nzyRWpsTS2UF3lWrWt+eYTTOlvpg8wdR7auIiIgoKdVtRYqd+5wpBOoslBdsblM7Zw89G6thZUHhAtaVrYtiD0VERESir+/4cB2prTuSOXFMFkszBwFQ/VXbduGL1JX6Ou9rKnxtK4kgIiIizVNSqptKaCh27nMkE6izUlO8tU3t9EjowdG5RwMwZ82cqPVPREREJBZyJ4XrSu2s789470pW9BwOQOF/57WpvYGpAxmWNoyAGWDulrlR66eIiIgoKdVteSLFzh2pBGqt+EpbvxVyRKTg+dsb3qbaXx2V/omIiIjEgifFSVZKOQAF85fjmTQp/MKyxYS83ja12biEb5OW8ImIiESTklLdVEJDTSmfM5mAzwLl29rc1qE9DmVgykBqA7W8teGtaHVRREREJCb6Dk8BYMuGAIdPHcdOVzLWQD21Cxe2qb2ZA8JL+OYXzKe4tjhq/RQREYl3Skp1UwkNM6V8jlQwDWzFbU9KGYbROFvqpTUvYZpmNLooIiIiEhN9J40BYFvNMI5O3s7SnGEAbJ37RZva653Ym4OyDsLE5KMtH0WtnyIiIvFOSaluyhMpdO5JA8BdVtiu9k4ZeAoem4dNFZv4ruC7dvdPREREJFZyhmRit/rxmilULfgG39gJAFR++WWb2zxxwIkAvL/p/aj0UURERJSU6rYihc79jvD0dU9VebvaS3QkcsqgUwB4afVL7WpLREREJJasVgt9+oZndm9dXsTgE6YBkLR9E4Gysja1Ob3fdCyGhSXFS8irzotWV0VEROKaklLdVELDTCm/NQETA3dNNQQD7WrzvGHhJXz/3fZfCmoK2t1HERERkVjpO2EQAFtLezNthIfNyT2xYLLxrQ/b1F6WJ4uJORMBFTwXERGJFiWluil3kh3DAAwLfkcS1EHNvL+3q83BaYOZ2GMiITPEy2tejk5HRURERGKg77hcAArqh+Fc+wnbDgrvwlf4+httbjOyC5+W8ImIiESHklLdlMVqwZ28awlfoM6KY9597dqFD3bNlnpt3Wv4g/5291NEREQkFpIz3aQm+zCxsn3hanLPPgOA9LVL8RcWtanN4/oeh82wsaZsDRvLN0azuyIiInFJSaluLLKEz+dIpqQ2CXuwDt77P2jH7nlH9z2abE82pd5S7T4jIiIiB7S+o7IA2LrdxdGT+rM2vR8W02TF7Nfb1F6qK5Ujeh8BwPubNVtKRESkvZSU6sYixc59zhSKalPxm1ZY+z6servNbdotds4eejagguciIiJyYOs7YQAAW73jcG78hNJJRwNQ/c47bW7zhP7hJXwfbPoAsx0f9ImIiEgnJ6W++OILTjnlFHr16oVhGLzxxhv7veazzz7j4IMPxul0MnjwYJ555pmY97Or8kSKnTtSSPHX8VgwvHse7/8GvBVtbvdHQ3+EzWJjSfESVpWsikZXRUREpBU0hmqZXkNTsVpCVIeyKV/0JSPOP4ugYSEzbwNV69u2/O6YvsfgtDrZXLmZ1aWro9xjERGR+NKpSamamhrGjh3L3//esgLcmzZt4qSTTuLoo49m8eLFXHvttVxyySV8+GHbdlHp7nafKZXgq+X56mMptPeGqh3w6d1tbjfTncnxfY8H4KU1mi0lIiLS0TSGahm7w0qv/uHx0NY11Rw6pg8rew0HYOkzc9rUZoI9gaP6HAWo4LmIiEh7dWpSaubMmdx9992cccYZLTr/scceY8CAAfzpT39ixIgRXHXVVfzoRz/ioYceinFPu6bITKlARh8AJu5Yw43en4ZfnP8kbF/Q5rbPGx4ueP7exveo8LV91pWIiIi0nsZQLZc7rh8AW2tHYdnyJYGjwx+s8cmHbV5+d+KAEwH4YPMHhMxQVPopIiISj2yd3YHW+OabbzjuuOOaHJsxYwbXXnvtXq/x+Xz4fL7G55WVlQCYphmTOgCRdg+EGgORmVL1ab0AOL5wGdcOuIztA06lz9a3MN++Gn7xGVjtrW57XNY4hqYNZW3ZWv6z7j9cOPLCAybujnIgvdcdJR5jhviMWzHHj3iMu7Uxd4efTTyPoXJHpsPrG8j3j6J+xftMnPUbql98lPSyQnZ89wM9Dzu41W1O7jWZBHsCO2p2sLhoMeOzx7epb/r7ix/xGLdijh/xGLdibtn5LdGlklIFBQXk5OQ0OZaTk0NlZSV1dXW43e49rrnvvvu444479jju9XpxOBwx6afP58MwjJi03Ro2V/jRa/EAMLRwPWneSv5qvYj7XfMwCldQ/+VfCRz2yza1/6OBP+LehfcyZ80czh54Nn6f/4CIuyMdKO91R4rHmCE+41bM8SMe425NzF6vN8a9ib14HkN50q14EkLU1jjJX7qBPtMTeWPAOMat/56Vz84hbezINrV7VK+jeH/L+7yz/h1GJI9oc//09xc/4jFuxRw/4jFuxbxvLR0/damkVFvcfPPNXHfddY3PKysryc3NxeVy4XK5on6/SObQ6XR2+i9oWnb4/t7aEM6DxuJbuoQj85fybso07j7lDuzvXI3tqz9iG3sWpPZrdfunDj2Vvy79K9urt7OwZCGHZBxyQMTdUQ6k97qjxGPMEJ9xK+b4iBniM+7Wxuz3+zugVwee7jSG6jsmh9XfFrOtvB99S1biOvFE+Mv3JH/zOU6bDcPW+iHxyYNP5v0t7/PJ9k+4+fCbsVla34b+/uIjZojPuBVzfMQM8Rm3Yo7e+KlLJaV69OhBYWFhk2OFhYUkJyc3+wkfgNPpxOl07nHcMIyY/fJE2u7sX05PsgMMMEMmjuNOxLd0CccWLOPtgUcyL2EGx/Q7EmPLl/De/8H5L0Mr+5vgSOC0wafx/KrnmbNmDhMnTzwg4u5IB8p73ZHiMWaIz7gVc/yIx7hbE3N3+LnE+xiqX0NSaqt/PEcufp4p59/DhsceJLmuknUffsawk49vdZuTek0i1ZlKqbeUBYULmNRrUpv6pr+/+BGPcSvm+BGPcSvm/Z/bEp1a6Ly1Jk2axCeffNLk2Ny5c5k0qW2DgO7OYrXgSQpPr7ceMgWAocUbyair4P3lhXDKw2B1wLqPYMV/2nSPSMHzeXnzyKvOi0q/RUREJLrifQzVZ3gahgFlgb5ULfqUTDdsHHU4AJvntG0MZLfYOb5fOJn1weYPotZXERGReNKpSanq6moWL17M4sWLgfB2xYsXL2br1q1AeNr4hRde2Hj+5ZdfzsaNG/nNb37D6tWrefTRR3n55Zf59a9/3Rnd7xISUsOfcPrsSbgPPhjDNDkyfylzVxVSnzYIplwfPvGDm6CuvNXt90vuxxG9jsDE5LUNr0Wx5yIiIrI3GkO1jivBTo+BKQBsrDoIlr9G9pmnA5C1+BsCtXVtanfmgJkAzN0yF38wPpd5ioiItEenJqUWLFjA+PHjGT8+vGPJddddx/jx47ntttsA2LFjR+PgCmDAgAG8++67zJ07l7Fjx/KnP/2Jp556ihkzZnRK/7sCT8MOfLUVfpJPOAGAY3Yspby2nvmbSuHIX0PGEKguhE/2LGbaEucNC8+WenvT2/iCvv2cLSIiIu2lMVTrDZkYLvS+pm4q/PBvjjh1GkWeNNz1XhbNebtNbR6cfTDZ7myq/FV8nf91NLsrIiISFzq1ptS0adP2uU3gM8880+w1ixYtimGvupeElPBMqZoKH0kzZlB4330M3bmJzNpy3l++g8mDM8PL+J45CRb8E8b+GHIPbdU9jupzFD08PSioLWDulrmcMuiUGEQiIiIiERpDtd7gQ7L58uW1FAcGU7qlkPSy1RROnEr2529Q8uZbcPE5rW7TarEyvf90nl/1PO9vep9pudOi33EREZFurEvVlJLWi8yUqqnwY8/JxjNhAgBT8pfw4YpCQiET+h8J4y4IX/D2NRCsb9U9rBYrZw09C4CX17wcvc6LiIiIRIk70UHf0ZkArK2bCgv/zaAfh8cvvdYuprq4tE3tRpbw/Xfbf6kLtG0ZoIiISLxSUqqba5wpVR5eVpc0M7yE7+j8JRRX+fhha1n4xOl3gScDilbC139t9X3OHHwmVsPK4uLFrCldE53Oi4iIiETR0EPDS/jW1h2FueQVDj5iBNvSe2MPBVnw71fa1OaYzDH0TuxNXaCOz7d/Hs3uioiIdHtKSnVzCY01pcJJqeTp08FiYUjpVrJrSnl/eUH4RE86zLg3/P3nD0DpplbdJ8uTxdTeUwF4ZW3bBnUiIiIisTTgoEwcLitVoWx2VPfEWPkmNVOOA8D3wfttatMwjMbZUu9vbFsbIiIi8UpJqW4usvteTUV4RxhbVhaeiROB8BK+D5YX7KpJcdC5MGAqBLzw7nWwj1oVzTlrUHgK/Nsb3qamviZKEYiIiIhEh81hZdDB2QCsqZsGC//N2IvOIYRB3+1r2LG2dR/KRUSSUvPy5lHlr4pWd0VERLo9JaW6OU9yOClVW+kP148CkhuW8E3LX0JeeR3L8yrDJxsGnPwQWJ2w4VPY+Fmr7nVI9iH0T+5PbaCWdze+G7UYRERERKJl6GE9AFjvnUxg60IGZvrZ3HsIAIufaVttzCGpQxiUMoj6UD2fbv00an0VERHp7pSU6uY8KQ4cbhtmyKRoczj5lNSwhG9w2XZ61uzk/eU7dl2QMQjGnhf+fnXrEkuGYXD20LMBmLNmzj53BRIRERHpDL2HpJKY5sRvJrDFNwF++DeW48MznRyfzW1Tm7sv4Xt17auEzFDU+isiItKdKSnVzVksBrkj0gHYsqIEAFt6OgmHHwbAkXlLmy7hAxgWHlSx7sNWL+E7bdBpOK1O1patZUnxkvYHICIiIhJFhsVgyMRIwfOpsORFDrvgZOotVnqV5rHqqx/a1O6pg07FbXOzuHgxL65+MZpdFhER6baUlIoD/UaHk1Jbl5c0Hks6oWEJX94SNu6sYW1h9a4LBkwFmwvKt0Lx6lbdK9mZzAn9w23PWTOnnT0XERERib5hDUv4Nvsn4K2pJ73sW7YMHgfAmhdea1ObPRN7cv2E6wF4aOFDbKzYGJW+ioiIdGdKSsWBvqMyACjaWkVdVbjgedLxx4PVysCKPHpXF/NBZBc+AIcH+k8Jf7/2w1bf79xh5wLw0eaPKPOWta/zIiIiIlGW0TuRjD6JhEw7671HwMJnSDn1lPBr3/2XQCDYpnbPGXYOk3tNxhf0ccu8W6gP1Uez2yIiIt2OklJxICHFSWZuIpiwdWUpALa0NBImTQJgSt6SpnWlAIbOCD+2ISk1OnM0I9JH4A/5eXP9m+3qu4iIiEgsDDs0PFtqbd1U2DyPQ6ePpNbmJLOmjO/f/m+b2jQMgzsn30myI5kVJSt4cumT0eyyiIhIt6OkVJyIzJbastsSvsgufEflLWF1QRWbd9bsuiCSlNr2LdSWtupehmE0zpZ6ee3LKvYpIiIiB5whE3PAgB31I6kMZONa/TI7xoY/sMt/7Y02t5vtyebWw28F4ImlT7B85/JodFdERKRbUlIqTvRrSEptXVlCKBQuXp503HFgtzOgcge5VYV8sGK3JXypfSF7JJgh2ND6rY1nDphJkj2JbVXb+Db/26jEICIiIhItiWlO+gxLA2Ct9yhYPJvcs04GIHfpN1RX17W57RMGnMDM/jMJmkFunnczdYG2tyUiItKdKSkVJ3oMTMbhtuGrCVC0uRIAa0oKCUfsvoSvoOlFjUv4Pmj1/Tx2D6cMCtdmUMFzERERORBFCp6v8R2LWbOTMYN9lHtSSPLX8tWL77Sr7d8e/luy3dlsrtzMwwsfjkJvRUREuh8lpeKExWohd0R4F74tK3ZfwjcTCC/hW7KtnPzy3T7JG9KQlFo3F4KBVt/znGHnAPDZ9s8oqCnYz9kiIiIiHWvguCxsdgvl9T0oDgzCsvhZKg6fBkDVO+1LSqU4U7hz8p0AzF49m2/yv2lvd0VERLodJaXiSL/R4aTU1t3qSiUdcwyG3U6/qkL6Ve7gw92X8PWZCO408JbD9u9bfb9BqYOYkDOBkBni9XWvt7f7IiIiIlHlcNsYMDYTgDV1U2Hjfxl1engH4kHrF5Gfv7Nd7U/uPbmxzuatX91Kpb+yfR0WERHpZpSUiiORYudFW6uoq/IDYE1OJuHII4HwbKkmS/isNhh8XPj7NizhAxoHYq+tfU3bIouIiMgBZ2jDEr519ccSMi30sSxkZ1oOrmA93z7b/g/VrptwHf2S+1FYW8i9393b7vZERES6EyWl4khCipPM3EQwYevKXTvqJZ8YXsI3JW8J328qobjKt+uioeEd+lj3UZvueVzf40h3pVNUV8Tn2z5vc99FREREYiF3ZDruJDt19R62+cdhLH6BwLSGD+U+/hDTNNvVvsfu4Z4j78FiWHh347t8uPnDKPRaRESke1BSKs5EZktt2W0JX+LRR2M4HORWF9O/YgdzVxbuumDQMWBYoWgllG1p9f3sVjtnDjkTUMFzEREROfBYrRYGH5IDwJr66VBdwPijewIwdPsqVizf1O57jM0ayyVjLgHgrm/vori2uN1tioiIdAdKSsWZfg1Jqa0rSwiFwp/8WRMTSTgqXD8hvIRvx64LPOmQe1j4+zbOlvrR0B9hYPDtjm/ZUtn6xJaIiIhILA07NLyEb1PdIfhDLlKLP6Sg9yCsmHz94KP4A6F23+Pygy5nRPoIKnwV3Pb1be2egSUiItIdKCkVZ3oMTMbhtuGrCVC0eVexzV278C3mm/U7qajdrf7T0IZd+Na2bbp578TeTOkTTnq9vObltnVcREREJEay+yeRmuMhELSyyXcYrJ9Lnx+fCsCU+e/yz9/9td1JJLvVzr1H3ovD4uDLvC95dd2r0ei6iIhIl6akVJyxWC3kjgjvwrdlxW678E2bhuFy0aumhH5l2/l41W5L+CJJqU1fgL+mTfeNFDx/c8ObeAPetnVeREREJAYMw2DooQ1L+DgdzBBDBpdSc+b5ABz55hO8+Mjsdt9ncNpgrj74agAe/P5BtlVta3ebIiIiXZmSUnGo3+hwUmrrbnWlLAkJJE6dCjSzC1/WcEjtC0EfbGxbsfLJvSbTK6EXFb4KPtrStmWAIiIiIrEytGEJ3/bKftQE0+CH55hw183snDIdqxli5BP38+GLbduNeHc/GfkTDsk5hLpAHb/98rcEQ8F2tykiItJVKSkVhyLFzou2VlFb6W88njwzvNPeUXmL+WJtEdW+QPgFw9htF762LeGzWqz8aOiPABU8FxERkQNPSpabHgNTME2DdcHpULkdY8OnHPmPP7Fj5CE4QwHS772FBZ9+3677WAwL9xx5Dwn2BBYXL+b5Nc9HKQIREZGup11JKa9Xy7C6ooQUJ5m5iWDCtpW77cJ31FEYbjc9asvov3MLV7+4CG99w6d3QyJ1pT6CNtZUOGPIGdgsNpYWL2V16er2hiEiIiISVcMOD8+WWhM8MXzgh39j2Gwc9dzj5PcZSmJ9Hb7rf8WGZevadZ9eib246dCbAHhs+WN8mfdlu9oTERHpqlqdlAqFQtx111307t2bxMRENm7cCMCtt97K008/HfUOSmxEZkttWVHaeMzi8ZB09DQAjs5fyqeri7jw6flUeuuh/5Fg90BVPhQsa9M9M92ZHNf3OECzpUREROTAM/jgbCxWg50VyZTU58Ka96GqAFuCh8Nf+hcF6b1Ir6tg6yW/oHh7wf4b3IfTBp3GyQNPJmgGue7z61havDRKUYiIiHQdrU5K3X333TzzzDP84Q9/wOFwNB4fPXo0Tz31VFQ7J7HTb3Q4KbV1ZQmh0K6ZT0knhJfpnVq+iiSnlfmbSznv8W8p9how8OjwSW3chQ/gnGHnAPDuxnep9le3uR0RERGRaHMl2hvHSGvtPwYzCJ/cBaZJQmY6I5//FyUJafSoKGTxrJ9RW1HV5nsZhsEdk+7g8B6H4w14ufKTK9lYsTFaoYiIiHQJrU5KPfvsszzxxBPMmjULq9XaeHzs2LGsXq0lWV1FjwHJOD02fDUBijZXNh5PPOooLB4PluJCXjrMQWaik5U7Kjn7sa8p7T0tfNLathf5PCTnEAalDKIuUMc7G99pZxQiIiIi0RUpeL625nBM0wKLn4e5t4JpkjOwLzmPPU6lI4E+hZuY9+OfE/T62nwvu9XO/ZPuZ3TGaMp95Vw29zIKato3A0tERKQraXVSKi8vj8GDB+9xPBQKUV9fH5VOSexZrBb6DA/vwrdlxW678LlcJJ9yCgCeZx7nlUsPo0+am80ltVw0LzV8Ut5CqC5u030Nw+DsYWcD8MKqF6gP6XdGREREDhz9D8rA4bZRXWWQP/Ef4YNf/xW++CMAQyaOgfv/jNfqoO/GZXz6s19hhkJtvp/H7uFvx/6N/sn9Kagp4PK5l1Phq4hGKCIiIge8VielRo4cybx58/Y4/uqrrzJ+/PiodEo6Rr/R4aTU1uUlTY5nXXUlFo8H79KlpH/3Ga9efgRDshNZVpXAKgYAJqyf2+b7njboNNKcaWyu3Myra19tTwgiIiIiUWWzWxl8cBYAa8vGwox7wy/892749jEADjvxKApu+D0Bw0KfH+Yx79e/xWzjRjAA6a50Hj/+cbLd2Wyo2MBVn1xFXaCu3bGIiIgc6FqdlLrtttu46qqreOCBBwiFQrz++uv84he/4J577uG2226LRR8lRiLFzou2VFFb6W88bsvKIuPSS8Ov/enPZDtMXr5sEmNzU/koMA6AnT+81eb7JjoSuXLclQA8uvhRKv2V+7lCREREpOMMPSy8hG/9D8UEDrkcpt0cfuGDG2HR8wDMvPgMVlxwDQBZH77Bwvseadc9eyX24rHjHyPJkcTi4sXc8PkNmlEuIiLdXquTUqeddhpvv/02H3/8MQkJCdx2222sWrWKt99+m+OPPz4WfZQYSUhxkpmbCMC2lU1nS6X/9CJsvXoSKCig9JlnSEtwMPuSw9jZaxoAzi2f8eHSLW2+91lDz2JgykDKfeU8ufTJNrcjIiLSlQQCAT7++GMef/xxqqrCRbLz8/OprtbmHweSXoNTSUx34q8LsOGHYph6I0y6KvziW7+CFf8B4NxbfsHXM34CQMKzj7P6ny+0675D0obwt2P+htPq5IvtX/D7r3/frhlYIiIiB7pWJaUCgQB33nknAwYMYO7cuRQVFVFbW8uXX37J9OnTY9VHiaF+DbOltqwobXLc4nKRfd31AOx88inqC4tIcNr43S9mUWlNI8mo49mX5vDy99vadF+bxcb1h4Tbf2HVC2yrals7IiIiXcWWLVsYM2YMp512GldeeSXFxeH6jA888AA33HBDJ/dOdmdYDIZP6gnAZ7PXsGNDBUy/Gw6+EMwQvPYLWDcXwzC44E83Mm/iiQAEH7yHba+/3a57H5xzMH+c+keshpW3NrzFQwsfanc8IiIiB6pWJaVsNht/+MMfCAQCseqPdLC+Ddseb11ZQijU9JO45JNOxD12LGZtLcWPhKekO+12EkfPBGCaZRG/eW0pT3yxoU33ntJ7CpN6TqI+VM/DCx9uexAiIiJdwDXXXMMhhxxCWVkZbre78fgZZ5zBJ5980ok9k+ZMOKEfuSPTCfiCvP23JRRuqYKTH4ZRZ0KoHuZcAJu/wmmzcvZj9/L10COwmCbVt/yGH359E6Ha2jbfe1ruNG6fdDsA/1rxL/694t9RikpEROTA0urle8ceeyyff/55LPoinaDHgGScHhu+mgBFm5vWdjIMg5ybbwKg4j//oW7FCgAsQ2cAcGbicgDufW81D364ptXTyw3D4IaJN2AxLHy05SMWFS1qbzgiIiIHrHnz5vG73/0Oh8PR5Hj//v3Jy8vrpF7J3tjsVmZePobeQ1Op9wZ5+y+LKc6rhTOfgCEzIOCF2edC3g+kJTg55ok/89HIYwBwv/8m86efTPH3bR/bnDHkDK49+FoA/rjgj7y1oe31PEVERA5UrU5KzZw5k5tuuokbbriBF198kbfeeqvJl3QtFquFPsPDu/BtWVGyx+vuceNIPukkME2KHvhDOPE06Biw2MjwbuXeozwAPPrZBp74svU1poamDeWMwWcA8OD3DxIy276lsoiIyIEsFAoRDAb3OL59+3aSkpI6oUeyP3aHlRN/eRA9Bqbgqw3w1iOLKSn0wTn/hv5TwF8Fz58JhSsZ0COFn7/4CP+95FZ2ulJI2bmDwgsv4Jvb/4DZxlUGPxv9M34yMlyz6ravbuOL7V9EMzwREZFO1+qk1C9/+UsKCwv585//zKxZszj99NMbv84444xY9FFirN/ocFJq6/I9k1IA2ddfh+F0Ujt/PtWffAKuZOg3GYDz01Zxx6mjAHj0881s3lnT6vtfNf4qPDYPy3Yu44NNH7QxChERkQPb9OnTefjhhxufG4ZBdXU1t99+OyeeeGLndUz2yeGycfKvxpLdLwlvdT1vPbyY8lITfvwi9J4AdWXw3OlQupFEp41f3nA+rudeYsHACVjNEKlz/sV/Z5zBjlXrWn1vwzC44ZAbOHngyQTNIDd8fgNLipdEP0gREZFO0uqkVCgU2utXc5/+yYGvb0Ox86ItVdRW+vd43d6rF+k//SkAhQ8+iOn3Q8MSPtZ+wIWT+jF5cAb+YIjb31rR6mV8me5Mfj7m5wA8/MPDeAPetgcjIiJygPrjH//IV199xciRI/F6vZx//vmNS/ceeOCBzu6e7IPTbeOUq8eR0SeR2ko/bzy0iIpKG8x6FbJHQXUhPHsaVISXYU4c058fvfEMi35yLdV2Fz3z1lNw9o/4+I9PEAq1bla4xbBw5+Q7mdx7MnWBOq785Eq+zvs6FmGKiIh0uFYnpaT7SUhxkpmbCMC2lc3Plsr4xS+wZmVSv2UrpS/MhqEnhF/Y8hWGr4o7Tx2F3WrwxbqdvLesoNV9uHDkhfRI6MGOmh08v+r5NsciIiJyoMrNzWXJkiX89re/5de//jXjx4/n/vvvZ9GiRWRnZ3d292Q/XAl2TrtmHGk9E6gp9/Hmw4uo8nrgJ/+B9IFQvjU8Y6o6vKui22Hj/N9ehufZl1jfayiugJ/eTz3E26dewJb121t1b7vFzp+n/pmDsg6iwlfBZR9fxu+//j3V/uoYRCoiItJx2pSU+vzzzznllFMYPHgwgwcP5tRTT2XevHnR7pt0oH4Ns6W2rCht9nVrYgLZ11wDwM5HHyVgSYf0QRAKwMb/MjArkV9M7gfAne+soMpb36r7u2wurh5/NQBPLn2SnXU72xqKiIjIAae+vp5Bgwaxbt06Zs2axR/+8AceffRRLrnkkiY78cmBzZ3k4LRrx5GS5aaqxMubDy2iJpQKF74JyX1g51p44SzwVTVeM2r8MGZ8+BobzvwZ9RYrQ9cvIv+sM3jj0Tl77Hy8Lx67hyePf5JZI2YB8Nq61zjzrTP5Ol+zpkREpOtqdVLq+eef57jjjsPj8XD11Vdz9dVX43a7OfbYY5k9e3Ys+igdoO/ocFJq68qSvQ6QUs44A+fw4YSqqtj5t7/vmi219kMALp3Sj37pHgorfTw0t/V1E04aeBKjMkZRG6jl0cWPti0QERGRA5Ddbsfr1fL07iAhxclpvx5PUoaLiuI63nxoEbWWHuHElCcTdiyBl2ZBwNd4jcNu4+R7/w/3089RmNGbVF81w/7ye54770o2bG35B3Eeu4ebDr2Jf874J30S+7CjZgeXzb2MO7+5k5r61tf1FBER6WytTkrdc889/OEPf2DOnDmNSak5c+Zw//33c9ddd8Wij9IBegxIxumx4asJULS5stlzDKuVnJtuBKDspZfwuQ4Kv7DuIzBDOG1W7jgtXPT8ma83sSK/olV9sBgW/m/i/wHhT//WlbU+sSUiInKguvLKK3nggQcItHEnNjlwJKW7OP3X40lMc1JWUMtbjyzG6+4Hs14BewJs+hz+czn8T/2oIZPGc+TH71Aw40wADlv2GUXn/5jnHnkRb33La7NO7DGR1059jfOGnQfAK2tf4cw3z+TbHd9GLUYREZGO0Oqk1MaNGznllFP2OH7qqaeyadOmqHRKOp7FaqHP8PAufFv2sgsfQMLhh5N4zDEQDFL4wqfgSIKaYshfBMDUoVmcNKYnIRN+98byVk1LB5iQM4Hj+h5HyAzxpwV/antAIiIiB5jvv/+e119/nb59+zJjxgzOPPPMJl/StSRnujnt2vF4kh2U5FXz1iOL8aUfBOc+BxYbrHgdPrwF/mcDGJvbxdGP3IPnb49RmZROz5oSJj52F/856Xy+/nJZi+/vsXv47eG/5enpT9M7sTf5Nfn84qNfcNc3d2nWlIiIdBmtTkrl5ubyySef7HH8448/Jjc3Nyqdks7Rb3Q4KbV1xd6TUgDZ/3cD2GzUzJtHNRPCB9d+0Pj6rSePJMFhZdHWcl76flur+/HrCb/GZrHxVf5XfJn3ZauvFxERORClpqZy1llnMWPGDHr16kVKSkqTL+l6UnM8nHbteFyJdoq3VvHOXxfj7zMVTv9H+ITv/gFfPdzstf2Om8qETz+g5MSzCBgWxm1diufS83n28lsp2tn8rPXmHNrzUF4/9XXOHXYuAC+vfZmz3jqL+Tvmtzc8ERGRmGt1Uur666/n6quv5oorruC5557jueee4/LLL+faa6/lhhtuaHUH/v73v9O/f39cLheHHXYY8+fv+x/Qhx9+mGHDhuF2u8nNzeXXv/61ajRESd+GYudFW6qorfTv9TzngAGkzzo/fO6nZZghwkv4GvRIcfHr44cC8MAHqymp9jXXzN77kdyX84eH2//Tgj8RCGmZg4iIdH3/+te/9vnVWhpDHRjSeyVw2rXjcHpsFGys5NUHFrLNfhzMuDd8wse/h0UvNHutNTGRCff8ll6vvErhoNE4QwEmfvYqK2acyNuPvdziGeceu4ffHf47npr+FL0SepFXncfPP/o5d397N7X1tVGKVEREJPpanZS64ooreOmll1i2bBnXXnst1157LcuXL2fOnDlcdtllrWprzpw5XHfdddx+++388MMPjB07lhkzZlBUVNTs+bNnz+amm27i9ttvZ9WqVTz99NPMmTOHW265pbVhSDMSUpxk5iYCsG3lvmdLZf7yl1hTUvBtL6Z8YwLGjiVQtaPx9Z8e0Z8RPZOpqKvnvvdXt7ovlx50KSnOFNaXr+f1da+3+noREZEDVXFxMV9++SVffvklxcXFbWpDY6gDS2afJE69ZhyuRDtlO2p465HFvLd4CuUHhWtx8tavGjeGaU76qOFMfedlAr+9i/KEVHrUlDD44dt548TzWLlgVYv7cVjPw3j9tNc5Z+g5AMxZM4cz3zqTdze+S32wdTsji4iIdATDNM3WFf2JosMOO4yJEyfyt7/9DYBQKERubi6/+tWvuOmmm/Y4/6qrrmLVqlVNlg9ef/31fPfdd3z5ZcuWeVVWVpKSkkJFRQXJycnRCWQ3pmni9XpxuVwYhhH19mPt2zc2sPCDLQyZmMP0n4/a57mlzz5H4b33YnVbGDQzD6PHQAx3GoZhAYuVKl+Q5fnVhDAY3SeNFI8TDEv4y2KDg86GUWfstf0XVr3A/fPvJ92VzrtnvEuiIzHa4bZLV3+v2yIeY4b4jFsxx0fMEJ9xtzbmaI0dampq+NWvfsWzzz5LqKEAttVq5cILL+Svf/0rHo+nxW1pDHVg8tbU8/27m1j2WR5myMRiNRibu5JDau/C4QAuehtyJzae31zM9VXVfHHrA2R/+Do2M4TfYmPDsWdw/N2/ISml5WOhb/K/4favb2dHTfhDwwxXBj8a+iPOHno2OQk5UY27NbrD+9wW8Ri3Yo6PmCE+41bM0Rs/2Vrbke+//55QKMRhhx3W5Ph3332H1WrlkEMOaVE7fr+fhQsXcvPNNzces1gsHHfccXzzzTfNXnPEEUfw/PPPM3/+fA499FA2btzIe++9x09+8pO93sfn8+Hz7Vo+VlkZXqNvmiaxyMdF2u3EXF+79B2VzsIPtrB1ZQnBYAiLZe+/bKnnnUvZiy/i37SJkpWJZDs2NHk9CZhkbXiyY4/LMTd8Aj3HQVr/Zts/e+jZvLj6RbZUbuGpZU9xzcHXtCmmWOnq73VbxGPMEJ9xK+b4EY9xtzbmaP1srrvuOj7//HPefvttJk+eDMCXX37J1VdfzfXXX88//vGPFrWjMdSBy+mxceTZQxh5ZC++enU921aWsmjzCFbbn+Jw9z8Z9sI5WH7+AWSGyxw0F7MtMYFjHrqTvCU/ZsXNt9N303JGzH2FRd98Cldey5EXndmi/xg4vOfhvHbKa7yw+gVeWfMKRXVFPL70cZ5a9hTH9j2W84afx4TsCR3+H1Pd4X1ui3iMWzHHj3iMWzG37PyWaHVS6sorr+Q3v/nNHkmpvLw8HnjgAb777rsWtbNz506CwSA5OU0/qcnJyWH16uaXe51//vns3LmTI488EtM0CQQCXH755fucen7fffdxxx137HHc6/XicDha1NfW8vl8XTZbmtLTicNtxVcTYNOyQnoPS93n+anXXkPRNddSsj4N+2W34+mTDaEgmCEwQ9T66vnDh2uo9dVz0ugspg1OAzOEdemLWPO+J/j+zfjP3HsdjV+N+RU3fHUDz618jlP7nUrPhJ5Rjrh9uvJ73VbxGDPEZ9yKOX7EY9ytiTladZdee+01Xn31VaZNm9Z47MQTT8TtdnPOOee0OCmlMdSBz5Nm5bhLhrJ9ZTnfvbmZymL4b/1VLK/dwBGP30DGpY9AUnhMs7eYM4YNYMpr/+b75/6D9Ym/k1VdAg/cygcvv0yPX1zM8BlHYVj2XYnDho2Lhl7ErMGz+CzvM15d/yo/FP/AR1s+4qMtHzEoZRBnDz6bE/qegMfe8pl67dVd3ufWise4FXP8iMe4FfO+tXT81Orle4mJiSxdupSBAwc2Ob5p0yYOOuggqqqqWtROfn4+vXv35uuvv2bSpEmNx3/zm9/w+eefN5vc+uyzzzjvvPO4++67Oeyww1i/fj3XXHMNv/jFL7j11lubvU9zn/Ll5uZSXl6uqed78fmLa1jxRT4pWW7O+d1E7A7rXs81TZNtP7+E2m++wXXoofT959NYrE3Pf3Xhdv7v1aW47VY++vVR9ElzQ/FqeOxIjFAAc9ZrMPjYvbZ/yUeX8H3h98zsP5MHjnogqrG2R3d4r1srHmOG+IxbMcdHzBCfcbdl+nlqamq7l615PB4WLlzIiBEjmhxfsWIFhx56KDU1NS1qR2OoriUYCLHss+0seHcTfm942ebglKVMuvrHJPbMalHMNeVVfHLrAwz45E3sZhCAkpRsLCefxsGXXoAzO6vF/VlbtpY5a+bw9sa38QbC/8GQZE/itMGnce6wc+mX3K8d0e5fd32f9yce41bM8REzxGfcijl646dWJ6UyMjJ45513mgyCAL7++mtOOukkysrKWtSO3+/H4/Hw6quvcvrppzcev+iiiygvL+fNN9/c45opU6Zw+OGH8+CDDzYee/7557n00kuprq7Gsp9Pi0D1EFrCV1vPi3fOp6bcx9hjcjnynCH7Pn/jJjaddRZmXR1Z111H5qW/aPK6aZqc+/i3zN9cyvEjc3jywoYlnh/cAt/+HTIGwxXfgK35T11XlqzkvHfOw8TkT1P/xPT+06MSZ3t1h/e6teIxZojPuBVzfMQM8Rl3Z9WUOvbYY8nIyODZZ5/F5XIBUFdXx0UXXURpaSkff/xxi9rRGKprqq30892rS1k5vxywYDXqGT+9PyOO7k1SSkKLYl6zcAXLHnqMAYvn4QmEE4ZBw0LZwUcw/JKfkDn1yP3Onoqo9Ffy5vo3eWn1S2yt2tp4fHKvyZw19Cym9J6Cy+ZqS6j71N3f572Jx7gVc3zEDPEZt2KO3vip1bvvTZ8+nZtvvpmKiorGY+Xl5dxyyy0cf/zxLW7H4XAwYcKEJgU3Q6EQn3zyyR4Jr4ja2to9Bk3Whlk58bSWM9acHjtHXzAcgCX/3Ub+uvJ9nz9wADkN0/+L//IX6pYsafK6YRjcfcZobBaDuSsL+XhlYfiFaTdCQjaUrIdvH91r+yMzRnLRqIsAuPWrW9lQvmGv54qIiByoHnnkEb766iv69OnDsccey7HHHktubi5ff/01jzzySIvb0Riqa/IkOzj6Z4dwzmUZ9HKuJmjaWfBhHq/c/QPfvbmRqtL9L3MYNmEUP3r+r/T8+FMWn3cl6zL6YTVDZC78kp1XXMaCI49m/UN/I9CCXR2THcn8ZORPePuMt/nHcf/gqD5HYWDwVf5XXPfZdUydM5Ubv7iRT7d+ii/o2297IiIibdHqmVJ5eXkcddRRlJSUMH78eAAWL15MTk4Oc+fOJTc3t8VtzZkzh4suuojHH3+cQw89lIcffpiXX36Z1atXk5OTw4UXXkjv3r257777APj973/Pn//8Z5544onGqedXXHEFEyZMYM6cOS26pz7la7lPnl3F6q93kJLl5txbD93nMr5QKMS2a6+l9qO52Pv0YcB/XsealNTknPveX8Xjn2+kd6qbudcdhcdhg8UvwhuXgz0BfrUAkns1234gFOCyuZcxv2A+/ZP7M/uk2SQ5kpo9t6N0p/e6peIxZojPuBVzfMQM8Rl3Z82UgnBy6IUXXmis/TRixAhmzZqF2+1uVTsaQ3Vt5qZ5bHzyAb6qmEVVMFwbzDBgwNgsRk/tTZ/haS36GdT5g7z/5hcUvfgyE9Z+S2LDcrygxUro8CMZcPEsEidPbvHsqW1V23hl7St8uOlD8mvyG48n2hM5OvdoThhwApN6TsJutbch6rB4ep93F49xK+b4iBniM27FHL3xU6uTUhDe0viFF15gyZIluN1uDjroIH784x9jt7f+H6i//e1vPPjggxQUFDBu3Dj+8pe/NBZRnzZtGv379+eZZ54BIBAIcM899/Dcc8+Rl5dHVlYWp5xyCvfccw+pqaktup8GVC3XmmV8pmlSU1zMjvN+TCA/n+STTqLXHx9s8jOo9Qc4/s9fkFdexxXTBnHjCcMhFIJ/nQDbvoPRP4IfPb3Xe5TUlXDuO+dSWFvIMbnH8NDRD2ExWj3ZL2q603vdUvEYM8Rn3Io5PmKG+Iy7M5NS0aQxVBe38k0Cc37GZt+hLPOdRn7d0MaXUnM8jJ7am+GTeuJ0739folDI5Ivl2/j+mVcZ8O1cRpVubnzNn9WDzJkzSD3maDwHj8doQZF60zRZtnMZH2z+gA83f0hRbVHja0mOJI7teywz+s/gsJ6HYbe0bvwfd+9zg3iMWzHHR8wQn3Er5k5OSnVlGlC1zpblJbzztyVgwBnXHUyvIanNnheJO7RqFVt/ciEEg/S87z5Szzi9yXkfrSjg0ucWYrMYvH/NFIbkJEH+YnhiGmDCT9+D/pP32p9lxcu46IOLqA/Vc83B13DJmEuiFWqrdbf3uiXiMWaIz7gVc3zEDPEZd2clpe677z5ycnL42c9+1uT4P//5T4qLi7nxxhvb3HZH0Bgq+sw1H2C+dwOWim2U1Oey3Hoxa8oPpt4fHp7bnFaGHZrDmGl9yOid2KI21xVW8fprX2C89ybTtiwgqb6u8bWgy4Nz0iSyjjuGxKOmYMvaf4H0kBliSfESPtz8IR9t/ojiul1LA1OcKRzX9ziO6XsM47LHkezY/+9FPL7PEJ9xK+b4iBniM27F3AlJqbVr11JeXs6hhx7aeOyTTz7h7rvvpqamhtNPP32f2wofKDSgar1Pn13Fqv0s49s97pLHH6f44UcwPB4GvPYqzgEDmpx7yb+/5+NVRUzol8acSw/HZrXAO7+GBf+E7FFw2Rdg3fungq+ufZU7vrkDA4PHjnuMI3ofEfWYW6I7vtf7E48xQ3zGrZjjI2aIz7g7KynVv39/Zs+ezRFHNP1367vvvuO8885j06ZNbW67I2gMFX2maeKtKsO16CmMeX+GQB1+M4E1WTeyrPgQygp31XLqOTiFMVP7MHB8Flbb/meKl9b4mfPlOlb/530GbVjCIUWrSfNVNzknNGQ4mccdTfK0qbjGjNnvMr9gKMiiokV8sPkD5m6ZS6m3tPE1A4NBqYMYlz2O8dnjGZc1jtyk3D3ey3h8nyE+41bM8REzxGfcirkTklJnnHEGY8aM4c477wRg06ZNjBo1iilTpjB8+HD++c9/ctddd3Httde2LKJOogFV6/nqArx053dUl+19Gd/ucRMKsfXin1E7fz6ukSPp99KLWHabKr6ttJaZj8yj2hfgV8cM5vrpw6C2FP56MNSVwcw/wGGX7bNPv//697y27jVSnCm8dNJL9EnqE/W496c7vtf7E48xQ3zGrZjjI2aIz7g7KynlcrlYtWoVA/7nw5qNGzcycuRIvN79F7ruTBpDRV+TmCvzYO5tsPy18GvOFPJH3MOywrFsXFKCGQoP2d3JDoYd1oMRk3qS3iuhRfdYV1TNJysKWP3FfJIWf8fEglUMLd/e5LxgcipJR00h7ZhpJBx1FNbEfc/MCoaCLChcwIebP+S7Hd812cEvIt2VzrishiRV9jhGZozEbrHH3fsM+v1WzN1bPMatmDshKZWbm8vLL7/cuKvL3XffzauvvsrixYsBePrpp/nrX//a+PxApQFV2+xvGd//xl1fWMimU08jWFFB+sUXk3Pjb5qc/+biPK55aTGGAc///DAmD84Mz5R659fgSoGrFkLi3qeU+4I+fvr+T1lespzh6cN5buZzMdm2eF+663u9L/EYM8Rn3Io5PmKG+Iy7s5JSQ4YM4fbbb+eCCy5ocvy5557j9ttvZ+PGjW1uuyNoDBV9zca8+St4/0YoXBZ+nj2S6sn3sWJrX1bOy6e20t94fXa/JEYc0ZPBh+TgSmhZbafSGj+fry3i6+/XUfflPA7avoKDi9aSENiVFA06nHhmzKDHj8/DPX5ci96PnXU7WVK8hMVFi1lctJgVJSuoD9U3OcdusTMqYxSj00czLmccozJH0Tuxd1y83/r9VszdWTzGrZg7ISnldrtZu3Zt4+56xx57LEcccQR33XUXABs2bGDChAmUl5e3pLlOowFV2+1rGV9zcVd9+inbf3klALlPPkHilClN2rvx1aXMWbCNrCQn7109hawEGzx5NOxYAuN/Aqf9bZ/9Kagp4Nx3zqXUW8qpg07l7sl3d+jPvDu/13sTjzFDfMatmOMjZojPuDsrKfWHP/yBP/zhDzz44IMcc8wxQLgUwm9+8xuuv/56br755ja33RE0hoq+vcYcCsLCZ+DTu6GuYYnciFMJHnsXW7YnsPqbHWxZVkKoYfaUxWYwcGwWwyf1JHdkOhZLy35+9cEQCzaX8enyPLZ+/i191v7A4QUr6VO9q26Ut3c/cs47h+wfnYEtLa3FsfmCPlaVrGJR0aJwoqp4cZPlfhFpzjRGZo5kdMbocMIqczRZnv3Xuupq9PutmLuzeIxbMXdCUqp379785z//4dBDDyUUCpGWlsbs2bM56aSTAFi1ahWHH344FRUVLQypc2hA1Xa7L+M76Jg+TDln1y4xe4u74K67KXvhBawZGQx84z9NCmrW+YOc+rcvWVdUzZQhmfz74kOxbJ8P/5wePuGST6HPhH326bsd33Hp3EsJmSF+e9hvOW/4edENeh+683u9N/EYM8Rn3Io5PmKG+Iy7s5JSpmly00038Ze//AW/PzzbxeVyceONN3Lbbbe1ud2OojFU9O035tpS+Ox++P4pMINgc8Hka2DK9dTWGqydX8Dqb3ZQklfTeElCioNhh/dg+KSepPXY//K+3W0sruaTlYUs/Wgeg7//hKPyluAKhmc7Ba02vIdNYdDFs0idPGm/9aeai3Vb1TZ+KPqBRQWLWF2+mrVlawmEAnucm+3JbkxQjc4YzajMUaQ4U1p1vwONfr8Vc3cWj3Er5k5ISs2aNYvKykoeffRRXnnlFW6//XYKCgpISAj/Y/faa69x5513smTJkhaG1Dk0oGqfLStKeOevey7j21vcIZ+PzWefg2/tWhImTyb3ySeaDGLWFFRx6t++xBcIceMJw7li2iD4zxWwZDb0Ohgu+QT2M+h5Zvkz/Gnhn7AZNv51wr8Ylz0uFqHvobu/182Jx5ghPuNWzPERM8Rn3J2VlIqorq5m1apVuN1uhgwZgtPpbHebHUFjqOhrccyFK+GDG2HTF+HnOWPgrCchewSmabJzWzWrvtnB2vkF+Gp2JXlyBiQzfFJPhk7MweHe+yYyzdm0s4b3vllH0RtvcsjyeQyuyGt8rTItG+tJpzH6klm4euS0Oeb6UD1ry9ayfOdylu9czoqSFWys2EjIDO1xXbYnm0EpgxiUOogBKQMYlDqIQSmDSHWltur+nUW/34q5O4vHuBVzJySlNm/ezPHHH8+GDRuwWq385S9/4Yorrmh8/fTTT2fAgAE89NBDLWmu02hA1X6RZXzJWW7Oa1jGt6+4fevXs+lHZ2N6vWT/3/+R8fOmW2G/OH8rN7++DKvF4OXLJjEh3Q9/nQD+Kjj1r3Dwhfvsj2ma3PD5DXy05SOy3Fm8fMrLZLozox53c/ft7u/1/4rHmCE+41bM8REzxGfcnZ2UitiyZQs1NTUMHz4cSytnnXQGjaGir1UxmyasfBPevR5qd4ZnTU2/GyZeAg3XButDbF6+k9Vf72DLitLG4ug2p5WhE3MYfVRvsvomtbqPK/Ir+eLdefDOmxy6cUFj/amgYWHHiAmkzTyBvmOGkjawL7bMzH3OotpfzLX1tawqXcWKnStYXrKcFTtXNFtEPSLdlc6g1EEMTBnIwJSB4WRV6iAyXBkH1O+Rfr8Vc3cWj3Er5k5ISgEEAgFWrFhBVlYWvXr1avLakiVL6NOnDxkZGS1trlNoQNV+zS3j21/cZS+/TMFtt4PNRv8XZ+MeM6bxNdM0+dWLi3hn6Q56p7p57+oppCx5Aj68BTwZ8KuF4N53DYPa+lrOf/d8NlRs4ODsg3lqxlPYLS0r+NlW8fBe/694jBniM27FHB8xQ3zG3dFJqX/+85+Ul5dz3XXXNR679NJLefrppwEYNmwYH374YWPdzgOVxlDR16aYqwrhzV/C+o/Dz4dMh9P+DonZTU6rqfCxdn4hq77Kp6ygtvF4dr8kRh3VmyGH5GB3WmmNUMhkwZp8lr3wOhmfv8+w4k17nBOw2vCmZWLk9CQhtxdpA/rh6tMbe6+e2Hv1wtqjB/5QqFUxV/mr2FixkY3lG9lQviH8fcVG8qrz9npNjieHI3sfyeTekzms52EkO6L/O9sa+v1WzN1ZPMatmDspKdUdaEAVHVtXlPD2bsv4eg5O2WfcpmmSd+2vqfrwQ+x9+zLg9dexJu6qc1Dlreekv3zJ1tJaThjVg3/8eAzG41OgeDUceimc+OB++7SpYhPnv3s+1fXVzBoxi5sOvSmqMf+veHmvdxePMUN8xq2Y4yNmiM+4Ozopdfjhh3PZZZdx8cUXA/DBBx9wyimn8MwzzzBixAiuuuoqRo4cyVNPPdXqtjuSxlDR1+aYTRPmPwEf3QpBH3gy4fRHYeiMZu+xY305y7/IZ8MPRYSC4aG/w21j2OE9GDWlFxm9Elvd9/pgiG8+XcCO2XNwb1hNalUJ6XWVWGnBf1qkpmJNSMDq8WBxu7G4XFjcbgy3O/zc7cJw7fa9x4OjTx+cgwdj69mz8WdVW1/LpspNbCwPJ6kiCattVduaLAG0GlbGZo1lcu/JTO49mRHpI7AYHTs7Ub/firk7i8e4FbOSUm2mAVX0fPrcKlZ9FV7Gd+5vJxI06/cZd7Cigo1nnEEgfwcpp51KrwceaPL60u3lnPWPr6kPmtx12ih+krMFnj0VDAtcNg96jN5/n7Z+yjX/vQaAe4+8l1MGndL+QPcint7riHiMGeIzbsUcHzFDfMbd0UmpjIwMPvvsM8Y0zBK+4oorKC4u5tVXXwXgs88+4+KLL2bTpj1nnRxINIaKvnbHXLgSXrsEilaEn0/8BUy/C+zuZk+vrfSz+psdrJiXR+VOb+PxnoNTGH1UbwaNz8Zqb1uypspbz9q8Mjas3ETBus1UbtmGP38HqZUlZNWVk11bRnZdWWPh9LayJCTgGDwI5+DBOAcPCT8OGYwtJ6fxZ+gNeFlYuJAv877kq/yv2FTR9G8r3ZXOEb2OYHLvyRzR6wjSXent6lNL6PdbMXdn8Ri3YlZSqs00oIqeJsv4ju7DIafm7jfu2h8WseUnP4FgkJ7330fq6ac3ef2peRu5+91VOGwW/vPLIxj15dWw8g3oewRc/F5jzYR9+euiv/LE0idwWV08PeNpDso6qJ2RNi+e3uuIeIwZ4jNuxRwfMUN8xt3RSSmPx8OqVavo168fAGPHjuXnP/85V199NQBbt25l2LBh1NXVtbrtjqQxVPRFJeZ6L3xyJ3z79/DzzGFw1lPQc+/jHzNksm11KSu+yGfT0p2NtadciXZGTOrJiMmt37mv2fuYJnnldazeUcWawipW5VewbUsBNXk7sNX7cQX9OAN+nMH68PdBP66G585g5PV6EgJeelcX06emGGtozyLoAJbERJyDBzcmrFzDh+MaMQJrSgp51Xl8lfcVX+Z9yXc7vqM2sGs5o4HBqIxRTOwxkf4p/emb1Jd+yf3IdGdG9fdQv9+KuTuLx7gVs5JSbaYBVXTtvozvxCtH0X9U9n7j3vmPf1D8yF/AbqfPX/9C0rRpja+Zpskl/17AJ6uLGJiZwDsX9sfz5CSor4Uzn4KDzt5vn4KhIFd+eiVf5X1Fkj2JJ6Y/wejM/c+yaq14e68hPmOG+IxbMcdHzBCfcXd0UmrEiBHcc889nHnmmezcuZMePXrw3XffMWHCBADmz5/PqaeeSkFBQavb7kgaQ0VfVGNe/wm8cQVUF4LVAcfeBodfud9djKvLfKz6Op+VX+ZTXeZrPJ7VN4khh+Qw+JBsktJd7evbbkzTpK6uDsPmoMYfpNoboNrX8OUNUOMPUOUNUOPbdTy/vI7/ri4mVO+nV/VOBtcWcayrhoMCpXh2bMG/ZSsEAs3ez96nD66RI3GNGoVr5Ehsw4ewLLg1PIsq7yvWlK1p9jq3zU3fpL70Te7bmKjKTcptc8JKv9+KuTuLx7gVs5JSbaYBVfRFlvElpjs573eH4vTsu8C4GQySd/0NVH3wAYbdTp9HHyVxypGNr5fW+DnxkXkUVHo58+De/LnHx/DpXZDYA65ZvNcp6burra/lio+v4IeiH0iyJ/Hk9CcZlTmqvaE2jSMO3+t4jBniM27FHB8xQ3zG3dFJqfvvv59HHnmEX/7yl3z66acUFxezfPnyxtcffvhh3nnnHT7++ONWt92RNIaKvqjHXFMCb/0K1rwbfj5gKpzxGCT32vd1QCgYYsvyElZ8mc/W3XbuA+g1JJUhE3MYdHAW7kRHu7rY1phLa/z8Z1EeL83fyrqi6sbjg7MT+fH4HpySHsCVvxXfuvX41q3Du3o19du2NduWrWfPcKJq5Ah8g3rzQ1oFS81tbKvaxpbKLeyo2dGkJtX/ctvc9E7sjdPqxGaxNfmyG/bwo8Xe9LjFjsfiITsxm3R3OumudDJcGaS70kl2Jnd4jauOoL/p+IgZ4jNuxdzJSany8nLmz59PUVERof+ZQnvhhRe2trkOpQFV9PnrArx093yqSrwMP6Inx144Yr/XmPX15F13PVVz52I4HOQ+9g8Sjjii8fX5m0o574lvCJnw0JkjOOPLU6ByO5zxBIw9t0X9apKYcjQkpjKil5iKx/c6HmOG+IxbMcdHzBCfcXd0UioUCvH73/+et99+mx49evDnP/+ZESN2/Vt59tlnc8IJJ/Dzn/+81W13JI2hoi8mMZsmLHwGPrgZAnXhHYyPvxPGng9WW4uaqKv2s+GHYtbOL2DH+orG4xaLQe7IdIZMzGHA2Ewcrpa117R77YvZNE1+2FrOS/O38s7SHdTVBwGwWw2mj+rBjyf25YhBGVgsBsGKCryrVuNdsQLvypV4V6zAv3lzs+1akpKwpqRgTU7GSE7C57ZR44IKR4CdNi+FlmryjAryjHKqnCY1Lqh2Q40LQpb2vXc2w0aaK410VzhZFUlaJTuSSbAn4LF5wo92Dx6bB4/d0+S42+be42dpmib+kB9/MPxVH6qnPli/61jITzAUJNmRTKorlVRnKjZL69/PfdHfdHzEDPEZt2LuxKTU22+/zaxZs6iuriY5OblJZwzDoLS0tDXNdTgNqGIjb20Zbzy0CEyYefkYBo7L2u81pt/P9l9fR/Unn2A4neQ+/hgJhx/e+PpfPlnHn+euxeOwMm/SAjK+exD6ToKffdDiftXU13D53MtZXLyYZEcyT05/kpEZI9sU4x79j8P3Oh5jhviMWzHHR8wQn3F3dFKqu9AYKvpiGnPxWnj9EtixJPw8c1h4Sd/wk1pUozOiqtTL+gVFrP2+gJ3bds1Qstkt9D8okyETc+g3KqPFBdKjGXOVt563luTz0vxtLMvblTzLTXczbWg2fdLc9Enz0DvNTe9UN5mJDkI1NfhW75aoWrkS34aNsJdaVS0RSnARTPIQTHQTSHRRn+iiPsFJfaITf4IDX4KdOo+NcuqoMOooC9VQEqqiOFROmVlDvRX8Nqi30ar3JsLAwG1z47A6GhNOgVDzyxn3JdmRTJorjVRnKmmuNNKcaaS6UsOPztTGWV3JjvBXkiMJl23vSzv1Nx0fMUN8xq2YOzEpNXToUE488UTuvfdePB5Pay49IGhAFRumaTLvlTUs+zQfV6KdH992GJ7k/U/vNv1+tl99DdWffYbhdocTU4ceCkAwZPKTp7/j6w0lHJHt54Wqn2OYQfjld5A9vMV9q/ZXc/nHl7OkeAnJjmSemv4UIzL2P5trv32Pw/c6HmOG+IxbMcdHzBCfcSsp1TYaQ0VfzGMO+OH7J+GLP0JdwwfHfQ6F434P/Se3urmyghrWfV/I2u8LqSjaVZjfZreQ1S+JHgNS6DEwhZyBySSkOJttI1YxL8+rYM7323hjcR5V3uYTMk6bhd6pbnqnuenTkKjqneamt8ugl6+ctJAfs6qSYGUlwYpKQlXhx2Bl0++DlRWEKioJ1dRErf8RIbuNkMNKwGbB77JRl2ijJtFGlceg0m1S7jYpdQXY6fRT7PRT4TGpdEO9fd8/S5vFhsPiwG6147A4cFgdWAwLlf5KKnwV+7x2XxwWR2OiKsmRFE5YOZNJsoe/T7AmkJWY1ZjkiiS9mpvd1R3E4/+PQXzGrZg7MSmVkJDAsmXLGDhwYGsuO2BoQBUbpmlSU13LO4+soGR7Nf3GZHDSLw9q0c8g5Pez/aqrqPliHobHQ98nHsdzyCEAFFV6mfnIPEpq/LyX8w9GVsyDw66Amfe3qn/V/mou+/gylhYvJcWZwtPTn2ZY+rA2xRoRj+91PMYM8Rm3Yo6PmCE+41ZSqm00hoq+DovZWwFf/QW+fTS8eQzAkOlw7O3Qo/WbwZimSfHWKtZ9X8i6BUXUlPv2OCcp3UXOwGR6DAgnqbJyk7DaLDGPuc4f5KOVBawpqCKvvI68sjq2l9VRWOVlf//V47Ba6JPupm+6h37pHnLTPfRN99A3I/zocTRd3mbW1xOsqiJYXkGwopxgRQWhigqCFRUNx3b/KidYWwd+P6bfj+n1Emp4bM8srUZuF6SlYMlIx5qViS0rC3tWFo7sHjize+DIzsaamYktIwPDam1yaSAUoNJfSbm3nFJvKeW+csp8ZZR7w49l3rLGx0pfJVX1VVT5q/ZZd2t/nFZn4+yrVGdq44ysZGcyVsOKzWLDYliwGtbwl8Xa7PcWi4VgKEjQDBIIBQiEAo3fB0NBAuaex+xWOwn2BBLtieGlkLYEEh3h7xPtiY1LI9uSOAuFQni9Xtzu7pl02xv9/7dibk7MklJnnnkm5513Huecc05rLjtgaEAVG5G4a0oCvHL/AkIBk2mzhjFqSu8WXR/y+dh+xS+p+fprLB4PuU89hefg8QB8tqaIn/7re6ZalvBvxwOYrlSM61e3qOD57qr8VVw+93KW7lxKqjOVp6Y/1a7EVDy+1/EYM8Rn3Io5PmKG+IxbSam20Rgq+jo85qoC+PwBWPhvMIOAAQedC0ffAmn92tSkGTIpL6qlYGMFBZsqKdxYQUl+DfzPf2FYbRay+iaSMyCFtN4uevRPIzXHg9XaMQW+/YEQBRVetpfXsr0snKyKJK3yyuvIL68jENr3fxZlJjrp25C06pPmIdVjJ9llJ8llI9nd8NjwPMllx2HbFdu+3muzvp6Qz4/p94WTVT4fps9HqKqKQGkZwbJSAqWlBEvLCJaWEigrJVhW3vB9GdTXt/wHYbFgzUjHlpmFLSsTW1o61rQ0rOnpWNNSsaWlhZ+nhZ9bU1IwmtnBMWSGqKmvocpfRaW/kkpfJZX+ysbnFb4KKv2VlNWWURmopNwXTniVecuoD7Wiv53IYlhIsCXgsDoImSFChAiZIUzTDD8SfowcC5pBzIZffJfN1ZjYinw1eW53N3nNwMDExDRNGv9n7vkIYGJit9ibXN/ky77n/ewWezhp15Ck2z1R19zzkBnCYXXgtDpx2Vw4rc7GL6vFusfPSv//rZibE7Ok1NNPP82dd97JxRdfzJgxY7Dbm+60duqpp7amuQ6nAVVs7B73kk+28dWr67E5LJz7u0NJzW7ZMs+Q18u2y6+g9ttvsSQk0PefT+MeOxaAJ7/YyH3vreAL57X0MXZinvE4xtjzWt3PKn8Vl829jGU7l5HmTOOpGU8xNG1oq9uB+Hyv4zFmiM+4FXN8xAzxGbeSUm2jMVT0dVrMJRvCOxuv+E/4udUBEy+BKTdAQka7m/fXBSjcUknhxkoKNlVQuLESb82eiQiL1SA1x0N6zwTSeyWQ1iP8mJLt7rBkVUQgGGJHhZdtpbVsKa1la8PXttJatpTUUlHX+kSKy25pkqQyMDEMg5BJQ6IBQqaJabLrmElDsgM8DivZSU6ykpxkJbnISnI2Po88OqwWQtXV4QRVSSmB4mICO4vDj8XFBHbubPw+WFLa+llZFku4AHxD0sqamITF48GS4MHi8WB4wo/hr4SG1xIaXnNTj4ErKRGLw4HhcIDNhtcSpKwhUVXmLWt8LPOVUeWvImgGG2c/Bc0goVCIgBlOlOx+PGgGCZkhLIYlvMuhYWucZWW1WLEZtsbvrYYVu8WOxbDgD/qpqa8JfwVqqPE3PDYcq62vbUwuyZ5sFhsuq6sxYRVJXlmwYLVYsRgWLIYFA2PX94aBhV3fWw3rHscshgULu32/25eBgWEYNP6v4f8vmxxveAQaXw+ZoSZJt2Ao2Jici8yk231G3e6pkcjvwO7JwN0fMcP3j8yya5J8tDdNCu7+emS2X+T3tfF7y//MBGx4jMQRSX5GkqG7J0V3P6c5//vvS+TnFJHtyW7RRgcHTFLK0kymvLExwyAYDLamuQ6nAVVs7B43Jrz5yCLy1pSTMyCZM284GEsLBxahujq2XXY5tfPnY0lMpO+//ol7zBgAnv5yE2Xv38MN9lfYmjiW3Os/b9PPuNJfyWUfXcbykuWkOdN4esbTDEkb0up24vG9jseYIT7jVszxETPEZ9xKSrWNxlDR1+kx5/0AH98Om74IP3ckweSr4ZCfQUJm1G5jmiYVRXUUbqqgYGMlBZsrqCiso97X/H83NCareiWEE1Y9I8kqD5Z27nTXVhW19Wwr25Wsyiuro9JbT2VdPVXeAJXehse6emr8HfffQylue2OCKs3jwGY1sFoMbBYDq8XS8Njw3AiRUFuFp6oCT1UZ7upyUvw1JPpqcNdV4ayuxFpVgVlRTrCsnFBlZew6bhgYDgeG3d700enAmpSMJTkJa1Iy1uQkLEnJWJObO5aEJSkJw2YDixXDagGrNTyzy2pt899UyAxRF6hrTFL5g/7GpYR7S54YGOEZRCbUeevABnXBOuoC4a/a+trG73f/qq2vxRv0YprmHsmV5hIukcf6UP2uNgK11NXv2XZdoG6fs9IMjHAyL5LQszQk9AxrY/LOF/ThDXrbVDhfuo4Pz/qQXom99nveAZOU6uo0oIqN/427qtTLS3d+h98b5LBTB3DIiQNa3Faotpatl15K3YKFWJKTw4mpUaMAeOW/8znjsxnYjBB/G/48V557cpsTU5d+dCkrSlaQ7krn6elPMzhtcKvaiMf3Oh5jhviMWzHHR8wQn3F3ZFLquuuua/G5f/7zn1vVdkfTGCr6DoiYTRM2fAof/x4Klu467smArOGQOTT8mDU0vINfcq827RC363bhmJ0OJ9XlPkrzayjbUUvpjmpK82soLaglsJdkldVuaUxQZfRKJL13+DEh1XFA/c4EgiGqfQEq68LJqkpvPVV19fj89TgddiwWCxYj/GMMJxnAYhhYDKPhWDhhUOMLUFTlo7jKR1GVt/H7yJc/GIVaVM1w261kJjnIclnpY/XTEx/ZIS8ZwVo8AR/Oeh92vxeH34vN78Xm82L11WHxerF468BbC7W1mLW1hHy+xjpaHc4wmiapIo8OR3jmlsuF4XJicTjD3zsdWJwuDKcTi8uJ4XBiuJzhZJnV1pD0smFYrRg2azgRZrOG27TaoCEpFgiFcLjc4URb5HVb+HvDag23Ybc1/33kHFvDfSLf/8/EENM0Mf1+QtXV4ZlyVeHHUE01waoqQtU1hKqrCVRX4q8oJ1jvx2oPx211uLBEfgaRhODuycHIl9UKFguGxULIMKgnSD0B6gniN8OP9WYAPwF8Zj3+YACrw07IYmBaLISsEDLANCBoMQgaJqYFQhYINhwPEWqYMRgiZJqEDBPTDBHcfWlkw3mR72mYTbi3pY2RYwBWLNixYguCLQSOkAVbIPy9LWhiDxpYQ2ANmFiDJobNhumwg8uJ6bSD0xF+7nSALTxrafdZRrXeWgKWAN6AN5wc3C0BuXuyMPKaN+DdNdNvt1pojbO4zADBYD2GCYYJFjO8Eto0dv0sI///G5mJFklW7v59c/aV9nnr9Lfomdhzv39SSkpFiQZUsdFc3Gu+K+Djf63EYjE468YJZPdr+c87WF3Dtksvpe6HH7CkpNDvmX/hGhHeMW/bP84gt/BT/hk4gfUTfsfdp41u06dmFb4KfvHRL1hVuop0Vzr/nPFPBqUOavH18fhex2PMEJ9xK+b4iBniM+6OTEodffTRLTrPMAw+/fTTVrXd0TSGir4DKuZQCFa8Ht6pr3jV3s9zJO1KUGU1fGWPgNT/b+++4+yo6v+Pv+b2vdt7SXaz6b2QQEIooQVCL4IgoHRQ2hdFfiqKIFhA9KuI8AVFwUpHAQURCAklpJBAKultU7b3duvM74+7u9klm7KbLcnO+/l43MfcO3dm7nnfuYSTT86cGXJQxaoDZbZMi/rqQKxAVdxIdXFj2/NIqPMijNfvaitUpQ+KJy0vgcR0H754Ny6Po/+/W3r+XFuWRW1zuKVgFStS1TSFiJgWUdP6wtKMLaMd14ciJtVNISobglQ0hChvCBKK9GyhK9HrIivJS3ail+x4N7l+J5lxDrLjnGR4DTK8DtK8Bj7MWJElEIgVVerqidbXtSzrMetqie61rnfugHhYMowOhSozFOraXGIDTet/Q61V3HbP2//XZUUiHPAOBwfL7cbh9WLE+dqKl5bDgQOwTDP2Z6hptj23LBOirc8tiEY7PLcsK7ZP++cHc3mtYYDDESsYdva8/ffQVuU2vvBdtRTXWtYVvvA87pycA350vxalHn30UW666SZ8Ph+PPvrofrf9n//5nwM2rj+pQ9U7OsttWRb/fWoNmz8tIzXHz5e/fwxuz94T4+1LtKGBHdffQPOKFThTUij485/xjR4FG9+Bv19CjRXPjODjnD9tGA9dPAnnIRam0n3p/P6M3x/0HFN2PNd2zAz2zK3M9sgM9syty/e6R32onnfYZg42QOVGKN8A5eugYgOUr4eqLS0TpHfClwK5kyFvCuROiS1Th+5VqOpuZsu0qKtspnJXI1W7G6jc1UjlrgZqypqx9jNBudPlwBvvwhfvxhfvxuuPPffGu/G1rPf6Y+tdbgcujxOn29H23OV24PQ4DnmOq8P2XLdjWRaNoWhbkaqiIUhlQ6xoVdkYe90UihIItz5MApEowbBJMNLyOhw94ITxnUloKV6l+j1tbWmdZ8tsnWfLjM3BBbGlaYFlRomGI0TDUaKRKJFwhGg0QjQSxTBNHJaFwzJbHrHnbjOCNxrGY0bwWxGSHCYJhkmCESHeMIm3IsRZEeKIEmdGcJsRHFg4zChOs+VYponDjGK0Hjsa+zzDMnFEw7iwcFpmbHszumf7aATDNLGiEQhHsKLRWOEkGsWKRmOFi67yx2MkJGD5/Zhx8ZhxfqJxfiI+P2Gfn7A3jojTiRGJYkTCGJEwRCI4ImGM8J7nRCM4wmGMSAQjGo7lMU2wrJbnsaKKYZlgWhhWy/tmtGVptn0Hrc8xoxgt381ho/1osPYPlwsrEtlzs4Hm5v4Z4dcPRsx9F/egA9+grF+LUkOHDmXp0qWkp6czdOi+L8MyDIMtW7YcsHH9SR2q3rGv3IGGMM/9eDFNtSEmnjKYWZd1bVLxaH09RdddT2DVKpxpaQz529/wFhbAb6ZAbRHfCt/CP6MncOGUPH755cm4utFhqA3WcsPbN7Cuah1xrjjuOfYezh9+4An77Xiu7ZgZ7Jlbme2RGeyZW0Wp7lEfqucdcZkjIajaHCtQVbQUrFoLV53NXeNNhtxJ7QpVR2GlFhIIhnosczRsUl3a2LFYtbuBptoQZrTnLggxHEZLocqB0+3A7XWRkOolMdVLYrqPxDQfiek+ElJ9xKd69ypiHXHn+hBEoiaBiElzKEJZTQO1QShruQSxtC42squ0LkB5y7Kpj+bg8jgduJwGkajVa5c+Hiy/x0m810WC14W/5R/tI1GLSEtxyoxEsSJRzGgUou2KV5HY66DDRZPLR7PLg2X07U0BustoVxh0WiaGZbWMarJa3gej3QTzHgf4XA68LgdetwOP08BhtV7yauHEaKl5W7R+Aw4jdozW8VJhw0EIJwHDIIiToOUgYu0ZLRiJWkRMs+210zBwOx24nQYelwOPA/yGid+K4LeixJsh4qwoPjOCz4oV8CKGg7AFIQvC0diyw/MohCyLoAkmBg6nE7fLidPtwu1y4nI5cLU8d7tduFxO3B4XbqcTt9uB2+HAbVi4HeACnAa4DXAZ4DIsXA5iRVDDwI0Vu4+j2XJZo9lyaaNltY3GstpuqrBn/SWXnERKcvwBz6Eu3+sh6lD1jv3lLlpTyb9+uwKA8/5nMgXjunZXl2hdHUXXXEvg889x5eZS+OzfcW/4G8z7CVXp05hefBcR0+LcSbn8+rIpuLtRmKoJ1PD/Pvh/LCpeBMAFwy/g+zO+j9+97zsH2vFc2zEz2DO3MtsjM9gzd38WpZYuXcqLL75IUVERoS/8C+w//vGPQzp2b1MfqucNmMyREJR9DsXLYfdyKF4BpashuvcoA8ubhJkxBkd8KoYnEbwJ4Gl5dPbcmxC7ZDAhE7xJBz2flWVZhINRAo1hgo0RAk0ty8Zwy7owgaZIbNkYJtQcIRIyiYRNIuEo0Zbn3WEYEJ/iJTHNR0JLsSoxzYsnwUFaViJJ6XF44g58p6sj3cH+vhuCEUrrApTVBaltDgEGDqNlni1H6x3WaJtzq8MScLsceJwOPC5Hx4KCM/ba44pN+N6+DaGISVMoQmMoSlMwtmwMRmgMRmgKRWkMRVpeRwlGYvMdRVsufWx9vmcdHd4PhiMEIlaHYzQEIzQEI0S7MYrsQNxOA6/LidflwOeOLT0uB163M1bUcTtxO1oLOC3zERm0XebWOodZ23Oj9fztufItNldTy/xGbRHar7OIRE1a7y4ZG8nWcbRb62uz7RJSs210XSAcJRAxe/zyUTk4H333FAan7vvvva16q/808P80lH5XMD6diScNYtX7u3jvz2v5yr0z8MW7D3p/Z1IS+X94iu1XfpXQ1q0UXX8DQ578X1zzHyStchl/Oi+Ja/9dx79XFhOJWjx6+VF4XF0rTKX4Unhy9pP8YdUf+L8V/8drm19jdcVq/vfk/+3SPFMiIiL78vzzz3PVVVcxZ84c3n77bc444ww2bNhAaWkpF110UX83T6T7XJ7YaKi8KTCtZV00DGVr2xWqlkPJaoxgHc5dS7r5OXGQmNPukQsJ2bFlYvae175kDMPA43Ph8bmga/8e2sayLKIRM1asCrUUq8Kx56FAhIbqAPWVAeqrg7FlVYCG6gBmxKKhOkhDdRA213Z6bE9cy0irlsJV2/OWZXyKF2cX+7NHqgSvi4TMBIZnJvTZZ3pcDjwuDykH/nt4l+zvL+2WZRGMxCbBb2wpUjUGYwUwh2G03SnR7ezkzokOA5fD0XZ3RY8zNnrI63J2awqTntZTBXbTjH1HgXCU5i9cJhqOmG1FsbYimfXFglnLFOcthbMO35/T0e47drS953LGvluHI3a8UMQkFDUJR03CEavteSjSsi5qEopahCJRwqEw8XHelt9TSxHU6dirUBpbGjgNg1DLsYKR1mWs8BmMmATDsc8OhqMtS7PdqK7Y3HCRL8wRFzHNlhFfsW2gZTSZsaega0DHGym0FHRbt4n39G9ZqFsjpXbu3Mnrr7/e6b/y6c4xA+RfvLroQLnDoSgv/vQTakqbGDEtizNuGN/l7ye8ezfbrriSSEkJvgkTKDjPiXPrf2DGzbw39Ft846+fEoqazB6bxeNXTsXrOvj5q9r7pOQTvvPBd6horiDOFccPZvyAC0Zc0OXMA5EdM4M9cyuzPTKDPXP310ipSZMm8fWvf51bb72VxMREVqxYwdChQ/n6179Obm4u999/f7eP3RfUh+p5tsscDWOVrSVc/DluK4gRaojNXxVqeXT2PNgAwXoI1R/857jiID4T/KkQlxa7k6A/reV5y+u41I7rPAmHdFfBVpZp0VQfor4qVrBqqAq2PG+mrqqZxpoQwcbIgQ9kQFyCG3+Sh7hET2yZ5MHfyXNfovuQ57zqDbb7fWPPzGDP3Mrcj5fvzZ07l/PPP59hw4axbt06JkyYwLZt27Asi6lTp+rOMTb8ccLB5S7dVscrDy/DMi1mXzuO0TMOPMP/FwU3b2b7lV8lWlODf9Io8kfPxxGfAt9ex/tbG7jpL0sJRkxOGpXJ7742DZ+7e4WpiuYKvv/h91lYvBDo/HI+O55rO2YGe+ZWZntkBnvm7q+iVHx8PGvWrKGwsJD09HTmz5/PxIkTWbt2LaeeeirFxcXdPnZfUB+q5ylzFzKHmqChBOpLob4YGlqWX3wd6Hxk0gG54iB5MKTkQ3J+y7Jgz+vEXHB2fzRB+9zhYLRlJNWeolVDdYD6qiANVQEaqoNEu3gZky/ejS/BjdvrxOVx4PY6cXucuLxOXB4nbo8DV8s6d+s6rxOv37Vn8nd/bGSZ0UMjb/T7tkdmsGduZe7Hy/fuvvtu7rrrLu6//34SExN55ZVXyMrK4sorr+TMM8/s6uHERrILkzjmnEKW/GsrHzy/gbyRKSSm+bp0DO/w4eQ/9XuKrr6GppUb2F0/iEHH7ML4/HVOmnwZz1xzDNf/eSnvbyjn+j9/wh+uOoa4Ltzxr1VGXAZPnh67nO/x5Y/rcj4RETlkqamp1NfHRnsMGjSI1atXM3HiRGpqamhqaurn1okc5jx+SBsWe+xPuBnqS6CpsuVRBc1VsWVT5Z7nzdV73o8GIdIcu+Ng5cbOj2s4IWnQF4pWg1ueF8Te8xzctWAen4u0XBdpuZ1PLGxZFs31YRprgzTXhWiqD9FUF6K5LkRzfbjj64Ywlmm1zZV1qAwDPH4XXr8bn9+Ft/WOhS13KDScBli0TKBM20TKeyZPBlreMy0LMxrF43XjdDtwuhw4XQYOV+xuhrF1Bg5ny3str11uB07Xnrsgtl863Q4cX5gfSkSObF0uSq1du5bnnnsutrPLRXNzMwkJCTzwwANccMEF3HzzzT3eSBk4pp05hG2rKinbVsdLDy1lxNQsRkzLInd48kH/q0zcxIkMfvwxdtz0deq3hik2kskd8jTG5Ms4bkQGf75uOtc+s4QFmyq5+pklPH3NMSR4u/4vWw7DwU2TbuKorKP4zgffYXPtZi5/4/J9Xs4nIiKyP7NmzeKdd95h4sSJfPnLX+aOO+7gvffe45133uG0007r7+aJDAzuOEgbGnscDMuCcFOskFW7E2p3QM2OlmVRbFm7K3Z3wdqi2GNf/BntRlsVtHveUrwyDq5oZRgG/qTYpXkHbH5LQaqpLkSgMUw4GG2ZAytKOBglHIoSCUYJh8yWZbRtGQ5GCbZM9h5sihAJx+7MFWyMEGyMUHdw32CfMwzaClQutxOPz4knzhV7dHjuwhvnwhPnjM0v1rLe7XW2G0kWK4ipyCXSf7p8+V5OTg7z5s1j7NixjBs3joceeojzzz+fFStWcPzxx9PQ0NBbbe0RGnreO7qSu6a0iVd//RmNNcG2df5kD8OnZjFi6sEXqOreeYddd3wTTJP0MfVk/f5dyBoDwLLt1Vzz9BLqgxGmFqTwp+umk+Q7+MnVv6izy/nunn43jqjDVudav2/75FZme2QGe+bu68v3Vq9ezYQJE6iqqiIQCJCXl4dpmjz88MN8/PHHjBw5knvuuYfU1NTuxOkz6kP1PGU+QjKbZuzSwdqdewpVrYWr2p2x5wcx55XlcENCFkZCFsRnQULrIzu2jG993rU7DfaUSLi1SNVyt8J2BavWuxdalrVnEmXDwHDsWdJ+vQEYEA5HMHBgRi3MsEk0Gps8PhoxMSN7nsceLa/DsUek3fOuXs7YFYZB26WNey5xdLRd4ujyOHE6W0d4tSzbjfByuIzYyC+XgeE0iJpRvF43RsuIro7fU8t31GFp4HS1jAZz7T0yzOmKjQ7rb5ZpEYm0K262FD8jodjrqBkhPtGPN86F2xcrBLo8R3bBr20UoGlhmRZmy9IyIRo1CQYDeL1d+7PMapmVvcOxrS+sN2MTuGMBBm2jAw3Hnt+Sw7GP1y3/7RktM5zHbrq457/NQ/0+Dos5pS688ELOOeccbrzxRu666y5ee+01rrnmGv7xj3+QmprKu+++25XD9Tl1qHpHV3NHIyY71lax+dMytiyvINS8Z8LH+JYC1fBpWeQO23+BquaVVyj+wT0AZJ07jvRfvtL23oodNXztj4upC0SYNDiZv1w3nRT/gf/FaV9My2y7nM+0TIYnD+cnx/6E8dldn7T9SKXft31yK7M9MoM9c/d1UcrhcHDMMcdwww038JWvfIXExMTuNLvfqQ/V85R5gGS2rNhcVm3Fqp0to6p27ileNZR27ZguX2xCdl8KxKV0vvQld1zn9kM0tOcRaX0ejN0NMRLs+L4ZiRW/Ohw7Nfbc2f1/zN3ztfTcubbMljshthSoouHY80goSjgQJdgcIRSIEGqOEmqOxB6B2DLYuq7ldesIMjPS5ft99RuH0+hQsHI4YwWIPYWJPc8dDr7wuuWWa1b7Sy1peb2nCAJ7iiSmaRENmy0j61qKT+GuFwYNg9ioNF9sFJvb62wbreZ07ylYGY49d4prLagYLbeHa10CHQpDZrQlT7TdOhMs02xZ7tnGjHbcb09x6QvvmS3Hay0MmUfOb6Qr2r5bAAOuvP9YkjLiDrjfYVOU2rJlCw0NDUyaNInGxka+/e1vt/0r369+9SuGDBnSlcP1OXWoeseh5G4rUC0rY8uKLxSoUrwMn5rJiKlZ5OyjQFX58+9R9sxrAOT86IekfuWKtvfW7K7lq39YTHVTmHG5SfzthhmkxXe/MAWxu/N994PvUt5cTpwrjgeOe4Azh9pjPjX9vu2TW5ntkRnsmbuvi1IffvghzzzzDC+//DKmaXLxxRdzww03cOKJJ3an+f1Gfaiep8z2yAxghQMEq3biDddiNJZBQ8ujsSxWsGpot64rdxrsLe74zoth/vTYnQ3bHhl7li5vh0Mc7ufajJp7Lmv8wqWNkaDZdolj60gtMxobzdW2bB3hFW153jIKLBKKYBiOPSNeWkbAmC0jYrCsjs+jVmzfdqPComET8zAuiDjdjtjIMk9sRJnL7WgbPRUORAgFo7FRPrKXthFL7Ytvjo6jDPe8T9tvyDItzHajtlpHWPWEr/10JknpR0hRKhqNsmDBAiZNmkRKSsrB7nZYUYeqd/RU7mg4VqDa9GkZW5eXEwpE295LzfFzxg0TyBic0HEnM0rZFROpXG6BYTDoN4+QdMYZbW+vL6nnyj8spqIhyKjsBP5+w7FkJnb8n2ZXVTZX8t0PvsviksUAXDv+Wv5n6v/gcnT/rixHAv2+7ZNbme2RGeyZu7/uvtfY2MiLL77In/70Jz788ENGjBjB9ddfz9VXX01OTtfvSNvX1Ifqecpsj8zQxdyhplixqqkyNgKruQYCNQdehgPg9IDLE1u2Plze2MgnZ8vS5Y2tNxwQrGt3nFoIdvPuhQDe5FhxKiEL4jOw/JlE3PG4fIkYHn9svi9XXGzp9oPb17Jsee3ytXvu7fPLF3tCT/6+W4tfraPCopHYaKVo2IyNADK/MMLHZK9RRG2jiUyr3eWEexdF2l63K4603sXR5d5TfIq9duw1UOCLuS3LihX6grGRbKFAhFAgGitYtSyjLSPV2ibIb72ErcO6diO7aLmMrZORYIbDiI0eM1pGirU+d3Z839G6ncPR7rmx3+MaDjqOSGu7XBWCwWC//lnWVvC09lxW2HYZIO1GelmxywEts+N+se/Vwp/kweF0HNTn9XtRCsDn87F27VqGDj3IyQMPM+pQ9Y7eyB0NmxS1jKDauiJWoHK5HZz81TGMntGx827Nf5iShx+jZks8httN/u+eJP6449re31TWwJV/WERpXZBhmfE8e8Ox5CR37c5/XxSOhvnVJ7/ib+v/BsCM3Bn8YtYvSPUd3nOCHAr9vu2TW5ntkRnsmbu/ilLtbdq0iWeeeYa//vWvlJSUcOaZZ/L666/3yLF7i/pQPU+Z7ZEZjqDcZjRWCGstdjVXd3zeVAWN5S2PitiyqSJ2KWCPMjoWrDztnrvjOhavLDPW7mg41o72j2g49p4ZiU1Wb0Zid1JsLcy5vC3PvR3XtX/PFQeeePAktCy/+LzltcvTu+c5dm1du1xhiEb2ZGvN3/57aN3OsmIFSYcbnK6Wpafd8y++56ZtqA4tOQxjn4XCI+b33Z5pdvxdmFFwuGK/rYO4fPWIzHyIDpui1NFHH83Pf/7zI/YuMepQ9Y7ezh1oCPP202vY8XkVAJNOGcxxl4zA2VrRrSvG+t/x7FqQRP3OOAy/nyF/eoa4SZPajrGtopErnlrE7toAQ9L9PHvjsQxKOfAwxX1pzfx+yfvc+/G9NEeayYvP49en/Jpx6eMOKe/hSr9v++RWZntkBnvmPhyKUhAbOfX3v/+du+++m5qaGqLR6IF36kfqQ/U8ZbZHZhjguU0zVrhqX6RqLMdqKCPaWI3TDGFEArG7HIabWx5NsNe65liB4EjlcGN54mNFLIcDIzb0iLaJe/ZMmETbUJvWdVZL4Sz6haLaFwtqh412hSoMrHbFqz2/7/ZFrPbbt77naHkY4HC2e93y3OH4wjpjz5Cp1msD27/u7D0zuue7a1/AMyO0DRvqNJ6zZWSfr93S1zLSL7a03D6iloHTsDDM6J7iaOu53Gtdy2uHK5bX4WrJ6dr/utZiZKePTt7bK0v7P2+Mztdf8HhshOMB9Fb/qcvXGv3kJz/hrrvu4sc//jHTpk0jPj6+w/u90UkR8SW4Ofe2ySz51xaW/Wc7K+ftpHxHPXNunEB8sheScjHGnkWe+W92LM+naWMFO276OoUvv4Rn8GAACjPieeHrM7niD4vYXtnEpU8u5Lkbj6Ug/eBuz7svcwrnMDxlON+c902K6ou46j9Xce/Mezl/+PmHdNzaYC1FdUVMyJgw8DovIiI29sEHH/D000/zyiuv4HA4uPTSS7n++uv7u1kiIt3jcIA/LfbIHLVnvWURDgRw+nwHfyleNNyxcLXX8y8sI80tf4l37/mLvNPV7i/17j1/2Xe6Y8UGK7pn0vdIMDYRfKRlQvhIcO/3ws2xSypDDRBqbPdoeR1tuaO4GcYI1AA1PfwFH4DhbBnp5NqTs7NRT3uNpmp53TaaqisTmXcsChkDbf4oK9pyfhv2uYlBN4oph6tIoF8//qBHSj3wwAN8+9vf7nDHmPZ/UW69Paj+lW8A/yvIfvRl7i3Ly5n7p88JBaL4kzycedMEckekwMZ34O+XEHWmULT8aAJrPif5ggvI+/lDHfYvrm3miqcWs7WikdxkH3+/YQbDMhM6/7D9+GLmulAdd394Nx/s/ACAr4z+Ct855ju4u3D3kqgZZXHxYl7d9Cpzi+YSMkN8a9q3uG7CdV1uX2/Q79s+uZXZHpnBnrn7Y6TU7t27+dOf/sSf/vQnNm3axHHHHcf111/PpZdeutc/8B2u1Ifqecpsj8xgz9y2yRwNtxWprGADwcYavB4PRuvEPm2jeKw9o1s6rLP2Hi3T/nX7glPrSJrWwpPDFSsK9gTTbFe4CrcbecTebf7COssyCQSD+Lye2AixfY1can3d/nitI4gsM1YMahtdZHVcZ5kcePTVF95zti9Oug7w2tlyl8rm2Pxs+1wGINyMFW4mHArg9vgwHM6W0V7OPefNcLRb17K+/ai41lFwrZdk7mud4ejkYXS+vv130KHM88Vz0G4dwLgLwXvgvw/3++V7TqeT4uJi1q5du9/tTjrppIM5XL9Rh6p39HXumtIm/vO7VVTtbsThMDj+yyOYOCsX49EpULuD5on3se0HT4HTyfA338DzhbtCltUFuOIPi9lU1kBmopdnb5jByOyu3aK7s8ymZfLkiid5YsUTAEzNmsr/nvy/pPvSeW35bmqaQlx9XOFe39GO+h28tuk1Xtv8GiWNJR3e8zq9/OP8f1CQVNDFb6nn6fdtn9zKbI/MYM/cfV2UOuuss3j33XfJyMjgqquu4rrrrmP06NHdaXq/Uh+q5ymzPTKDPXMrsz0ygz1zK3M/XL7XWrs63ItOYg8p2X4u/s405v1tHZuWlvHhCxsp3VrHyZOuwf3hj4mrfZv4WSfS+MGHVPzu9+T97Kcd9s9K8vH8Tcfy1T8sZl1JPV/5/SL+dsMMxuYeWifbYTi4ZcotjEsfx90f3s2nZZ9yyetfJjfwdRaujRW9Bqf6mT0um+ZIM+9uf5d/bvonn5R80naMRE8i5ww9hwtHXshvlv2GhcULeWDhAzx1xlO2+QNPRGQgcbvdvPzyy5x77rk4nc7+bo6IiIjIYaNLY/30F2I5nHh8Ls64fjzHXzICw2GwYUkpr3w0k9poHhQtJPOKcwGofe01Qjt27LV/RoKX5248lgmDkqhsDHH5U4v4eFNFj7Tt5PyTee6c58jzF1IZqGCV9RDulEWAxaML3uVHH/+IU148he9/9H0+KfkEA4Pj8o7jF7N+wbxL5/GDY3/A+PTx/HDmD/E5fSwuWcxrm1/rkbaJiEjfev3117ngggtUkBIRERH5gi4VpUaNGkVaWtp+HyJ9yTAMpswu4IJvTiEu0U1lcZCXqn/FtuBU4po/Jv6EEyAapeJ3v+t0/9R4D3+/4Vim5KdQ0xTmij8s5ua/LWNHVdMhtcs0Ld74NMKm5dcRrpuAYUTx5b5K/IgH2ep5iFc2vkJjuJHBCYO5bcpt/Pfi//K703/HmUPPxOv0th0nPzGfm6fcDMAvl/6SyubKQ2qXiIiIiIiIyOGiS0Wp+++/n1//+tf7fXTV448/TmFhIT6fjxkzZrBkyZL9bl9TU8Ott95Kbm4uXq+XUaNG8eabb3b5c2VgGTQqlUu/P53soUkEI17eqP4BS+Y3kXZjbILw2ldfI7RzV6f7Jse5+dsNM/jqsQU4DPjP6hJO+9/3+flb62gIdv3Wq2X1Aa56egm/+O96olEPZ2T8P26edDsOw4HDXYdlusl1HM/Tc57mjS+9wdcnf53chNx9Hu+qcVcxJm0MtcFaHv7k4S63R0REBib1oURERORI16W7GH7lK18hKyurxz78hRde4M477+TJJ59kxowZPPLII8yZM4f169d3+jmhUIjTTz+drKwsXn75ZQYNGsT27dtJSUnpsTbJkSsh1ctF357KRy9uYPUHu/mk9kKKXi9n7HFn4vj4LSp//3tyH7i/8329Ln5y4US+euwQfvzvz1mwqZIn5m/mpaU7+c6c0Vw8bTBOx4EvX/1wYznfemE5FQ0h4txO7r9gPF+eNhjDmMpxg6azYPs6fvFPN1uII//8iTiMA9eFXQ4XP5r5I6548wre3Pom5w0/jxMGndDl70dERAYO9aFERERkIDjokVK9MZ/Ur371K2688UauvfZaxo0bx5NPPonf7+fpp5/udPunn36aqqoqXn31VY4//ngKCws56aSTmDx5co+3TY5MTpeDk64Yw+xTqnEbTZRWJvCB71y2DjmTqldfI7x79373H5OTxN+un8EfrjqaoRnxVDQE+c4rKzn/sY9YvGXfl86FoyY/f2sdVz29hIqGEGNyEvnX7cdz6dH5bf/tTMmawq3HfIVjCnKJmBZ/WbjtoHONzxjPlWOvBOAni35CU/jQLi8UEZEjm/pQIiIiMhAcdFGq9e57PSUUCrFs2TJmz569pzEOB7Nnz2bhwoWd7vP6668zc+ZMbr31VrKzs5kwYQI/+9nPiEajPdo2OfKNvvRLXDHrPYZ4l2KaBluHnsfSSXey8f+ePeC+hmEwe1w2//3mLO45ZyyJPhdrdtdx2e8Xccvf955vamd1M5f9biFPzN+MZcGVMwp49dbjGZGV2Onxrz9hGADPLimiKXTwlwfeNuU28uLz2NWwi8eXP37Q+4mIyMCiPpSIiIgMFAd9+Z5pmj36wRUVFUSjUbKzszusz87OZt26dZ3us2XLFt577z2uvPJK3nzzTTZt2sQtt9xCOBzmvvvu63SfYDBIMBhse11XVwfEimw9XWhrf9zeOPbh7HDMHX/xjzm7+nQ2bv+QD+u+TkPCYOaX51L11xXMuGw8Lvf+74Lkdhpcf8JQLpySx6/f3chzS4p4c1UJ764t44YThvKNk4Yxf20Z97y+jrpAhESfiwe/NJFzJsbmh9rXdzF7bBYFaX6Kqpp4ZdlOvnrskIPKE+eK4wczfsCt793K39b+jbOHns249HFd+1IO0eF4nvuCHXMrs33YMXdXMx9u3436UAOHMtuHHXMrs33YMbcyH9z2B6NLc0r1N9M0ycrK4ve//z1Op5Np06axa9cufvGLX+yzQ/Xggw9y//17zyMUCATweDy90s5gMNgrlzse7g6/3A6MC55i1J/nkO+5hbe2f5ti/0RWLKhk28YlnHDZcLKHJR3wKPEuuOfMEVx6VA4P/ncji7ZW83/zN/O3RdupC8RGOk0alMT/XjyewalxBAKBAx7zq9MH8bO3NvLHj7bypclZOA7yezsm4xjOyD+Dt3e8zX0L7uOZ2c/gcvTtf8aH33nuG3bMrcz2YcfcXcl8MH+uH+7Uhzp8KbN92DG3MtuHHXMr8/4dbP+p34pSGRkZOJ1OSktLO6wvLS0lJyen031yc3Nxu904nXtGuIwdO5aSkhJCoVCnHaS7776bO++8s+11XV0d+fn5+Hw+fD5fD6XZo7Vy6PV6bfUDPWxz542DCx/H/+JVzIn/CctWncr6UV+htgzeeGwNE08ezLHnD8XtO/B/CpOG+Hj2xnTeXVvGz95cy7bK2GV8N80ayl1njMbtPPibWV5+7FB+O38r2yqbWLitjtPGZh94pxZ3H3s3i0oXsb5mPS9vfZlrxl9z0PseqsP2PPcyO+ZWZntkBnvm7mrmUCjUB606eOpDDRzKbI/MYM/cymyPzGDP3Mrcc/2nfitKeTwepk2bxty5c7nwwguB2L/izZ07l9tuu63TfY4//nieffZZTNPE4YgVADZs2EBubu4+/8XO6/Xi9Xr3Wm8YRq/9eFqPbZcfZ6vDNve4C2Dmbfg/fowCx6ekLNlI0ZzvsL0xi1XzdrJtZQWnfHUM+WPTDngowzA4Y3wOJ4/O4rXlu8hOcHLi6NwuZ070ubliegG/+2ALf/xoG7PHdf6XiM5k+DO46+i7uPfje/m/5f/H7CGzyU/M79LnH4rD9jz3MjvmVmb7sGPurmQ+3L4X9aEGFmW2DzvmVmb7sGNuZT7wtgfj4Id29II777yTp556ij//+c+sXbuWm2++mcbGRq699loArrrqKu6+++627W+++Waqqqq444472LBhA2+88QY/+9nPuPXWW/srghwpZv8IY8hMMsfV4I40M+Ldhzj7qiEkpvmorwzw+m+W895f1hJsCh/U4TwuB5dMG8wxQ1K73aSrjyvE6TBYuKWSNbtru7TvhSMuZHrOdALRAD9Z9BNbXcssIiLqQ4mIiMjA0K9Fqcsuu4xf/vKX3HvvvUyZMoXly5fz1ltvtU3cWVRURHFxcdv2+fn5/Pe//+WTTz5h0qRJ/M///A933HEH3/ve9/orghwpnG645Bn8w1KIywhhBYP433+Jr9w7nYmnDAYD1n5czHMPLGH7mso+aVJeShxnt0yK/sePtnZpX8MwuHfmvXgcHj7e/TH/3vLv3miiiIgcptSHEhERkYHAsGw2xKKuro7k5GRqa2tJSjrwJNddZVkWgUAAn89nq2F8R0zurR/S8PNL2DE/DcPjYsR783BlZLB7Uw3v/WUttWXNAIw7IY/jLx6BJ27fV7j2ROblO2q48PEFuJ0GC757KllJXZuj46mVT/HoZ4+S6k3ltQtfI9XX/ZFbB+OIOc89zI65ldkemcGeubuaubf7DkcK9aF6njLbIzPYM7cy2yMz2DO3Mvdc/6lfR0qJ9LmhJxJ/xXfwpYewQhEqf/MQAHkjUrjsnulMPjUfDPj8o9089+PF7FhX1avNmZKfwtFDUglHLf6ycHuX979mwjWMSBlBdbCaXy79ZS+0UERERERERKR3qCgltmOc8C0yzxgNQPU/3yCyazMAbo+TEy4dyUV3HkVSho+GqiCvP7Kc959dTygQ6bX23HDiUAD+tng7zaFol/Z1O9zcf9z9GBi8vvl1Fu5e2BtNFBEREREREelxKkqJ/TgcxP+/v+HLNLAiUPXDa8E0297OG5nKZfdMZ8JJgwBY/cEuXvjJEnZtqO6V5pw+Lof8tDhqmsL847OdXd5/UuYkLh9zOQAPLHyA5khzTzdRREREREREpMepKCW2ZPjTyPjWXQBULSkl8taDHd73+FycdPlozv/mFBLSvNRVBHj1V5/x4QsbCAe7NprpQJwOg2uPi42W+uNHWzHNrk/z9j9T/4dsfzY7G3by+5W/79H2iYiIiIiIiPQGFaXEthIuuhZfYTZWxEHVH56ErR/stU3+mDQu/+EMxp2QB8DKeTt54SdLKN5U06NtufSYfBK9LraUNzJ/Q1mX9493x/P9Gd8H4C9r/kJJY0mPtk9ERERERESkp6koJbZlGAYZd90DQPUGP5G/XQd1xXtt54lzccpXx3De7ZOJT/FSW97MP/73Uxa8solIF+eA2pcEr4vLZxQAsdFS3XFK/ilMy55GyAzx+PLHe6RdIiIiIiIiIr1FRSmxtYTTTsM7ZjRmxEHVZwF44asQDnS6bcH4dC6/dzpjZuaABSve3cFbT3xONGp2un1XXX1cIU6HwYJNlXy+u67L+xuGwbemfQuA1ze/zsbqjT3SLhEREREREZHeoKKU2JphGGTceisA1RsTiG5dBq/fBlbn8zp5/W5Ou3oc59wyCW+ci7JtDSx7c3uPtGVQShxnTcgBuj9aanLmZE4fcjqmZfLop4/2SLtEREREREREeoOKUmJ7iaedhnfUKMywwe5FaYSXvAIf/nK/+xROymDWFaMAWPbWdkq3dn1kU2duOHEYAK+v2EVZXecjtg7k9qNux2k4mb9zPstKl/VIu0RERERERER6mopSYnuGw0HW//t/4HDQsNvLljeyqHry11ir/rnf/UYenc2wqelYpsW7f/qccA/MLzUlP4VpQ1IJRy3+uqh7I7CGJg/lSyO/BMCvlv0Kax+jvkRERERERET6k4pSIkDCiScw9OWX8E2ahBlxUPppMtu+/h2a33tlv/vNvHgY8SleakqbWPjKph5pyw0nDAXgb4u2Ewh3r9B18+SbiXPFsbJ8Je8Vvdcj7RIRERERERHpSSpKibTwjRtH4XPPknPvD3H4HASqXGy75R5Kfvg9onWdX57n9bs49aoxAKx6fxdFayoPuR1njM9hcGoc1U1h/vHprm4dI9OfydfGfQ2ARz59hIgZOeR2iYiIiIiIiPQkFaVE2jGcTlKvuILhb/yLpNEuAKpfeo3NZ51N7b/f6PRSuPyxaUw8eTAAc/+ylkBj+JDa4HQYXHt8bLTUHz/agml27/K7a8dfS4o3hW1123h106uH1CYRERERERGRnqailEgnXIOGMeiZ1yiYE8KTGCFaWcnuu+6i6LrrCG7d+854M780nJRsP021Id5/bv0hf/6lRw8m0etic3kjT36wuVvzQiV4Evj6pK8D8H/L/4+mcNMht0tERERERESkp6goJbIvacOI/9ZfGHp2NZkT6zBcDpoWLmLr+RdQ/tvHMIPBtk3dHiezrx2H4TDYtLSMDZ+UHNJHJ/rcXH9ibLTUw2+t5/o/L6WyIXiAvfZ26ehLGZQwiPLmcv6+9u+H1CYRERERERGRnqSilMj+FB6P44JfkTG+gWFziomfMhIrHKbi8cfZev4FNL7zLlYoBEB2YRJHn10IwAfPbaChOnBIH33HaSN54ILxeFwO3ltXxpm/+ZCPNlZ06Rgep4fbjroNgKdXP011oPqQ2iQiIiIiIiLSU1SUEjmQqVfBzNvwJEbJH7+YQffejiszk3BREeXf+Q6bTjqZkp/8lObVa5h6ZgFZQxIJNkV47y9rsbo5HxSAYRhcNbOQ1249npFZCZTXB/na04t56D/rCEfNgz7O2UPPZkzaGBrCDTy16qlut0dERERERESkJ6koJXIwTn8ARs7BiAZI2v0bhj3/B9K/fhPOjAyiNTVU/+1vbLvkEoouupCjE9fidBvsWFvNqve7d/e89sbmJvH6bSdwxYwCLAuefH8zlzy5kO2VjQe1v8Nw8K2p3wLg+XXPs6vh0NskIiIiIiIicqhUlBI5GA4nXPwHyBwLDSU4/30DmbfcyOD/vMng3z1J0tlnYXg8BDduIvDYgwxf+yIAH7+0nsqiQ79kLs7j5GcXTeSJK6eS5HOxYkcN5zz6Ea8tP7gC08y8mczInUHYDPPYZ48dcntEREREREREDpWKUiIHy5cEVzwP/nQoXgGv3ozhdJAwaxaDfvUrRn70ITn330/cUUcxaOf7pFatJWoavPmD19n1wx/R9Oln3bqLXntnTczlP9+cxTGFqTQEI9zx/HK+/eIKGoKR/e5nGAbfmhYbLfXGljdYV7XukNohIiIiIiIicqhUlBLpitRCuOzv4HBjrH0d99vfg+YaAJxJSaRedimFzz3LiLfe5Pjp4Io2Uxefz/JP6tl+xRVsOe886ufNO6Ti1KCUOJ678VjuOG0kDgNe+XQn5z76Iat21u53v/Hp4zmr8CwsLB759JFuf76IiIiIiIhIT1BRSqSrhsyE834DgGv5X+A3k2Deg23FKQBPYSFD7rqVU26cCsD2wrOoyxxNaNNmdt58Czuuv4HAhg3dboLL6eBbp4/i+ZtmkpfsY1tlE196YgFPfbAFcz+Tq99+1O24DBcLdi1gcfHibn++iIiIiIiIyKFSUUqkO466Euuyv2NmjMYI1sH7D8WKU/N/DoE9I5ZGTc9l5NFZWDjYePJ3SL7+Rgy3m8aPP2brhRdRfP/9RKqqut2M6UPTePOOEzlzfA7hqMVP31zLDX9ZSm1zuNPt85Py+fLoLwPw62W/xrQO/i5+IiIiIiIiIj1JRSmR7hpzDsHr5mFd8gxkjokVo+b/DB6ZBO//AgJ1AMy6fDTxyR5qywNsHHwOQ9/4N4lnnAGmSc1zz7N5zplUPv0MVijUrWak+D088dWp/PSiCXhcDt5bV8YFj33E+pL6Trf/+qSv43f5WVO5hre3v93t+CIiIiIiIiKHQkUpkUNhOGD8RXDzx3DJ05AxGgI1MO8n8MhE+OAX+FwBTr16LACr39/F269WEf/9n1Hwlz/jHTcWs76esocfZvN551E/d2635psyDIMrZwzhHzcfx6CUOLZVNnHR/y3gjZXFe22bHpfONeOvAeDRTx8lbHY+qkpERERERESkN6koJdITHE6YcDHcshAu/iOkj4wVp96LFacKKv/E8Rfm43AZFH1exfM/XsLKXWkM/uvz5P70JzgzMghvL2LnrbdRdO11BNav71YzJgxK5l+3n8DxI9JpCkW59dlPefA/a4lEO16md/X4q0nzpbGjfgevbHilB74AERERERERka5RUUqkJzmcMPESuHUxfOkpSB8BzdUw9wGmrDiDy89cyZBRcZhRi0//W8SzDyyhrOB4hv3nP6TfdBOGx0PTokVsvehLFN97H5HKyi43IS3ew5+vnc5Ns4YB8Lv3t3DNM59Q1bjn8kC/28/Nk28G4FfLfsVLG146pDsCioiIiIiIiHSVilIivcHhhEmXwi2L4aLfQ9pwaK4iZem9nFt3JucU/pWk+ACNtSHe+ePn/Ov3G3BceiPD3nyTxLPOjM039eKLbDr9DIquv4Gy3/yG+vfeI1JeflAf73I6+P7ZY/nt5UcR53by0aYKzvvtR6zetWcS9otHXczM3Jk0R5p5YOED3Dr3VsqbDu74IiIiIiIiIofKsGw2PKKuro7k5GRqa2tJSkrq8eNblkUgEMDn82EYRo8f/3Blx9xdyhyNwOpXYMVzsO1DMCNELDfLGy9gWeOXiVgeDAMmnDSIGecPI/r5CkoffIjAmjV7HcqVm0vcxInETZqIb+IkfOPH40yI3+dHryup4+t/Xcb2yia8LgcPfmkiX5o6GADTMvnr53/l0U8fJWSGSPGm8MNjf8gZhWcceuYBxI65ldkemcGeubuaubf7DkcK9aF6njLbIzPYM7cy2yMz2DO3Mvdc/0lFqR5mxx8n2DN3tzM3VcGG/8Laf8HmudQHE1hQdy2bg8cBEOcJcuypPsacNZPgxs0EVq2kedVqAqtWEty0Gb74n6xh4Bk+jLiJk4ibNJHE00/HlZHRYZPapjDffOEz5q2PjYS65rhCfnD2GNyV68CfzsZIPd//6Pusq1oHwDnDzuH7M75PkqfjfyN2PM9gz9zKbI/MYM/cKkp1j/pQPU+Z7ZEZ7Jlbme2RGeyZW5lVlOo2dah6hx1z90jmYANsehfW/ZsdK3bwYeUVVEfzAcjybmHWUVvInjYVhp4ECZlEGxoJrFlDYPUqmleuonnVSiK7O95hz/D5SL3iCtJvuB5XWlrbetO0+M07a1k0/03OdC7hPM8yMswKcLhh5i2Ej/8mT6z7G39c/UdMyyTbn82Pj/8xM/Nm9mzmI5AdcyuzPTKDPXOrKNU96kP1PGW2R2awZ25ltkdmsGduZVZRqtvUoeoddszd45kjQaKbPmDVW5+zZP0IwlYcYDImbh7HJvyd+LzBMOwkGHYKDJkJntgle5GKCppXrSKwahUNH3xIYPVqAAy/n7QrryTt6q/iqlkJn78O696Apoq2jwzixks49iIhB05/gOW5o/nBRz+gqL4IgCvGXME3p32TOFecLc8z6PetzAObHXOrKNU96kP1PGW2R2awZ25ltkdmsGduZVZRqtvUoeoddszdm5kbqxpZ+PdPWL/GBMBlNDMt/h9MiX8dlxGKjW7KnwHDTo498o4CpwvLsmj88EPKf/MbAms+B8Dhskgd1UD6mAacHgt8KdQVnsHD20fyUvVIjneu4Rfxz5Ee2hn78MHTaZrzY361611eWP8CAIVJhTx44oOMTx9vu/MM+n0r88Bmx9wqSnWP+lA9T5ntkRnsmVuZ7ZEZ7JlbmVWU6jZ1qHqHHXP3ReaSrbV89OJGSrfWAZDga+K41BcZYb5Gh4/0JsPQE2HwMbD7U6wN79CwPUr5qiSCNW4AHD4XaRefQdrtP8CZkkZDMMIPX13NPz/bhYcwt8W9zS2Of+CKNgMGTL2Kj8afyb3Lfkl5czlOw8mNE2/kqlFXkeBPsM15Bv2+lXlgs2NuFaW6R32onqfM9sgM9sytzPbIDPbMrcwqSnWbOlS9w465+yqzZVlsXFrKwn9spqE6CEBOgZcTpmwhu/5t2Po+BGr33jE5H2vMudRXD6bi+bcJbtgIgCMpifRrryH1a1/DmZDAx5sruPe1NWwqayCbKn6Z8jInBubHjuFLpnbWt/lJqIi3tr8NwPDk4Zw+5HSm505ncuZkPE5Pr2U/XOj3rcwDmR1zqyjVPepD9TxltkdmsGduZbZHZrBnbmVWUarb1KHqHXbM3deZw6EoK94tYtlb24mEYpf1jZ6Rw7HnF5IQWAtb5sPOZZA5CsaeH7ukr6VdlmlS//bblD/2GKFNmwFwJieTdv31pF5xBVFfHE8v2MqjczfSFIoy3bme3yQ9S25zrJBF1jj+M/USfrz1FepD9W1t8jq9TMmawvSc6UzPmc74jPG4He5e/y76mn7fyjyQ2TG3ilLdoz5Uz1Nme2QGe+ZWZntkBnvmVmYVpbpNHareYcfc/ZW5oTrI4tc2s25RCQAuj4OjzhjCUWcU4PY497uvFY1S95+3qHjsMULbtgFguN3EHXUU8TOPpWniVH6+1eCNNeU4MLnR/wHfdr6AJxwbiVUx9mzeHnY8nzVtZ0nJEqoCVR2O73f5OSr7KGbkzGB6znTGpI3B6dh/mw6FZVlsrNmIz+mjIKmgVz9Hv++Bz46ZwZ65VZTqHvWhep4y2yMz2DO3MtsjM9gztzKrKNVt6lD1Djvm7u/MZdvr+OjFjRRvjhWMElK9HHvhcEYdk43h2H97rEiEujfeoOKJJ9uKU60cCQk0j5vMa4485vqHUJsYz8Pp/+aM5jcwLBPLFQcnfw+OvYUtDTtYUrKEJcVL+KT0E2qDHS8jTHQnMi1nGicNPomT808mIy6jR7LvbtjNG1ve4F9b/sXW2q0AnD7kdG6ZfAsjUkf0yGe019/nuj8osz0ygz1zqyjVPepD9TxltkdmsGduZbZHZrBnbmVWUarb1KHqHXbMfThktiyLTcvKWPiPzdRXBQDwxLnIHppEztAkcoYlkz00Ca+/80vqLMsivH07jQsX0rhwEY2LF2PWdiwsVfmS+CxjBOXZaVw7ajmF1srYG9kT4fzfwKBpAJiWyYbqDbECVcknLC1dSkO4oe04BgaTMydzSsEpnJp/KoXJhV3K2hBq4J3t7/D65tdZWrq0bb3X6SUUDWFhYWBw5tAzuWXyLV0+/v4cDue6rymzPTKDPXOrKNU96kP1PGW2R2awZ25ltkdmsGduZVZRqtvUoeoddsx9OGWOhKOsmLuDT/9bRKg5stf7qbnx5AyLFalyhiaTmuPvdDSVFY0SWLuOxoUf07RwEU3LlmEFgx22qUtKYHBBFemDa/AkR3Ec+3U49R7wJnZskxlhXdU6FuxawLwd81hTuabD+8OSh3Fqwamckn8KEzIm4DAce+cyI3y8+2P+tflfzNsxj2B0T1um50zn3GHncvqQ0yluLOb/lv8f7xa9C4DDcHDesPP4xuRvMDhx8MF/kftwOJ3rvqLM9sgM9sytolT3qA/V85TZHpnBnrmV2R6ZwZ65lVlFqW5Th6p32DH34ZjZjJpU7mqkZEstJVtrKdlSR115817bef0usguTyB6WTM7QJLIKk/DF7z2aygwGaf5sOY0LF1I87wNcG9fhaPdHhicxTGJ+AOfwOKrO+zFDj78Ul3Pv4hJASWMJ83fM572i9/ik5BMi1p7iWWZcJqfkn8KpBadyTM4xbKzZyL83/5s3t77ZYd6qYcnDOG/4eZwz9BxyE3L3+oy1lWt5fPnjvL/zfQBchosLR17I1yd9nZz4nIP+Hr/ocDzXvU2Z7ZEZ7JlbRanuUR+q5ymzPTKDPXMrsz0ygz1zK7OKUt2mDlXvsGPuIyVzU12I0pYCVcmWWsq217Xdva+9lGx/rFA1NPZIH5yA8wsFpqbKapa/9C+a351L5tpPcUf3FJY8iWEq8tOYP/MbDJl1GrNGZ5GXEtdpm+pCdXy08yPe2/EeH+78kKZIU9t7boebsBlue53mS+PsoWdz7vBzGZc27qC+65XlK3l8+eN8vPvjtmN+edSXuWHiDWT6Mw+4/xcdKee6JymzPTKDPXOrKNU96kP1PGW2R2awZ25ltkdmsGduZVZRqtvUoeoddsx9pGaORk2qWkdTbamlZGvno6mcbgeZ+YltRarswiQS0rwEg0F8Ph9mYyO73nyH4ldfI3HFEozonj9KahPieTPvWLZNOo4R0yczYXAyo3MSGZ6ZgM/d8W58oWiIxcWLmbdjHvN2zKOiuQKv08sp+adw3vDzmJk3E7ej8zmxDmRZ6TIe++yxtjmovE4vXxn9Fa6beB1pvrSDPs6Req4PhTLbIzPYM7eKUt2jPlTPU2Z7ZAZ75lZme2QGe+ZW5gFWlHr88cf5xS9+QUlJCZMnT+a3v/0t06dPP+B+zz//PJdffjkXXHABr7766kF9ljpUvcOOuQdS5uaGEKVb6yjdVkdZyzLYtPfcVHGJbtIHx5M+KJG0nHhScvyk5cTjJkjdP/9C7bO/I7A9iGXu+T52JGQyf/BRzBs8ldLEDAoz4hmdncio7ETG5CQyKieRIWl+XE4HpmWyrXYbmf5MEj2Je31+d1iWxeKSxTz22WOsKF8BgN/l5+4Zd3PB8AsO6twNpHN9sJTZHpnBnrkHSlGqL/tPoD5Ub1Bme2QGe+ZWZntkBnvmVuae6z+5erKR3fHCCy9w55138uSTTzJjxgweeeQR5syZw/r168nKytrnftu2beOuu+7ixBNP7MPWigxMcQkeCidmUDgxA4j9gVNb1kzp1tq2YlXFzgaa68PsXFvDzrU1Hfb3JbhJzZlB6hUnk9z4Gd5lz+PcUYJV1EB+QzlfW/c2X1v3NutS85k3eCofDJrCf3x7ik4el4MRmQmMzokVqyYPDjI5P45476H/EWUYBsfmHsuMnBl8tOsjHlv+GJ9Xfs4PF/yQj3d9zD0z7yHJc/j8JVNE5GCo/yQiIiIDQb+PlJoxYwbHHHMMjz32GACmaZKfn8/tt9/O9773vU73iUajzJo1i+uuu44PP/yQmpoajZTqZ3bMbbfMkXCU8qJ6SrZW01AZpqa0ieqSJuqrAvvcx0GYJHM32VXLydi4GG+wFgDLcLBr+AQ+LpzGqwmjqGbvy/OcDoOxuYkcPSSNaUNSmTYkdZ9zVHVF1Izy9OqneXz540StKHnxefx81s+ZkjVln/vY7VyDMtslM9gz90AYKdXX/SdQH6o3KLM9MoM9cyuzPTKDPXMr8wAZKRUKhVi2bBl333132zqHw8Hs2bNZuHDhPvd74IEHyMrK4vrrr+fDDz/si6aK2J7L7SRnWDIped4OfxCFQ1FqSpuoKWmiuqSR6pZiVU1pE9GwmxrHEGoyhrAh41zywsvJLVlG4uaVDN60kks3reQynw/H8bMonX4yK3NH83lFgM+2V7O7NsDqXXWs3lXHnz7eBkBeso9phWlMK0jh6MI0xuQk7vNuf/vidDi5cdKNTM+dznc/+C67GnZxzVvX8I3J3+DGiTfidDgPfBARkX6k/pOIiIgMFP1alKqoqCAajZKdnd1hfXZ2NuvWret0n48++og//vGPLF++/KA+IxgMEgwG217X1dUBsSpfbwwSaz3uYTBVV5+yY25ljnG5HWQMTiBjcELHbU2L+uoAuzfUsPa99RTvdLLLPY1d+dOILyinsGkhGUWrcRbvIjr3bTLmvs3s5GS+dOYcEufMoWbE0XxW3MDSbdV8WlTN58X17K4NsHvFbv61YjcAfo+TKfkpzBiaxqxRmUwclIzTcXD/UjEpYxIvnvsiP138U97c+iaPL3+cRcWLePCEB8mJzzlg7oFOme3Djrm7mvlw+276ov8E6kP1BWW2DzvmVmb7sGNuZT647Q9Gv88p1RX19fV87Wtf46mnniIjI+Og9nnwwQe5//7791ofCATweDw93UQg1omzyxC+9uyYW5n3zxNvUHhUKoVHHUtNaRPrP9jGpqWVNIYyWRN3Psbocxg8ZjmDmtYQv34tZk0tNS+8SM0LL2LExzNpxgxmzDoR/0UnEEhMZtWuOj4tqmX5zlqW76ijPhjh482VfLy5kl+/u5GUODfHD0/jhBFpHD88jcwE737b58bNfUffxzGZx/CLT3/BstJlXPKvS/jB0T/glMGndDv3QKHM9mHH3F3JHAjs+zLlI0F3+k+gPlRfUWb7sGNuZbYPO+ZW5v072P5Tv84pFQqF8Pv9vPzyy1x44YVt66+++mpqamp47bXXOmy/fPlyjjrqKJzOPZfXmKYJxIatr1+/nuHDh3fYp7N/5cvPz6empkbzIfQgO+ZW5u5ljoSjbP2sjDXvfM7unXsuvYt3VDLcWERu0y7MtVuJ1tR12M83cSIJJ80i4eST8Y4di4XBhrJ6lm2v5qNNlSzYWEF9sOMdA8fnJXHSqExmjcpkakEK7v1c6ldUV8T3PvweqytXA3DJyEv4f8f8P+JccTrXyjyg2TF3d+ZESElJOWzmlOqL/hOoD9UXlNkemcGeuZXZHpnBnrmVuef6T4fFROfTp0/nt7/9LRDrJBUUFHDbbbftNVFnIBBg06ZNHdbdc8891NfX85vf/IZRo0Yd8F/uNEln77BjbmU+9Mw1pU18PncdaxeVEwi1TnZukunaQrq5maSmMuKKd+PYXkT7T3NmZpAwK1agip95HM6EeMJRk+U7anh/fTnvbyhn1a7aDp+V4HVx/Ih0Zo3K5KRRmQxO9e/VnnA0zGPLH+OZ1c9gYTEseRgPz3qYUamjdK5twI6ZwZ65B8pE533ZfwL1oXqDMtsjM9gztzLbIzPYM7cyD5CJzgHuvPNOrr76ao4++mimT5/OI488QmNjI9deey0AV111FYMGDeLBBx/E5/MxYcKEDvunpKQA7LVeRA5/Kdl+jrtiKjO+bLLls2I+f2c1O3e4KY+MoJwR4AOGgndYPenhzSTVbsVXvJPE6u1EX/kHta/8A1xOEk48gYxv3MwxkydzTGEad80ZTUVDkA83lvP++nI+2FhBVWOI/64p5b9rSgEYnZ3IqWOzOHVMFkflp+ByOnA73Xxr2reYmTeT73/4fbbUbuHyNy7nzml3clHhRf37ZYmItKP+k4iIiAwE/V6UuuyyyygvL+fee++lpKSEKVOm8NZbb7VN3llUVITD0bW7a4nIkcXpdjBy+iBGTh9EQ3WQki21lKwvpnRTJeUlJkEzkd2uKexOnwLpsX0Sw7tJrN1GYuV2kpdspH7e5SSMG0TGN76B//QvkZHg5aKjBnPRUYMxTYvVu2t5f3058zeU81lRNetL61lfWs8T8zeT4ndz8qhMThmTxcmjsjg291hePv9l7ltwH/N3zufnn/ycZ1Y/w1lDz+LsYWczNm2sbf5FREQOT+o/iYiIyEDQ75fv9TUNPe8ddsytzH2TORoxqdjZQOnWWkq31FCyuYq6quhe27nCDaTUbCK1dhPZzo0MPT2HxLMvxRh+MngTO2xb0xTi/Q3lvLeujPnry6ltDre953QYTBuSyqljsjh1dCZLq//NY589Rn24vm2bwqRCzh56NmcNPYvC5MLeit6v9Pu2R2awZ+6BcPlef1Afqucpsz0ygz1zK7M9MoM9cytzz/WfVJTqYXb8cYI9cytz/2Vurg9RurWOkq21lG6po2RLDZFwxz/KXJEm0po2kp+4hqETTDKnTMYxajZkjYN2bY9ETT7bUcPctWXMW1fG+tL6DscZnBrHSaNTSEndyM7oIhbs/pBgdM/Ev+PSx3H20LM5s/BMsuM73p79SHa4nOu+ZMfMYM/cKkp1j/pQPU+Z7ZEZ7Jlbme2RGeyZW5lVlOo2dah6hx1zK/PhkzkaNSnfXs/ujTXsXLmb4s11RHB32MZtNZHrXcugpB0MHp1M1tEzYNjJ4E3osN2OqibmrS/jvXVlfLy5klDE3HMMp8FRQ+LIzdtMJYtYXbWUqBUbtWVgcHTO0Zw19CxOLzidFF9Kb8fuVYfrue5NdswM9sytolT3qA/V85TZHpnBnrmV2R6ZwZ65lVlFqW5Th6p32DG3Mh++mc2oSenqnWx6cT67N9VSkziUiKvj3fZy3Z8zLelVCkYnYoyaA6POgLRhHbZpCkX4eFMl760r5f0N5eyqCXR4Pz0pxNAhm2n2fEJR0+dt612GiwkZExidNpqxaWMZkzaGEakj8Dq9vRe6hx0p57on2TEz2DO3ilLdoz5Uz1Nme2QGe+ZWZntkBnvmVuYBdPc9EZGe5nA6yJ1cQO7kq4hUV1P5l79S9Mp7VHvyqE4eQWX6BIrD4/h35TjSFmxh6qevMDLpeziyRsLIM2DUHCiYid/jZva4bE4bm0VzczMljSYfbqzggw3lLNxSSWWdh8pVY4GxGK5qBg1ej5H4GTWR7SwvX87y8uVtbXIaToalDGNM6hjGpMUeo9NGk+xN7rfvSUREREREpD9ppFQPs2PFFOyZW5mPrMzRujqqn32Wqj/9maYmi6L809iVdwJmy+il+KZiRla9wVBjAXEpIXyZHrxTT8Qx4WysEbMJOBM75A5FTJZtr+bDjeV8sLGc1bvq2j7LcFfgS9hJXlY1Hn8xNdFtNITrOm1XXnweY9LGMCVrCmcNPYuc+Jze/zIOwpF8rrvLjpnBnrk1Uqp71Ifqecpsj8xgz9zKbI/MYM/cyqzL97pNHareYcfcynxkZjabmmhatozg+vXUrt3ChuJEtvsntl3e52uuYEjRO+SWLMJhhfEkRvGmhPENScU/dSpxx83BGHYiJOV2OG5FQ5AFmyp4f0M5H26soLw+2O5di7SkJkYPaSAttYyIaxfb6zeyq2FXh2MYGByTcwznDjuX2UNmk+jpeNfAvjQQznVX2TEz2DO3ilLdoz5Uz1Nme2QGe+ZWZntkBnvmVmYVpbpNHareYcfcyjxwMjfXNrHytTWsWlpLMOQAwBuqJb/oHQbtXoDTDLVtazhN4tLD+PP9xE2eQNzxp+Mcc3JsPqqW78SyLDaUNvDRpgo+2ljO4q1VNIWiHT5zaEY804fHUZBTgytuN4tKPmBp6dK29z0ODyfnn8y5w87lhEEn4HZ2nLi9tw3Uc70/dswM9sytolT3qA/V85TZHpnBnrmV2R6ZwZ65lVlFqW5Th6p32DG3Mg+8zOFQlM8/2s3yd4poqI6NdPJ6YVRyMYM2/ZPI2g2YzeGOOxkW3pQw/lw3/gkjiTvuFNxTzoCsceCIFbhCEZPPiqpZsKmCjzZVsGJnLVFzzx+9DgMm56dwzHADK+FTPil/l821m9veT/GmMKdwDucOO5fJmZP75Lsf6Oe6M3bMDPbMraJU96gP1fOU2R6ZwZ65ldkemcGeuZVZRaluU4eqd9gxtzIP3MzRsMn6xSUs++926sqbAfAluJl+biEjcpsJfLKQ5gVzaVr5OeHKxr32d8dHSBpukXHO0ThGnwRDjofsCW1FqrpAmEWbK1mwqYIPN1WwpbzjMTITPUwb0Ywz6VPW1L1PZaCi7b3BCYM5d/i5nFV4FoXJhTgMR698B3Y51+3ZMTPYM7eKUt2jPlTPU2Z7ZAZ75lZme2QGe+ZWZhWluk0dqt5hx9zKPPAzm1GTTcvKWPLvrdSWxYpTqbnxHPel4QyZkI5hGIRLS2lesoimD9+mefkKAjsqoeVPVVdclOyjaknMD2DEpcSKU4UnQGFrkcoJwO6aZuavL+e9dWUs2FRBc3jPpX5up8W4YWV4U5aztXkRgWhz23t+l58RqSMYmTKSkakjGZU6ihEpI0j1pR5ydruda7BnZrBnbhWlukd9qJ6nzPbIDPbMrcz2yAz2zK3MKkp1mzpUvcOOuZXZHpkBIpEoK+ZuZ/nbuwg0xi7fGzwmleMvGUHG4I6TkUcbGml8fx5lv3iYcEk5AP7sCNlHVeFLiezZ0Jfcrkh1QluRKhCOsmRrFe+tK2Pe+jK2Vzbt2ccIkZe3mfj0FVRE1hCxvnApYYuMuIy2QlXrY3jycHwu30FntuO5tmNmsGduFaW6R32onqfM9sgM9sytzPbIDPbMrcwqSnWbOlS9w465ldkemWFPbsN0suytIlbO24EZscCAMTNzOfb8YcSneDvsYwYCVP7xj1T+/imsYBAcDlJPGUfm1CjO0iUQqu/4Ib4UGDoLhp8Cw06BtKFYlsWWikbmrSvjvXVlLNlaRaRtLqooCQnVjB3SRFZGFVFnMdvqN+11R79WBgZ5CXkUJBYwJGkIQ5KGUJBUQGFSIXkJebgcrk4z2+lc2zEz2DO3ilLdoz5Uz1Nme2QGe+ZWZntkBnvmVmYVpbpNHareYcfcymyPzLB37rqKZha+uplNS8sAcHkcHHV6AUedMQS319lh39DOXZT9/OfUv/MOAM60NLK+9U2Sjy3EKPoYti2A7R/vXaRKLYRhJ8cKVENngT+N+kCYjzZWtI2iqmjYc1dAhwFHD0lj1phEhuU10MRONlZvZGPNRjZWb6QmWLPPfC7DxaDEQR0LVokFDIsfRnZytm3OtX7f9smtolT3qA/V85TZHpnBnrmV2R6ZwZ65lVlFqW5Th6p32DG3MtsjM+w7d8mWWha8vJGSLXUA+JM9zDh/GGNm5uJwdPx+GhYsoPSnPyO0ZQsAvkmTyPnhPcRNnIgVDkPxZxhb58PmebBzCZjtLvUzHJA7Zc8oqvwZmA43K3bW8O7aUuauLWNdScei1rCMeE4bm8XssdlMLUihNlzN9rrtFNUVsb1ue+xRv50ddTsIRAOd5nYaTo7KOoqT80/m1PxTyU/KP/Qv8zCm37d9cqso1T3qQ/U8ZbZHZrBnbmW2R2awZ25lVlGq29Sh6h12zK3M9sgM+89tWRabPy1n4T83UVcRK+6kD0ogf2wqkZBJOBQlEozGloEIzcXlBCtriBpuTKeHqDsOEycOl0Fimo+kjDiSUpwkOYpJDKwmqXoBSfWL8Rn1tH202x+bh2rMOTDmXIjPYEdVE3PXljJ3XRmLtlQSju75oz3F7+bkUZlMHZLK2NwkxuQkkuhzA2BaJmVNZRTVFbGtblusaFW/na21W9let71D1hEpIzg5/2ROyT+FCRkTeu3Of/1Fv2/75FZRqnvUh+p5ymyPzGDP3Mpsj8xgz9zKrKJUt6lD1TvsmFuZ7ZEZDi53NGyy6v2dLH1zG8GmSKfbHAq32yTJW0eiuY0kdpDsLCbTvZUM93bcQ4+GsefD2HMhKY/6QJgPNlQwd20p760vo6Zp7wnRB6fGMTY3ibG5SYzLTWRMThIFaf62EV6WZbG5YjOLyhcxf8d8lpYuJWrtuStgRlwGJw0+iVMLTmVG7gy8Tu9en3Gk0e/bPrlVlOoe9aF6njLbIzPYM7cy2yMz2DO3Mqso1W3qUPUOO+ZWZntkhq7lDjSGWf3BLoJNEdweBy6vE7fHicvjxO114vI4Yq+9TiKb1lP9xGNEN3xO1Okh4Eun2ZcRW8altywzCHmS9/l5BlFSXLvIcm0m072ZrDwPGVOPxj3pXEgtJBI1+bSohg82lPN5cR1ri+soru38cr14j5PROYmMyU1ibE4i43P8TBmSgcPhoDZYy0e7PmLejnl8tOsjGsONbfvFueI4Lu84Thx0IhMyJjA8ZfheE6cfCfT7tk9uFaW6R32onqfM9sgM9sytzPbIDPbMrcwqSnWbOlS9w465ldkemaF3c1vRKLWvvkbz8s+IVFYRqawgWllFpKoKq6kJgKjDTcCX1lK0ihWrmvzZ1CUWEPKm7HXMtkJVUhWZQ9PJOmoqGRPHtU3CXt0YYm1JHeuK61lbXMfakjo2lDYQiph7HSsjwcOsUZmcNCqTE0ZkkJ7gJRQN8UnJJ8zbMY95O+ZR1lTWYR+f08fotNGMTx/PhIwJjE8fz5CkITgdzr2OfzjR79s+uVWU6h71oXqeMtsjM9gztzLbIzPYM7cyqyjVbepQ9Q475lZme2SG/sttNjURqaomWlVJpLKSaFUVkcoqopWVBLdtpXnpMprDTuoTC6hPKKA+MZ/6pCEEOxlZZWCSl1LK8Pxqho2IEp+VDgk5kJAFiTlEfGlsrWxuGU1Vz5rdtSzdVkVzeE+hyjBgQl4ys0ZlMGtkbI4ql8Pg86rPmVc0j0/LPuXzys87jKJq5Xf5GZs+tkOhKj8x/7D6Hen3bZ/cKkp1j/pQPU+Z7ZEZ7Jlbme2RGeyZW5l7rv905F1fISJiEw6/H4/fD4MHdfq+FYkQWLOGxsVLaFq8mKZP52E1NxP0JO0pVKUOoSFhMAFXCrtqctlVk8sHq0xy3esZ7vsPw3wLSXRW4jKcjIzPZGRiNhckZGNlZBMYWshG33j+W53De5saWFtcx6pdtazaVcvj8zaT4HUxc3g6s0Zlcv7I67jtqNswLZPtddtZXbGazys/Z03lGtZVraMp0sSy0mUsK13W1v4kTxLTc6YzM28mx+Udx+DEwX311YqIiIiIyGFAI6V6mB0rpmDP3Mpsj8xw5OS2QiGaV6+mafFiGhctpvmzz7BCIQCafemUZ0ymIm8aNf7CDvtluzcw3Pcxw70LSXKV7X1ghwtyJtKUdRSrjdG8VZvPq9vcVH1hAvW2ydNb5qUak5PIkPR4LKJsrd3Kmso1rKlYw+eVn7Ouah0hM9Rh//zEfI7LO46ZeTOZnjOdRE9ij34/B3KknOeeZsfcGinVPepD9TxltkdmsGduZbZHZrBnbmXW5Xvdpg5V77BjbmW2R2Y4cnObwSDNy1fEilSLF9O8YgVEIgS8KZRnTKEs8yhqk4fHrstrkZllMXxILcOytpNUvwTH7mUYDaV7HdvyZ1CfMYU1jtG8XVvAK6VZ1Jl734HP53YwKjuRMTmxO/yNabnTX2KcwdrKtSzcvZCPd3/MyvKVRKw9dy10Gk4mZkxsK1JNyJjQ65OnH6nn+VDZMbeKUt2jPlTPU2Z7ZAZ75lZme2QGe+ZWZhWluk0dqt5hx9zKbI/MMHByRxsaaV62lMaPF9K4cCHBDRsIepIoz5hMeeZRVKeMBMPRtn1Kto+hk7MYOixKtmM1jl2fwM5PoHgFmB1HSVmGg4a0iazKOo//cCIryyOsL60nEN578nSArEQv4/OSOHtiLmdPzMUyAnxS8gkLixeycPdCttVt67B9gjuB6TnTGZ8xnmHJwxiWMoz8xHzcDnePfT8D5Tx3lR1zqyjVPepD9TxltkdmsGduZbZHZrBnbmVWUarb1KHqHXbMrcz2yAwDN3ekvJzGRYtpXLiQxo8/pqmygfKMSS0FqtFY7e6W57YCZDvLGZTcQG6Wid/XhMsswxnYhqt+La7Ibhyulv+deBJg4iVEp17Dds9I1pXUxx7FdawrqaeoqgkscACmAX6Pk3Mm5nLJtMFMH5qGYRjsbtjdNopqUfEi6kJ1e7XfZbgoSCpoK1INS449CpMLiXPFdfn7GKjn+UDsmFtFqe5RH6rnKbM9MoM9cyuzPTKDPXMrs4pS3aYOVe+wY25ltkdmsEduy7IIbdtG48KFNC1cSM3SlZS786lMn0hl2ngibn/btoYZJaV2IxmVq0mvWIU/UAGAI96LLzmMN74Ob0oEb3IYc8R06kZ+jdqkmdRWRqkpa6a6rInasiYiIZMKH6wixEaPSZ3DYki6n0umDuZL0wYzKCVWWIqaUT6v/JwlJUvYVLOJLbVb2Fq7leZIc6dZDAzyEvIYljyM4SnDOywTPAn7/Q4G+nnujB1zqyjVPepD9TxltkdmsGduZbZHZrBnbmVWUarb1KHqHXbMrcz2yAz2zG2aJk3l5bgaGgiXV1CysZqirSF2VbioD/k6bOtvLiWjfCX+plKa49JpjsuiKS6T5rhMol0YsVTmMlnnirLBHaXGZXH88AwumTaYOeNziPM4O2xrWialjaVsrt3MlpotbKnd86gN1u7zM7L92W0FqmEpwxiePJzhKcNJ9ibb8jyDPX/fKkp1j/pQPU+Z7ZEZ7Jlbme2RGeyZW5l7rv/Uu7PGiojIEckwDJxJSXizsvANH07isTCy5b2a0ia2rapg26oKijfW0hSXTVHB6Z0fyDLxBauJay7H31ROXHMZcc3lxEer8RXkU5ExkRLnECrCyWRFHGRFHMwKuKkhzPqVVfxsXSU/9K/m3Cl5XDJtECOzE4lzO3E7HeQm5JKbkMsJg07Y83GWRVWgKlagailWtRauypvLKW0qpbSplI93f9yhmem+dIalDGNI/BCGpg6lMLmQIUlDyEvI69F5q0REREREZA8VpUREpEtSsv1MyS5gyuwCgk1hij6vYtuqCppqQyRnxpGc5SclK47kTD9+RxPRzRsJLFpJcOl8ghXFBGtdWKYBNTvJYiFZQMidSHnGRMozplCdOpoUh5sZIZgRAndFFZ41b/DJH3bzV38ci3LHU56YQZzbic/tJM7jIM7tJM7txNuyjHM7ifOMJi9lMl/JT2XKcSm43QG21m5lc83mDsWq4sZiKgOVVJZU8gmfdMjqMlwMShxEQWIBQ5KGMCRpCAVJBRQmFZITn4Oj3cTwIiIiIiLSNSpKiYhIt3n9bkYenc3Io7P3sUU85GQSf/xxsZcNZVhL/0Jo3p8J7ioj2uwgEnASCTSSFihj+O75BLbHU+KfQnn6FCrTxhH2pRH2TQdgfLCGk7etILXxVUJJVWzKzWNtSiElpFFMGmVWKkE8nbYkPy2Oo/JTmZI/ldkFp/I/RyXhdTlpDDeytXYrm2o2salyE7uadlFUX0RRXRGBaIDtddvZXredD3d92OF4HoeHgqQCJmdO5uicozk6+2hy4nN66qsVERERERnwNKdUD7PjtaVgz9zKbI/MYM/cvZ7ZNKFqMzSWQ1Nlu0cVNFViNVZg1lTQXN5AUWUB24JT2OmYStjYM+G6O1RPRuVKsms/Y4h/OcmDGojPDhL2p9LozaLOk8V2cvmwqYC3awZRZGUBe7J4nA7G5SUxJT+FowpSOCo/hYw4g7i4OAzDwLRMyprK2opSRXVFsef129lRv4OIGdkr1uCEwUzLntZWpBqUMOiw/83o9605pQ6W+lA9T5ntkRnsmVuZ7ZEZ7JlbmTWnlIiIHMkcDsgYGXt0wgCcQAIwDhgXCRGtK2PHyp1s/rSSrZudBD2JFOceT3Hu8ayKNJNRsYqs9Z+R7/mM1LwiBudtpNBnchJwjxfCnhR2x49lpTmMufWDWdBUwPIdJst31PCnlimm0vxuJg1OYdLg5JZlCjNyc5iRO6ND+yJmhOLGYjZWb+TT0k9ZWrqUtVVr2dmwk50NO3lt82sA5MTnxIpU2bEi1ZCkIbbpuIiIiIiIHIhGSvUwO1ZMwZ65ldkemcGeuQ/3zGbUZPfGGjYvK2Xz0mKam/e854iGSK9aQ2bFcnKM7fjdtbhddbh8YVz+KO44s2UZJZSSyXbfGD6JFDK3dhCrI/lUk4jJnrmicpJ8TByczOTByUwcnMKkQcmkxu99iWBDqIHl5ctZWrKUpaVLWVOxhojVcTRVRlwG2f5svE4vca44vE4vXpcXn9OHz+XD5/ThdXk7vJ/iTSE3IZdB8YNI9ib3+Pk43M91b9BIqe5RH6rnKbM9MoM9cyuzPTKDPXMrs0ZKiYiIjTmcDgaPSWPwmDRmXT6Gkq11bP6sjM1LdtNQ56E88yjKM4/ic8ATrCW+cTcJjbuJr25ZNhbjNMNgWLji1jIrbg2nthSqXPFRrAQPTX4fFV4/1U2JVG9MonpDAoutBN4iEWd8OhmZueQOyictfzRDslIpSPNzwqAT2u4G2BRuYmXFyrYi1aryVVQ0V1DRXNHt3HGuOPLi88hLiD1y43MZlDCI3IRc8uLzSI9L1+TrIiIiInLEUFFKRESOaIbDIHd4MrnDkzn+4hFU7Ghg82dlbFlaQnV5kJA3mZA3meq0sXt2siziAuXEN8SKVAmNu4mv3E1cczkOy2zbLI5m/M4mCv27cfujuPwR3PFR3P4o7h1R3OuiWH7YYgziDauAXZ7h1KWMwcqeQGb2IArTh3BazjiuG/8NnM4oayvXUheqoznSTDAaJBAJtC0D0QDBSJBANNBhfVWgit2Nu6lorqA50szm2s1srt3c6XfhcXjIS8hjUOIg8hPyyU/c8xiUOIg4V1xvnw4RERERkYOmopSIiAwYhmGQWZBIZkEix14wnFAgQlVxI1W7G6nc1UDlrkaqdjfQXB+mOS6L5rgsKjKntDuChZcg3kg97uZq3I3VeEK1eEJ1eEL1eKrr8JbW4QnV4Yo0xaZNNyzc/gDHJHzOcYmr8CRE8CREqYv3s8E/mHnOQtaaBZT4R+JIH0F2SiKZiRlkJnrJTPBSkOglM8VLZqKXtHgPTkfnw6GD0SDFDcXsbtxNcUMxuxp2UdxYzO6G3exu3E1ZUxkhM8S2um1sq9vW6TEy4zLJT8xncOLgPcWqhEHEG/FkObJI9CRqpJWIiIiI9BkVpUREZMDy+FzkDE0mZ2hyh/VNdSEqdzdQtauRyt0txariRiLBKEF8BF0+SMyExH0f27CieEL1+AKVeAPV+IJV+Jpr8FZX4QtW4wtUkR2pYLCvlLMTF+BJiGLEW4TjXJheB1GPg4jHScTrIuRxss1wsxEXhtONw+XB4fLicnuI+NJoSBpBMG00ZsZo4pJGUBg3hskpbpJ8bpLi3PjcTsJmmNLGUnY17GJn/U521O9oe+ys30l9uJ7y5nLKm8v5tOzTTjM5DAdJniSSvckke5Jjy9bHF16neFPalgnuBNvMpyAiIiIiPUdFKRERsR1/kgd/Uhr5Y9La1plRk+ryesyQQVN9mOa6EE11IZpqQzTVh2iqC7Y9DzZGsAwnQW8KQW8KJHf+Oc5oEG9gT5HKW1+NqzqAYUVxmJG2pcOK4HSGcbnCuJwhXO4wblcdbneIOM8GEvxv4PVHcMdHqHQns9EaxEprMButwWwwB7HNWQC+VJLiXKT5PeSnDSc/bSLHpsZxaYGf/LQ4fN4guxpixaqdDXuKVrsbdlMdqCYQDWBaJjXBGmqCNV36Pl2GiyRvEinelA7FqtYCVpIniXh3PH6XH7/b3+G53+3H7/LjcqhLIiIiImI36gGKiIgQm5vKn+w5qDuKRMNmrFBVG6KhOkB9VYCGqiD11QEaqmKvm+vDRJ1emuJzaYrPPbS2mVG8dTV4y6qJC1USZ1Yx3KhgsnMjSe6PSfaVEYizKPLnsMWbS8MuP42Wj7X4WIqPJstL0BFHfEIySckppKbmMSZtArMy08kel47LYeDzOQhbTQTMegLRBhrCtdSF6qgN1lIbqqU2WEtNsCb2ut3zQDRAxIpQFaiiKlDV7Yxep7dDoar9HQnb35nQ5/S13ZnQ59rzfEjSEKZkTTmk71lERERE+paKUiIiIl3kdDtITPORmOYje2jnt7iNhKI0VLcvVAVpqA4QCUaJRi3MiEk0ahENRYmGwkSD4djzcJRoxMSMmkSjEDadWA4nAV86AV86tYzovE2hZnz1NfgClQwKV+CPlJEQ3U0ipSQaZXg9AZxeE6fHxNWydHpjjwanlypHEg1GElVWEpVWIlUkUedIpt6RTL0zhUZ3Ls2ucTS7U3B74ynwuUjwuojzm3g8zbhczRjOJnA2ETUaidJAiAaC0XpCVhMhs5lApJnGcCNNkSaaIk00hhuJmBEgNmdWMBqkOljdrXNy/vDzVZQSEREROcKoKCUiItILXB4nKdl+UrL9h3Qc07Roqg3SUB2krqSe2qJy6orraKhspqE+SlPIScjyEHXF0eiKozE+l8pOjuMO1RHXXEFcQyVx5eXEBSrxNVcQF6jAHW7EaYaJd9aT6Kyj0GXhcFk4nBaGs+W5a8/zqNtB0OOmye2j0R1HrSeeancile4kyjyplDlTqDaSqLIyqLaGUks8IVwkx3lIi/eQ6ndTEO8h1e8h2e8gIS6C3xvF543g9YRxusI4HGEwwlhGiLAZIhAJ0Bxt7nCHwvbLUamjDul7FhEREZG+p6KUiIjIYczhMEhI9ZGQ6iNnWDIcN3ivbcKhaGw0VkUTtUWV1BbXU1vRTH1NmPp6i1DEQdiTRNiTRF3ysM4/xwzjCjfhijTjijThjjTFluHY0hVpxhVswt3YhDvciCdUjztcT1KknmTqKaBkz8EMKzYSy2Pi9Fg4PCaGy8J0OYi4nERcTsIuFyGXm6DLTcDlodnlpcnlo8nlo8HpI+RwE3K4iDhcRJ0e8MQmf3d4PDhdHhJdiaR50nG5PXjcbnJcQ3vrFIiIiIhIL1FRSkRE5Ajn9jhJzYknNSceJmTu9X6wKUxdRYC6imZqy5upq4g9aisCNFQGME0L0+Em5E0m5N3HrO37YFhRPNFG3OEGPIE63C3FKk+oIbYMN+BqbsawTAwzGpvc3YotDcvEa0WJMyOkWUEMq6ZlfRSHGQUsOs7uZWE4wHBY0LI0jNiycUQunPzBoXyNIiIiItLHVJQSEREZ4Lx+N5kFbjILEvd6zzRN6msbMaIugs0Rgk0Rgk3hlmXnz5sbwgTqQ4QC0dhdCF1JBF1JEJfX4203zEisoNVarGpX3GotbDmsKJmu7Uzu8U8XERERkd50WBSlHn/8cX7xi19QUlLC5MmT+e1vf8v06dM73fapp57iL3/5C6tXrwZg2rRp/OxnP9vn9iIiIrJvhmHg8bnw+XwkHeCug18UCUcJNIRprg/TXB+KPRpan8eWTfVhwsEoZtTEjFpYphWb6L31ddTCjFqYptXpZ1gOF52/01HexDFdavtAoP6TiIiIHOn6vSj1wgsvcOedd/Lkk08yY8YMHnnkEebMmcP69evJysraa/v58+dz+eWXc9xxx+Hz+fj5z3/OGWecwZo1axg0aFA/JBAREbEnl9tJQqqThFTfIR/LsmIFK7NdkcpsV7zquN7ca5v4ZG8PJDpyqP8kIiIiA4FhWdbB/ANkr5kxYwbHHHMMjz32GBC7jCA/P5/bb7+d733vewfcPxqNkpqaymOPPcZVV111wO3r6upITk6mtraWpKTOb+N9KCzLIhAI4PP5MLr4L85HMjvmVmZ7ZAZ75lZme2QGe+buaube7jt0R1/3n0B9qN6gzPbIDPbMrcz2yAz2zK3MPdd/cvRkI7sqFAqxbNkyZs+e3bbO4XAwe/ZsFi5ceFDHaGpqIhwOk5aW1lvNFBERETlsqP8kIiIiA0W/Xr5XUVFBNBolOzu7w/rs7GzWrVt3UMf47ne/S15eXoeOWXvBYJBgMNj2uq6uDmi5TKAXBom1HrefB6D1OTvmVmb7sGNuZbYPO+buaubD7bvpi/4TqA/VF5TZPuyYW5ntw465lfngtj8Y/T6n1KF46KGHeP7555k/fz4+X+fzWTz44IPcf//9e60PBAJ4PJ5eaVcwGLTNEL727Jhbme3DjrmV2T7smLsrmQOBQC+3pm8dTP8J1IfqK8psH3bMrcz2Ycfcyrx/B9t/6teiVEZGBk6nk9LS0g7rS0tLycnJ2e++v/zlL3nooYd49913mTRp0j63u/vuu7nzzjvbXtfV1ZGfn4/P59tvR6y7WiuHXq/XVj9QO+ZWZntkBnvmVmZ7ZAZ75u5q5lAo1AetOnh90X8C9aH6gjLbIzPYM7cy2yMz2DO3Mvdc/6lfi1Iej4dp06Yxd+5cLrzwQiA2UefcuXO57bbb9rnfww8/zE9/+lP++9//cvTRR+/3M7xeL17v3nfkMQyj1348rce2y4+zlR1zK7N92DG3MtuHHXN3JfPh9r30Rf8J1IfqK8psH3bMrcz2YcfcynzgbQ9Gv1++d+edd3L11Vdz9NFHM336dB555BEaGxu59tprAbjqqqsYNGgQDz74IAA///nPuffee3n22WcpLCykpKQEgISEBBISEvoth4iIiEhfUf9JREREBoJ+L0pddtlllJeXc++991JSUsKUKVN466232ibvLCoqwuHYc5PAJ554glAoxCWXXNLhOPfddx8/+tGP+rLpIiIiIv1C/ScREREZCAzLTtPFE5sPITk5mdraWpKSknr8+JZlEQgE8Pl8thrGZ8fcymyPzGDP3Mpsj8xgz9xdzdzbfYcjhfpQPU+Z7ZEZ7Jlbme2RGeyZW5l7rv/k2Oc7IiIiIiIiIiIivURFKRERERERERER6XMqSomIiIiIiIiISJ9TUUpERERERERERPqcilIiIiIiIiIiItLnVJQSEREREREREZE+p6KUiIiIiIiIiIj0ORWlRERERERERESkz6koJSIiIiIiIiIifU5FKRERERERERER6XMqSomIiIiIiIiISJ9TUUpERERERERERPqcilIiIiIiIiIiItLnVJQSEREREREREZE+p6KUiIiIiIiIiIj0ORWlRERERERERESkz6koJSIiIiIiIiIifU5FKRERERERERER6XMqSomIiIiIiIiISJ9TUUpERERERERERPqcilIiIiIiIiIiItLnVJQSEREREREREZE+p6KUiIiIiIiIiIj0ORWlRERERERERESkz6koJSIiIiIiIiIifU5FKRERERERERER6XMqSomIiIiIiIiISJ9TUUpERERERERERPqcilIiIiIiIiIiItLnVJQSEREREREREZE+p6KUiIiIiIiIiIj0ORWlRERERERERESkz6koJSIiIiIiIiIifU5FKRERERERERER6XMqSomIiIiIiIiISJ9TUUpERERERERERPqcilIiIiIiIiIiItLnVJQSEREREREREZE+p6KUiIiIiIiIiIj0ORWlRERERERERESkz6koJSIiIiIiIiIifU5FKRERERERERER6XMqSomIiIiIiIiISJ87LIpSjz/+OIWFhfh8PmbMmMGSJUv2u/1LL73EmDFj8Pl8TJw4kTfffLOPWioiIiJyeFD/SURERI50/V6UeuGFF7jzzju57777+PTTT5k8eTJz5syhrKys0+0//vhjLr/8cq6//no+++wzLrzwQi688EJWr17dxy0XERER6R/qP4mIiMhAYFiWZfVnA2bMmMExxxzDY489BoBpmuTn53P77bfzve99b6/tL7vsMhobG/n3v//dtu7YY49lypQpPPnkkwf8vLq6OpKTk6mtrSUpKanngrSwLItAIIDP58MwjB4//uHKjrmV2R6ZwZ65ldkemcGeubuaubf7Dt3R1/0nUB+qNyizPTKDPXMrsz0ygz1zK3PP9Z/6daRUKBRi2bJlzJ49u22dw+Fg9uzZLFy4sNN9Fi5c2GF7gDlz5uxzexEREZGBRP0nERERGShc/fnhFRUVRKNRsrOzO6zPzs5m3bp1ne5TUlLS6fYlJSWdbh8MBgkGg22va2tr25a9MUistXoYDAZtUzEFe+ZWZntkBnvmVmZ7ZAZ75u5q5rq6urb9Dgd90X8C9aH6gjLbIzPYM7cy2yMz2DO3Mvdc/6lfi1J94cEHH+T+++/fa31BQUE/tEZERESOVPX19SQnJ/d3M/qM+lAiIiJyqA7Uf+rXolRGRgZOp5PS0tIO60tLS8nJyel0n5ycnC5tf/fdd3PnnXe2vTZNk6qqKtLT03ulollXV0d+fj47duw4bOad6At2zK3M9sgM9sytzPbIDPbM3dXMlmVRX19PXl5eH7TuwPqi/wTqQ/UFZbZHZrBnbmW2R2awZ25l7rn+U78WpTweD9OmTWPu3LlceOGFQKzDM3fuXG677bZO95k5cyZz587lm9/8Ztu6d955h5kzZ3a6vdfrxev1dliXkpLSE83fr6SkJNv8ONuzY25ltg875lZm+7Bj7q5kPpxGSPVF/wnUh+pLymwfdsytzPZhx9zKvH8H03/q98v37rzzTq6++mqOPvpopk+fziOPPEJjYyPXXnstAFdddRWDBg3iwQcfBOCOO+7gpJNO4n//938555xzeP7551m6dCm///3v+zOGiIiISJ9R/0lEREQGgn4vSl122WWUl5dz7733UlJSwpQpU3jrrbfaJuMsKirC4dhzk8DjjjuOZ599lnvuuYfvf//7jBw5kldffZUJEyb0VwQRERGRPqX+k4iIiAwE/V6UArjtttv2Odx8/vz5e6378pe/zJe//OVeblX3eL1e7rvvvr2Guw90dsytzPZhx9zKbB92zD1QMg+k/hMMnPPSFcpsH3bMrcz2YcfcytxzDOtwub+xiIiIiIiIiIjYhuPAm4iIiIiIiIiIiPQsFaVERERERERERKTPqSglIiIiIiIiIiJ9TkWpHvb4449TWFiIz+djxowZLFmypL+b1Gt+9KMfYRhGh8eYMWP6u1k97oMPPuC8884jLy8PwzB49dVXO7xvWRb33nsvubm5xMXFMXv2bDZu3Ng/je0hB8p8zTXX7HXuzzzzzP5pbA958MEHOeaYY0hMTCQrK4sLL7yQ9evXd9gmEAhw6623kp6eTkJCAhdffDGlpaX91OJDdzCZTz755L3O9Te+8Y1+avGhe+KJJ5g0aRJJSUkkJSUxc+ZM/vOf/7S9P9DOcasD5R5o57kzDz30EIZh8M1vfrNt3UA930ciO/WfwB59KPWf1H9qNRD/rFUfyh59KPWf+qb/pKJUD3rhhRe48847ue+++/j000+ZPHkyc+bMoaysrL+b1mvGjx9PcXFx2+Ojjz7q7yb1uMbGRiZPnszjjz/e6fsPP/wwjz76KE8++SSLFy8mPj6eOXPmEAgE+rilPedAmQHOPPPMDuf+ueee68MW9rz333+fW2+9lUWLFvHOO+8QDoc544wzaGxsbNvmW9/6Fv/617946aWXeP/999m9ezdf+tKX+rHVh+ZgMgPceOONHc71ww8/3E8tPnSDBw/moYceYtmyZSxdupRTTz2VCy64gDVr1gAD7xy3OlBuGFjn+Ys++eQTfve73zFp0qQO6wfq+T7S2LH/BAO/D6X+U+fUfxoYf9aqD2WPPpT6T33Uf7Kkx0yfPt269dZb215Ho1ErLy/PevDBB/uxVb3nvvvusyZPntzfzehTgPXPf/6z7bVpmlZOTo71i1/8om1dTU2N5fV6reeee64fWtjzvpjZsizr6quvti644IJ+aU9fKSsrswDr/ffftywrdl7dbrf10ksvtW2zdu1aC7AWLlzYX83sUV/MbFmWddJJJ1l33HFH/zWqD6Smplp/+MMfbHGO22vNbVkD+zzX19dbI0eOtN55550OOe12vg9ndus/WZb9+lDqP8Wo/xQzEP+sVR/KHufZstR/6o1zrZFSPSQUCrFs2TJmz57dts7hcDB79mwWLlzYjy3rXRs3biQvL49hw4Zx5ZVXUlRU1N9N6lNbt26lpKSkw3lPTk5mxowZA/q8A8yfP5+srCxGjx7NzTffTGVlZX83qUfV1tYCkJaWBsCyZcsIh8MdzvWYMWMoKCgYMOf6i5lb/f3vfycjI4MJEyZw991309TU1B/N63HRaJTnn3+exsZGZs6caYtzDHvnbjVQz/Ott97KOeec0+G8gj3+mz4S2LX/BPbuQ6n/pP7TQPuzVn2ogX+e1X+K6Y1z7TqklkqbiooKotEo2dnZHdZnZ2ezbt26fmpV75oxYwZ/+tOfGD16NMXFxdx///2ceOKJrF69msTExP5uXp8oKSkB6PS8t743EJ155pl86UtfYujQoWzevJnvf//7nHXWWSxcuBCn09nfzTtkpmnyzW9+k+OPP54JEyYAsXPt8XhISUnpsO1AOdedZQa44oorGDJkCHl5eaxcuZLvfve7rF+/nn/84x/92NpDs2rVKmbOnEkgECAhIYF//vOfjBs3juXLlw/oc7yv3DAwzzPA888/z6effsonn3yy13sD/b/pI4Ud+0+gPpT6T+o/DaRzrT7UwO5Dqf/UUW/8N62ilHTbWWed1fZ80qRJzJgxgyFDhvDiiy9y/fXX92PLpLd95StfaXs+ceJEJk2axPDhw5k/fz6nnXZaP7asZ9x6662sXr16wM3vsT/7ynzTTTe1PZ84cSK5ubmcdtppbN68meHDh/d1M3vE6NGjWb58ObW1tbz88stcffXVvP/++/3drF63r9zjxo0bkOd5x44d3HHHHbzzzjv4fL7+bo5IB+pD2ZP6TwOT+lADuw+l/lPv0+V7PSQjIwOn07nXrPOlpaXk5OT0U6v6VkpKCqNGjWLTpk393ZQ+03pu7XzeAYYNG0ZGRsaAOPe33XYb//73v5k3bx6DBw9uW5+Tk0MoFKKmpqbD9gPhXO8rc2dmzJgBcESfa4/Hw4gRI5g2bRoPPvggkydP5je/+c2APsew79ydGQjnedmyZZSVlTF16lRcLhcul4v333+fRx99FJfLRXZ29oA+30cK9Z9i7NaHUv8pRv2nI/9cqw818PtQ6j/1fv9JRake4vF4mDZtGnPnzm1bZ5omc+fO7XDN6UDW0NDA5s2byc3N7e+m9JmhQ4eSk5PT4bzX1dWxePFi25x3gJ07d1JZWXlEn3vLsrjtttv45z//yXvvvcfQoUM7vD9t2jTcbneHc71+/XqKioqO2HN9oMydWb58OcARfa6/yDRNgsHggDzH+9OauzMD4TyfdtpprFq1iuXLl7c9jj76aK688sq253Y634cr9Z9i7NaHUv8pRv2nI/dcqw8VY8c+lPpPvdB/OtRZ2WWP559/3vJ6vdaf/vQn6/PPP7duuukmKyUlxSopKenvpvWKb3/729b8+fOtrVu3WgsWLLBmz55tZWRkWGVlZf3dtB5VX19vffbZZ9Znn31mAdavfvUr67PPPrO2b99uWZZlPfTQQ1ZKSor12muvWStXrrQuuOACa+jQoVZzc3M/t7z79pe5vr7euuuuu6yFCxdaW7dutd59911r6tSp1siRI61AINDfTe+2m2++2UpOTrbmz59vFRcXtz2ampratvnGN75hFRQUWO+99561dOlSa+bMmdbMmTP7sdWH5kCZN23aZD3wwAPW0qVLra1bt1qvvfaaNWzYMGvWrFn93PLu+973vme9//771tatW62VK1da3/ve9yzDMKy3337bsqyBd45b7S/3QDzP+/LFu+QM1PN9pLFb/8my7NGHUv9J/adWA/HPWvWh7NGHUv8pprf7TypK9bDf/va3VkFBgeXxeKzp06dbixYt6u8m9ZrLLrvMys3NtTwejzVo0CDrsssuszZt2tTfzepx8+bNs4C9HldffbVlWbHbGv/whz+0srOzLa/Xa5122mnW+vXr+7fRh2h/mZuamqwzzjjDyszMtNxutzVkyBDrxhtvPOL/8tBZXsB65pln2rZpbm62brnlFis1NdXy+/3WRRddZBUXF/dfow/RgTIXFRVZs2bNstLS0iyv12uNGDHC+n//7/9ZtbW1/dvwQ3DddddZQ4YMsTwej5WZmWmddtppbZ0pyxp457jV/nIPxPO8L1/sVA3U830kslP/ybLs0YdS/0n9p1YD8c9a9aHs0YdS/ymmt/tPhmVZVvfGWImIiIiIiIiIiHSP5pQSEREREREREZE+p6KUiIiIiIiIiIj0ORWlRERERERERESkz6koJSIiIiIiIiIifU5FKRERERERERER6XMqSomIiIiIiIiISJ9TUUpERERERERERPqcilIiIiIiIiIiItLnVJQSETlEhmHw6quv9nczRERERI4Y6j+JCKgoJSJHuGuuuQbDMPZ6nHnmmf3dNBEREZHDkvpPInK4cPV3A0REDtWZZ57JM88802Gd1+vtp9aIiIiIHP7UfxKRw4FGSonIEc/r9ZKTk9PhkZqaCsSGhj/xxBOcddZZxMXFMWzYMF5++eUO+69atYpTTz2VuLg40tPT94ox0gAAA5ZJREFUuemmm2hoaOiwzdNPP8348ePxer3k5uZy2223dXi/oqKCiy66CL/fz8iRI3n99dd7N7SIiIjIIVD/SUQOBypKiciA98Mf/pCLL76YFStWcOWVV/KVr3yFtWvXAtDY2MicOXNITU3lk08+4aWXXuLdd9/t0Gl64oknuPXWW7nppptYtWoVr7/+OiNGjOjwGffffz+XXnopK1eu5Oyzz+bKK6+kqqqqT3OKiIiI9BT1n0SkT1giIkewq6++2nI6nVZ8fHyHx09/+lPLsiwLsL7xjW902GfGjBnWzTffbFmWZf3+97+3UlNTrYaGhrb333jjDcvhcFglJSWWZVlWXl6e9YMf/GCfbQCse+65p+11Q0ODBVj/+c9/eiyniIiISE9R/0lEDheaU0pEjninnHIKTzzxRId1aWlpbc9nzpzZ4b2ZM2eyfPlyANauXcvkyZOJj49ve//444/HNE3Wr1+PYRjs3r2b0047bb9tmDRpUtvz+Ph4kpKSKCsr624kERERkV6l/pOIHA5UlBKRI158fPxew8F7Slxc3EFt53a7O7w2DAPTNHujSSIiIiKHTP0nETkcaE4pERnwFi1atNfrsWPHAjB27FhWrFhBY2Nj2/sLFizA4XAwevRoEhMTKSwsZO7cuX3aZhEREZH+pP6TiPQFjZQSkSNeMBikpKSkwzqXy0VGRgYAL730EkcffTQnnHACf//731myZAl//OMfAbjyyiu57777uPrqq/nRj35EeXk5t99+O1/72tfIzs4G4Ec/+hHf+MY3yMrK4qyzzqK+vp4FCxZw++23921QERERkR6i/pOIHA5UlBKRI95bb71Fbm5uh3WjR49m3bp1QOzOLs8//zy33HILubm5PPfcc4wbNw4Av9/Pf//7X+644w6OOeYY/H4/F198Mb/61a/ajnX11VcTCAT49a9/zV133UVGRgaXXHJJ3wUUERER6WHqP4nI4cCwLMvq70aIiPQWwzD45z//yYUXXtjfTRERERE5Iqj/JCJ9RXNKiYiIiIiIiIhIn1NRSkRERERERERE+pwu3xMRERERERERkT6nkVIiIiIiIiIiItLnVJQSEREREREREZE+p6KUiIiIiIiIiIj0ORWlRERERERERESkz6koJSIiIiIiIiIifU5FKRERERERERER6XMqSomIiIiIiIiISJ9TUUpERERERERERPqcilIiIiIiIiIiItLn/j+9KImAzUgukQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "            if CFG.objective_cv == 'binary':\n",
    "                outputs.append(torch.sigmoid(model(x)).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'multiclass':\n",
    "                outputs.append(torch.softmax(\n",
    "                    model(x), axis=1).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'regression':\n",
    "                outputs.append(model(x).to('cpu').numpy())\n",
    "\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "\n",
    "def test_fn(valid_loader, model, device):\n",
    "    preds = []\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        preds.append(y_preds)\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def inference():\n",
    "    test = pd.read_csv(CFG.comp_dataset_path +\n",
    "                       'test_features.csv')\n",
    "\n",
    "    test['base_path'] = CFG.comp_dataset_path + 'images/' + test['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in test['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    print(test.head(5))\n",
    "\n",
    "    valid_dataset = CustomDataset(\n",
    "        test, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = EnsembleModel()\n",
    "    folds = [0] if CFG.use_holdout else list(range(CFG.n_fold))\n",
    "    for fold in folds:\n",
    "        _model = CustomModel(CFG, pretrained=False)\n",
    "        _model.to(device)\n",
    "\n",
    "        model_path = CFG.model_dir + \\\n",
    "            f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth'\n",
    "        print('load', model_path)\n",
    "        state = torch.load(model_path)['model']\n",
    "        _model.load_state_dict(state)\n",
    "        _model.eval()\n",
    "\n",
    "        # _model = tta.ClassificationTTAWrapper(\n",
    "        #     _model, tta.aliases.five_crop_transform(256, 256))\n",
    "\n",
    "        model.add_model(_model)\n",
    "\n",
    "    preds = test_fn(valid_loader, model, device)\n",
    "\n",
    "    test[CFG.target_col] = preds\n",
    "    test.to_csv(CFG.submission_dir +\n",
    "                'submission_oof.csv', index=False)\n",
    "    test[CFG.target_col].to_csv(\n",
    "        CFG.submission_dir + f'submission_{CFG.exp_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-0.5.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff65e197cd44c6c9c7a71b361d398f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     ID      vEgo      aEgo  steeringAngleDeg  \\\n",
      "0  012baccc145d400c896cb82065a93d42_120  3.374273 -0.019360        -34.008415   \n",
      "1  012baccc145d400c896cb82065a93d42_220  2.441048 -0.022754        307.860077   \n",
      "2  012baccc145d400c896cb82065a93d42_320  3.604152 -0.286239         10.774388   \n",
      "3  012baccc145d400c896cb82065a93d42_420  2.048902 -0.537628         61.045235   \n",
      "4  01d738e799d260a10f6324f78023b38f_120  2.201528 -1.898600          5.740093   \n",
      "\n",
      "   steeringTorque  brake  brakePressed  gas  gasPressed gearShifter  \\\n",
      "0            17.0    0.0         False  0.0       False       drive   \n",
      "1           295.0    0.0          True  0.0       False       drive   \n",
      "2          -110.0    0.0          True  0.0       False       drive   \n",
      "3           189.0    0.0          True  0.0       False       drive   \n",
      "4           -41.0    0.0          True  0.0       False       drive   \n",
      "\n",
      "   leftBlinker  rightBlinker  \\\n",
      "0        False         False   \n",
      "1        False         False   \n",
      "2        False         False   \n",
      "3         True         False   \n",
      "4        False         False   \n",
      "\n",
      "                                           base_path  \n",
      "0  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "1  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "2  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "3  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "4  ../raw/atmacup_18_dataset/images/01d738e799d26...  \n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_conv_tiny/atmacup_18-models/convnextv2_tiny_fold0_last.pth\n",
      "pretrained: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_565/610043316.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path)['model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../proc/baseline/outputs/atmacup_18_cnn_conv_tiny/atmacup_18-models/convnextv2_tiny_fold1_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_conv_tiny/atmacup_18-models/convnextv2_tiny_fold2_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_conv_tiny/atmacup_18-models/convnextv2_tiny_fold3_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_conv_tiny/atmacup_18-models/convnextv2_tiny_fold4_last.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb54b0d594b6448ebf321c34b16240fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
