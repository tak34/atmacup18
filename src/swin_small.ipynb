{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from timm.utils.model_ema import ModelEmaV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set dataset path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'atmacup_18'  # comp名\n",
    "\n",
    "    comp_dataset_path = '../raw/atmacup_18_dataset/'\n",
    "\n",
    "    exp_name = 'atmacup_18_cnn_swin_small'\n",
    "\n",
    "    is_debug = False\n",
    "    use_gray_scale = False\n",
    "\n",
    "    model_in_chans = 9  # モデルの入力チャンネル数\n",
    "\n",
    "    # ============== file path =============\n",
    "    train_fold_dir = \"../proc/baseline/folds\"\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = \"swin_small_patch4_window7_224\"\n",
    "\n",
    "    num_frames = 3  # model_in_chansの倍数\n",
    "    norm_in_chans = 1 if use_gray_scale else 3\n",
    "\n",
    "    use_torch_compile = False\n",
    "    use_ema = True\n",
    "    ema_decay = 0.995\n",
    "    # ============== training cfg =============\n",
    "    size = 224  # 224\n",
    "\n",
    "    batch_size = 64  # 32\n",
    "\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 40\n",
    "    if is_debug:\n",
    "        epochs = 2\n",
    "\n",
    "    # adamW warmupあり\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-4\n",
    "    if scheduler == 'GradualWarmupSchedulerV2':\n",
    "        lr /= warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    n_fold = 5\n",
    "    use_holdout = False\n",
    "    use_alldata = False\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    skf_col = 'class'\n",
    "    group_col = 'scene'\n",
    "    fold_type = 'gkf'\n",
    "\n",
    "    objective_cv = 'regression'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'minimize'  # 'maximize', 'minimize'\n",
    "    metrics = 'calc_mae_atmacup'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 18\n",
    "    target_col = ['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1', 'x_2', 'y_2',\n",
    "                  'z_2', 'x_3', 'y_3', 'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5']\n",
    "\n",
    "\n",
    "    # ============== ほぼ固定 =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'last'  # 'best'\n",
    "\n",
    "    min_lr = 1e-8\n",
    "    weight_decay = 1e-5\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== set dataset path =============\n",
    "    if exp_name is not None:\n",
    "        print('set dataset path')\n",
    "\n",
    "        outputs_path = f'../proc/baseline/outputs/{exp_name}/'\n",
    "\n",
    "        submission_dir = outputs_path + 'submissions/'\n",
    "        submission_path = submission_dir + f'submission_{exp_name}.csv'\n",
    "\n",
    "        model_dir = outputs_path + \\\n",
    "            f'{comp_name}-models/'\n",
    "\n",
    "        figures_dir = outputs_path + 'figures/'\n",
    "\n",
    "        log_dir = outputs_path + 'logs/'\n",
    "        log_path = log_dir + f'{exp_name}.txt'\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.ShiftScaleRotate(p=0.5),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        # A.CoarseDropout(max_holes=1, max_height=int(\n",
    "        #     size * 0.3), max_width=int(size * 0.3), p=0.5),\n",
    "\n",
    "        A.Normalize(\n",
    "            mean=[0] * norm_in_chans*num_frames,\n",
    "            std=[1] * norm_in_chans*num_frames, \n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * norm_in_chans*num_frames,\n",
    "            std=[1] * norm_in_chans*num_frames,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA が利用可能か: True\n",
      "利用可能な CUDA デバイス数: 1\n",
      "現在の CUDA デバイス: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA が利用可能か:\", torch.cuda.is_available())\n",
    "print(\"利用可能な CUDA デバイス数:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"現在の CUDA デバイス:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "def get_fold(train, cfg):\n",
    "    if cfg.fold_type == 'kf':\n",
    "        Fold = KFold(n_splits=cfg.n_fold,\n",
    "                     shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.target_col])\n",
    "    elif cfg.fold_type == 'skf':\n",
    "        Fold = StratifiedKFold(n_splits=cfg.n_fold,\n",
    "                               shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.skf_col])\n",
    "    elif cfg.fold_type == 'gkf':\n",
    "        Fold = GroupKFold(n_splits=cfg.n_fold)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.group_col], groups)\n",
    "    elif cfg.fold_type == 'sgkf':\n",
    "        Fold = StratifiedGroupKFold(n_splits=cfg.n_fold,\n",
    "                                    shuffle=True, random_state=cfg.seed)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.skf_col], groups)\n",
    "    # elif fold_type == 'mskf':\n",
    "    #     Fold = MultilabelStratifiedKFold(\n",
    "    #         n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    #     kf = Fold.split(train, train[cfg.skf_col])\n",
    "\n",
    "    for n, (train_index, val_index) in enumerate(kf):\n",
    "        train.loc[val_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "    print(train.groupby('fold').size())\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_folds():\n",
    "    train_df = pd.read_csv(CFG.comp_dataset_path + 'train_features.csv')\n",
    "\n",
    "    train_df['scene'] = train_df['ID'].str.split('_').str[0]\n",
    "\n",
    "    print('group', CFG.group_col)\n",
    "    print(f'train len: {len(train_df)}')\n",
    "\n",
    "    train_df = get_fold(train_df, CFG)\n",
    "\n",
    "    # print(train_df.groupby(['fold', CFG.target_col]).size())\n",
    "    print(train_df['fold'].value_counts())\n",
    "\n",
    "    os.makedirs(CFG.train_fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(CFG.train_fold_dir +\n",
    "                    'train_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group scene\n",
      "train len: 43371\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "dtype: int64\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "make_train_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数固定\n",
    "def set_seed(seed=None, cudnn_deterministic=True):\n",
    "    if seed is None:\n",
    "        seed = 42\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def make_dirs(cfg):\n",
    "    for dir in [cfg.model_dir, cfg.figures_dir, cfg.submission_dir, cfg.log_dir]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def cfg_init(cfg, mode='train'):\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if mode == 'train':\n",
    "        make_dirs(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_init(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common_utils.logger import init_logger, wandb_init, AverageMeter, timeSince\n",
    "# from common_utils.settings import cfg_init\n",
    "\n",
    "def init_logger(log_file):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- exp_info -----------------\n",
      "2024年11月21日 13:15:29\n"
     ]
    }
   ],
   "source": [
    "Logger = init_logger(log_file=CFG.log_path)\n",
    "\n",
    "Logger.info('\\n\\n-------- exp_info -----------------')\n",
    "Logger.info(datetime.datetime.now().strftime('%Y年%m月%d日 %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    # return roc_auc_score(y_true, y_pred)\n",
    "    eval_func = eval(CFG.metrics)\n",
    "    return eval_func(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calc_mae_atmacup(y_true, y_pred):\n",
    "    abs_diff = np.abs(y_true - y_pred)  # 各予測の差分の絶対値を計算して\n",
    "    mae = np.mean(abs_diff.reshape(-1, ))  # 予測の差分の絶対値の平均を計算\n",
    "\n",
    "    return mae\n",
    "\n",
    "def get_result(result_df):\n",
    "\n",
    "    # preds = result_df['preds'].values\n",
    "\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "    preds = result_df[pred_cols].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    Logger.info(f'score: {score:<.4f}')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_traffic_light(image, id):\n",
    "    path = f'./datasets/atmacup_18/traffic_lights/{id}.json'\n",
    "    traffic_lights = json.load(open(path))\n",
    "\n",
    "    traffic_class = ['green',\n",
    "                     'straight', 'left', 'right', 'empty', 'other', 'yellow', 'red']\n",
    "    class_to_idx = {\n",
    "        cls: idx for idx, cls in enumerate(traffic_class)\n",
    "    }\n",
    "\n",
    "    for traffic_light in traffic_lights:\n",
    "        bbox = traffic_light['bbox']\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # int\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        point1 = (x1, y1)\n",
    "        point2 = (x2, y2)\n",
    "\n",
    "        idx = class_to_idx[traffic_light['class']]\n",
    "        color = 255 - int(255*(idx/len(traffic_class)))\n",
    "\n",
    "        cv2.rectangle(image, point1, point2, color=color, thickness=1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_image_for_cache(path):\n",
    "    if CFG.use_gray_scale:\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # image = cv2.resize(image, (CFG.size, CFG.size))\n",
    "\n",
    "    # 効かない\n",
    "    # image = draw_traffic_light(image, path.split('/')[-2])\n",
    "    return (path, image)\n",
    "\n",
    "\n",
    "def make_video_cache(paths):\n",
    "    debug = []\n",
    "    for idx in range(9):\n",
    "        color = 255 - int(255*(idx/9))\n",
    "        debug.append(color)\n",
    "    print(debug)\n",
    "\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        res = pool.imap_unordered(read_image_for_cache, paths)\n",
    "        res = tqdm(res)\n",
    "        res = list(res)\n",
    "\n",
    "    return dict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import ReplayCompose\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        # aug = A.Compose(cfg.train_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        # aug = A.Compose(cfg.valid_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, cfg, labels=None, transform=None):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.base_paths = df['base_path'].values\n",
    "        # self.labels = df[self.cfg.target_col].values\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def read_image_multiframe(self, idx):\n",
    "        base_path = self.base_paths[idx]\n",
    "\n",
    "        images = []\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "\n",
    "            image = self.cfg.video_cache[path]\n",
    "\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.read_image_multiframe(idx)\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image=image)['image']\n",
    "            replay = None\n",
    "            images = []\n",
    "            for img in image:\n",
    "                if replay is None:\n",
    "                    sample = self.transform(image=img)\n",
    "                    replay = sample['replay']\n",
    "                else:\n",
    "                    sample = ReplayCompose.replay(replay, image=img)\n",
    "                images.append(sample['image'])\n",
    "\n",
    "            image = torch.concat(images, dim=0)\n",
    "\n",
    "        if self.labels is None:\n",
    "            return image\n",
    "\n",
    "        if self.cfg.objective_cv == 'multiclass':\n",
    "            label = torch.tensor(self.labels[idx]).long()\n",
    "        else:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aug_video(train, cfg, plot_count=1):\n",
    "    transform = CFG.train_aug_list\n",
    "    transform = A.ReplayCompose(transform)\n",
    "\n",
    "    dataset = CustomDataset(\n",
    "        train, CFG, transform=transform)\n",
    "\n",
    "    for i in range(plot_count):\n",
    "        image = dataset.read_image_multiframe(i)\n",
    "\n",
    "        if cfg.use_gray_scale:\n",
    "            image = np.stack(image, axis=2)\n",
    "        else:\n",
    "            image = np.concatenate(image, axis=2)\n",
    "\n",
    "        aug_image = dataset[i]\n",
    "        # torch to numpy\n",
    "        aug_image = aug_image.permute(1, 2, 0).numpy()*255\n",
    "\n",
    "        for frame in range(image.shape[-1]):\n",
    "            if frame % 3 != 0:\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            if cfg.use_gray_scale:\n",
    "                axes[0].imshow(image[..., frame], cmap=\"gray\")\n",
    "                axes[1].imshow(aug_image[..., frame], cmap=\"gray\")\n",
    "            else:\n",
    "                axes[0].imshow(image[..., frame:frame+3].astype(int))\n",
    "                axes[1].imshow(aug_image[..., frame:frame+3].astype(int))\n",
    "            plt.savefig(cfg.figures_dir +\n",
    "                        f'aug_{i}_frame{frame}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "        # self.cfg = cfg\n",
    "\n",
    "        if model_name is None:\n",
    "            model_name = cfg.model_name\n",
    "\n",
    "        print(f'pretrained: {pretrained}')\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=0,\n",
    "            in_chans=cfg.model_in_chans)\n",
    "\n",
    "        # モデルの出力サイズを取得\n",
    "        if hasattr(self.model, 'num_features'):\n",
    "            self.n_features = self.model.num_features  # num_featuresで取得するモデルが多い\n",
    "        elif hasattr(self.model, 'classifier') and hasattr(self.model.classifier, 'in_features'):\n",
    "            self.n_features = self.model.classifier.in_features  # classifierが存在する場合\n",
    "        elif hasattr(self.model, 'fc') and hasattr(self.model.fc, 'in_features'):\n",
    "            self.n_features = self.model.fc.in_features  # fcが存在する場合\n",
    "        else:\n",
    "            raise AttributeError(\"Could not find the output feature size.\")\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "\n",
    "        # nn.Dropout(0.5),\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(self.n_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, self.target_size),\n",
    "        )\n",
    "\n",
    "    def feature(self, image):\n",
    "\n",
    "        feature = self.model(image)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.feature(image)\n",
    "        output = self.final_fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(\n",
    "            optimizer, multiplier, total_epoch, after_scheduler)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [\n",
    "                        base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "def get_scheduler(cfg, optimizer):\n",
    "    if cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=cfg.factor, patience=cfg.patience, verbose=True, eps=cfg.eps)\n",
    "    elif cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer, T_max=cfg.epochs, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=cfg.T_0, T_mult=1, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'GradualWarmupSchedulerV2':\n",
    "        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, cfg.epochs, eta_min=1e-7)\n",
    "        scheduler = GradualWarmupSchedulerV2(\n",
    "            optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def scheduler_step(scheduler, avg_val_loss, epoch):\n",
    "    if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        scheduler.step(avg_val_loss)\n",
    "    elif isinstance(scheduler, CosineAnnealingLR):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, GradualWarmupSchedulerV2):\n",
    "        scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device,\n",
    "             model_ema=None):\n",
    "    \"\"\" 1epoch毎のtrain \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.use_amp)\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    preds_labels = []\n",
    "    start = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with autocast(CFG.use_amp):\n",
    "            y_preds = model(images)\n",
    "\n",
    "            if y_preds.size(1) == 1:\n",
    "                y_preds = y_preds.view(-1)\n",
    "\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.detach().to('cpu').numpy())\n",
    "\n",
    "        preds_labels.append(labels.detach().to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(preds_labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        if y_preds.size(1) == 1:\n",
    "            y_preds = y_preds.view(-1)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # binary\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(folds, fold):\n",
    "\n",
    "    Logger.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    if CFG.use_alldata:\n",
    "        train_folds = folds.copy().reset_index(drop=True)\n",
    "    else:\n",
    "        train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # train_folds = train_downsampling(train_folds)\n",
    "\n",
    "    train_labels = train_folds[CFG.target_col].values\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        train_folds, CFG, labels=train_labels, transform=get_transforms(data='train', cfg=CFG))\n",
    "    valid_dataset = CustomDataset(\n",
    "        valid_folds, CFG, labels=valid_labels, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                              )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "\n",
    "    model = CustomModel(CFG, pretrained=CFG.pretrained)\n",
    "    model.to(device)\n",
    "\n",
    "    if CFG.use_ema:\n",
    "        model_ema = ModelEmaV2(model, decay=CFG.ema_decay)\n",
    "    else:\n",
    "        model_ema = None\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr)\n",
    "    scheduler = get_scheduler(CFG, optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    if CFG.objective_cv == 'binary':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif CFG.objective_cv == 'multiclass':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif CFG.objective_cv == 'regression':\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "    if CFG.metric_direction == 'minimize':\n",
    "        best_score = np.inf\n",
    "    elif CFG.metric_direction == 'maximize':\n",
    "        best_score = -1\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    df_score = pd.DataFrame(columns=[\"train_loss\", 'train_score', 'val_loss', 'val_score'])\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss, train_preds, train_labels_epoch = train_fn(fold, train_loader, model,\n",
    "                                                             criterion, optimizer, epoch, scheduler, device, model_ema)\n",
    "        train_score = get_score(train_labels_epoch, train_preds)\n",
    "\n",
    "        # eval\n",
    "        if model_ema is not None:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model_ema.module, criterion, device)\n",
    "        else:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model, criterion, device)\n",
    "\n",
    "        scheduler_step(scheduler, avg_val_loss, epoch)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, valid_preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        # Logger.info(f'Epoch {epoch+1} - avgScore: {avg_score:.4f}')\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_Score: {train_score:.4f} avgScore: {score:.4f}')\n",
    "        \n",
    "        df_score.loc[epoch] = [avg_loss, train_score, avg_val_loss, score]\n",
    "\n",
    "        if CFG.metric_direction == 'minimize':\n",
    "            update_best = score < best_score\n",
    "        elif CFG.metric_direction == 'maximize':\n",
    "            update_best = score > best_score\n",
    "\n",
    "        if update_best:\n",
    "            best_loss = avg_val_loss\n",
    "            best_score = score\n",
    "\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "\n",
    "            if model_ema is not None:\n",
    "                torch.save({'model': model_ema.module.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "            else:\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save({'model': model.state_dict(),\n",
    "                'preds': valid_preds},\n",
    "               CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    \"\"\"\n",
    "    if model_ema is not None:\n",
    "        torch.save({'model': model_ema.module.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    else:\n",
    "        torch.save({'model': model.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "\n",
    "    check_point = torch.load(\n",
    "        CFG.model_dir + f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth', map_location=torch.device('cpu'))\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "\n",
    "    check_point_pred = check_point['preds']\n",
    "\n",
    "    # Columns must be same length as key 対策\n",
    "    if check_point_pred.ndim == 1:\n",
    "        check_point_pred = check_point_pred.reshape(-1, CFG.target_size)\n",
    "\n",
    "    print('check_point_pred shape', check_point_pred.shape)\n",
    "    valid_folds[pred_cols] = check_point_pred\n",
    "    return valid_folds, df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = pd.read_csv(CFG.train_fold_dir + 'train_folds.csv')\n",
    "    train['ori_idx'] = train.index\n",
    "\n",
    "    train['scene'] = train['ID'].str.split('_').str[0]\n",
    "\n",
    "    \"\"\"\n",
    "    if CFG.is_debug:\n",
    "        use_ids = train['scene'].unique()[:100]\n",
    "        train = train[train['scene'].isin(use_ids)].reset_index(drop=True)\n",
    "    \"\"\"\n",
    "\n",
    "    train['base_path'] = CFG.comp_dataset_path + 'images/' + train['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in train['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    # plot_aug_video(train, CFG, plot_count=10)\n",
    "\n",
    "    # train\n",
    "    oof_df = pd.DataFrame()\n",
    "    list_df_score = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold not in CFG.train_folds:\n",
    "            print(f'fold {fold} is skipped')\n",
    "            continue\n",
    "\n",
    "        _oof_df, _df_score = train_fold(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        list_df_score.append(_df_score)\n",
    "        Logger.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "\n",
    "        if CFG.use_holdout or CFG.use_alldata:\n",
    "            break\n",
    "\n",
    "    oof_df = oof_df.sort_values('ori_idx').reset_index(drop=True)\n",
    "\n",
    "    # CV result\n",
    "    Logger.info(\"========== CV ==========\")\n",
    "    score = get_result(oof_df)\n",
    "\n",
    "    # 学習曲線を可視化する\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.grid(alpha=0.1)\n",
    "    ax2.grid(alpha=0.1)\n",
    "    for i, df_score in enumerate(list_df_score):\n",
    "        ax1.plot(df_score['train_score'], label=f'fold {i}')\n",
    "        ax2.plot(df_score['val_score'], label=f'fold {i}')\n",
    "    ax1.set_title('Train Score')\n",
    "    ax2.set_title('Val Score') \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Score')\n",
    "    ax2.set_ylabel('Val Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CFG.figures_dir + f'learning_curve_{CFG.exp_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # save result\n",
    "    oof_df.to_csv(CFG.submission_dir + 'oof_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-0.5.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0192d28699c540da96c66dbd03aed254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 2s (remain 20m 41s) Loss: 6.0146(6.0146) Grad: nan  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 4.7669(5.3065) Grad: 33588.0430  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 4.5722(5.2882) Grad: 45047.0664  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 58s) Loss: 4.8279(4.8279) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 5.2882  avg_val_loss: 5.3822  time: 131s\n",
      "Epoch 1 - avg_train_Score: 5.2882 avgScore: 5.3822\n",
      "Epoch 1 - Save Best Score: 5.3822 Model\n",
      "Epoch 1 - Save Best Loss: 5.3822 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.9033(5.3822) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 37s) Loss: 5.2647(5.2647) Grad: 152223.5938  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 4.0304(4.9798) Grad: 153300.3594  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 4.4363(4.9623) Grad: 123405.2344  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 4.4131(4.4131) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 4.9623  avg_val_loss: 4.9514  time: 131s\n",
      "Epoch 2 - avg_train_Score: 4.9623 avgScore: 4.9514\n",
      "Epoch 2 - Save Best Score: 4.9514 Model\n",
      "Epoch 2 - Save Best Loss: 4.9514 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.4891(4.9514) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 5.6213(5.6213) Grad: 666546.5000  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.8800(3.3921) Grad: 83049.1875  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.8597(3.2900) Grad: 5028.0552  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 2.0692(2.0692) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 3.2900  avg_val_loss: 2.4617  time: 130s\n",
      "Epoch 3 - avg_train_Score: 3.2900 avgScore: 2.4617\n",
      "Epoch 3 - Save Best Score: 2.4617 Model\n",
      "Epoch 3 - Save Best Loss: 2.4617 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 2.2964(2.4617) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 10m 47s) Loss: 1.8505(1.8505) Grad: nan  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.5231(1.5523) Grad: 40603.9961  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.4592(1.5392) Grad: 39290.2812  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 1.0554(1.0554) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.5392  avg_val_loss: 1.1814  time: 130s\n",
      "Epoch 4 - avg_train_Score: 1.5392 avgScore: 1.1814\n",
      "Epoch 4 - Save Best Score: 1.1814 Model\n",
      "Epoch 4 - Save Best Loss: 1.1814 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.1520(1.1814) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 10m 32s) Loss: 1.3245(1.3245) Grad: 328374.5312  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2659(1.2749) Grad: 74246.5938  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0880(1.2758) Grad: 72091.8516  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.9378(0.9378) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.2758  avg_val_loss: 1.0050  time: 130s\n",
      "Epoch 5 - avg_train_Score: 1.2758 avgScore: 1.0050\n",
      "Epoch 5 - Save Best Score: 1.0050 Model\n",
      "Epoch 5 - Save Best Loss: 1.0050 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 1.0047(1.0050) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 1.4733(1.4733) Grad: 383316.2188  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0985(1.1977) Grad: 34852.7344  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.7135(1.1988) Grad: 48707.8594  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.9019(0.9019) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 1.1988  avg_val_loss: 0.9643  time: 130s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9576(0.9643) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_Score: 1.1988 avgScore: 0.9643\n",
      "Epoch 6 - Save Best Score: 0.9643 Model\n",
      "Epoch 6 - Save Best Loss: 0.9643 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 8s) Loss: 1.0055(1.0055) Grad: 314871.2812  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0123(1.1150) Grad: 56178.8164  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.1582(1.1155) Grad: 52405.4336  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.9043(0.9043) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 1.1155  avg_val_loss: 0.9260  time: 131s\n",
      "Epoch 7 - avg_train_Score: 1.1155 avgScore: 0.9260\n",
      "Epoch 7 - Save Best Score: 0.9260 Model\n",
      "Epoch 7 - Save Best Loss: 0.9260 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8969(0.9260) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 1.1850(1.1850) Grad: 323707.2812  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9967(1.0340) Grad: 42510.0039  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.3442(1.0383) Grad: 41552.4258  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.8769(0.8769) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 1.0383  avg_val_loss: 0.9016  time: 130s\n",
      "Epoch 8 - avg_train_Score: 1.0383 avgScore: 0.9016\n",
      "Epoch 8 - Save Best Score: 0.9016 Model\n",
      "Epoch 8 - Save Best Loss: 0.9016 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8719(0.9016) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 12m 37s) Loss: 0.9189(0.9189) Grad: 318389.6250  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8953(0.9864) Grad: 71277.2031  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7718(0.9856) Grad: 68156.0469  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8502(0.8502) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.9856  avg_val_loss: 0.8807  time: 131s\n",
      "Epoch 9 - avg_train_Score: 0.9856 avgScore: 0.8807\n",
      "Epoch 9 - Save Best Score: 0.8807 Model\n",
      "Epoch 9 - Save Best Loss: 0.8807 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8919(0.8807) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 18s) Loss: 0.8853(0.8853) Grad: 309177.0000  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.1260(0.9533) Grad: 81721.5156  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8641(0.9540) Grad: 54281.9648  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.8371(0.8371) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.9540  avg_val_loss: 0.8743  time: 130s\n",
      "Epoch 10 - avg_train_Score: 0.9540 avgScore: 0.8743\n",
      "Epoch 10 - Save Best Score: 0.8743 Model\n",
      "Epoch 10 - Save Best Loss: 0.8743 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8913(0.8743) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 11m 1s) Loss: 0.7878(0.7878) Grad: 265414.7500  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8816(0.9133) Grad: 59190.8672  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8010(0.9129) Grad: 71709.2500  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.8038(0.8038) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.9129  avg_val_loss: 0.8622  time: 130s\n",
      "Epoch 11 - avg_train_Score: 0.9129 avgScore: 0.8622\n",
      "Epoch 11 - Save Best Score: 0.8622 Model\n",
      "Epoch 11 - Save Best Loss: 0.8622 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8839(0.8622) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 10m 45s) Loss: 0.9706(0.9706) Grad: 200140.6250  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0588(0.8747) Grad: 48871.4727  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.9363(0.8726) Grad: 74477.0781  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.8107(0.8107) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.8726  avg_val_loss: 0.8566  time: 131s\n",
      "Epoch 12 - avg_train_Score: 0.8726 avgScore: 0.8566\n",
      "Epoch 12 - Save Best Score: 0.8566 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8567(0.8566) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Save Best Loss: 0.8566 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 12m 48s) Loss: 0.8863(0.8863) Grad: 287029.5625  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7222(0.8970) Grad: 25640.9082  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.9273(0.9015) Grad: 32970.0234  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.8175(0.8175) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.9015  avg_val_loss: 0.8588  time: 131s\n",
      "Epoch 13 - avg_train_Score: 0.9015 avgScore: 0.8588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8786(0.8588) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 10m 41s) Loss: 0.7833(0.7833) Grad: 245687.9844  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.7792(0.8963) Grad: 50016.7734  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6891(0.8954) Grad: 66212.1172  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7732(0.7732) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.8954  avg_val_loss: 0.8502  time: 131s\n",
      "Epoch 14 - avg_train_Score: 0.8954 avgScore: 0.8502\n",
      "Epoch 14 - Save Best Score: 0.8502 Model\n",
      "Epoch 14 - Save Best Loss: 0.8502 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8628(0.8502) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 56s) Loss: 1.0461(1.0461) Grad: 212191.9375  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7917(0.8580) Grad: 78124.1406  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.1252(0.8634) Grad: 46433.7852  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7915(0.7915) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.8634  avg_val_loss: 0.8400  time: 130s\n",
      "Epoch 15 - avg_train_Score: 0.8634 avgScore: 0.8400\n",
      "Epoch 15 - Save Best Score: 0.8400 Model\n",
      "Epoch 15 - Save Best Loss: 0.8400 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8203(0.8400) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 10m 39s) Loss: 0.9237(0.9237) Grad: 291732.3438  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2587(0.8692) Grad: 40857.6094  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7371(0.8688) Grad: 30989.8164  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7849(0.7849) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.8688  avg_val_loss: 0.8464  time: 130s\n",
      "Epoch 16 - avg_train_Score: 0.8688 avgScore: 0.8464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8086(0.8464) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 43s) Loss: 0.8087(0.8087) Grad: 215829.9062  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0458(0.8330) Grad: 33094.9414  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7727(0.8318) Grad: 39989.1484  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7942(0.7942) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.8318  avg_val_loss: 0.8363  time: 130s\n",
      "Epoch 17 - avg_train_Score: 0.8318 avgScore: 0.8363\n",
      "Epoch 17 - Save Best Score: 0.8363 Model\n",
      "Epoch 17 - Save Best Loss: 0.8363 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8014(0.8363) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 0.8579(0.8579) Grad: 240391.7500  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6468(0.8100) Grad: 56310.5352  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7082(0.8106) Grad: 64869.3203  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7691(0.7691) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.8106  avg_val_loss: 0.8288  time: 130s\n",
      "Epoch 18 - avg_train_Score: 0.8106 avgScore: 0.8288\n",
      "Epoch 18 - Save Best Score: 0.8288 Model\n",
      "Epoch 18 - Save Best Loss: 0.8288 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7983(0.8288) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 10m 32s) Loss: 0.8821(0.8821) Grad: 254648.7500  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8211(0.8000) Grad: 40756.2109  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8704(0.8028) Grad: 37697.5430  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7440(0.7440) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.8028  avg_val_loss: 0.8287  time: 130s\n",
      "Epoch 19 - avg_train_Score: 0.8028 avgScore: 0.8287\n",
      "Epoch 19 - Save Best Score: 0.8287 Model\n",
      "Epoch 19 - Save Best Loss: 0.8287 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7894(0.8287) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 10s) Loss: 1.1328(1.1328) Grad: 254801.3438  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6796(0.8059) Grad: 90188.2422  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7784(0.8064) Grad: 60623.9141  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7771(0.7771) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8111(0.8268) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.8064  avg_val_loss: 0.8268  time: 130s\n",
      "Epoch 20 - avg_train_Score: 0.8064 avgScore: 0.8268\n",
      "Epoch 20 - Save Best Score: 0.8268 Model\n",
      "Epoch 20 - Save Best Loss: 0.8268 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 11m 37s) Loss: 0.6069(0.6069) Grad: 176865.4375  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8749(0.7662) Grad: 76262.0312  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6576(0.7649) Grad: 43830.9336  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7695(0.7695) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.7649  avg_val_loss: 0.8249  time: 130s\n",
      "Epoch 21 - avg_train_Score: 0.7649 avgScore: 0.8249\n",
      "Epoch 21 - Save Best Score: 0.8249 Model\n",
      "Epoch 21 - Save Best Loss: 0.8249 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8162(0.8249) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 10m 38s) Loss: 0.6740(0.6740) Grad: 263128.5000  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9053(0.7562) Grad: 63648.9492  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6933(0.7592) Grad: 50911.2734  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7462(0.7462) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.7592  avg_val_loss: 0.8228  time: 130s\n",
      "Epoch 22 - avg_train_Score: 0.7592 avgScore: 0.8228\n",
      "Epoch 22 - Save Best Score: 0.8228 Model\n",
      "Epoch 22 - Save Best Loss: 0.8228 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8180(0.8228) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 11m 14s) Loss: 1.1649(1.1649) Grad: 198701.4844  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6935(0.7662) Grad: 64180.7852  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6904(0.7664) Grad: 58737.3945  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7437(0.7437) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.7664  avg_val_loss: 0.8193  time: 130s\n",
      "Epoch 23 - avg_train_Score: 0.7664 avgScore: 0.8193\n",
      "Epoch 23 - Save Best Score: 0.8193 Model\n",
      "Epoch 23 - Save Best Loss: 0.8193 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8041(0.8193) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 44s) Loss: 0.6165(0.6165) Grad: 238186.1719  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7961(0.7442) Grad: 38996.1445  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5957(0.7473) Grad: 50072.1758  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7214(0.7214) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.7473  avg_val_loss: 0.8168  time: 130s\n",
      "Epoch 24 - avg_train_Score: 0.7473 avgScore: 0.8168\n",
      "Epoch 24 - Save Best Score: 0.8168 Model\n",
      "Epoch 24 - Save Best Loss: 0.8168 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7985(0.8168) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 10m 51s) Loss: 0.7498(0.7498) Grad: 216776.4531  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0828(0.7312) Grad: 49593.2852  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7251(0.7323) Grad: 63645.8672  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7290(0.7290) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.7323  avg_val_loss: 0.8159  time: 130s\n",
      "Epoch 25 - avg_train_Score: 0.7323 avgScore: 0.8159\n",
      "Epoch 25 - Save Best Score: 0.8159 Model\n",
      "Epoch 25 - Save Best Loss: 0.8159 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8162(0.8159) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 11m 2s) Loss: 0.9206(0.9206) Grad: 222290.8438  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6272(0.7284) Grad: 55997.0234  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8693(0.7264) Grad: 48688.0195  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7180(0.7180) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.7264  avg_val_loss: 0.8179  time: 130s\n",
      "Epoch 26 - avg_train_Score: 0.7264 avgScore: 0.8179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8052(0.8179) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 41s) Loss: 0.7987(0.7987) Grad: 299577.8750  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9659(0.7185) Grad: 101006.1250  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6974(0.7198) Grad: 159335.2344  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7153(0.7153) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8018(0.8179) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.7198  avg_val_loss: 0.8179  time: 130s\n",
      "Epoch 27 - avg_train_Score: 0.7198 avgScore: 0.8179\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 11m 5s) Loss: 0.7056(0.7056) Grad: 186765.0312  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7290(0.7079) Grad: 122854.5703  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8338(0.7039) Grad: 94722.5000  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7152(0.7152) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.7039  avg_val_loss: 0.8157  time: 130s\n",
      "Epoch 28 - avg_train_Score: 0.7039 avgScore: 0.8157\n",
      "Epoch 28 - Save Best Score: 0.8157 Model\n",
      "Epoch 28 - Save Best Loss: 0.8157 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7912(0.8157) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 0.9627(0.9627) Grad: 211470.2500  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7902(0.7004) Grad: 48603.1172  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2600(0.7055) Grad: 59024.7852  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.7374(0.7374) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8112(0.8170) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.7055  avg_val_loss: 0.8170  time: 130s\n",
      "Epoch 29 - avg_train_Score: 0.7055 avgScore: 0.8170\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 11m 40s) Loss: 0.7053(0.7053) Grad: 236189.5156  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.7033(0.7001) Grad: 87178.8047  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.5093(0.6997) Grad: 94470.2969  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7090(0.7090) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.6997  avg_val_loss: 0.8137  time: 131s\n",
      "Epoch 30 - avg_train_Score: 0.6997 avgScore: 0.8137\n",
      "Epoch 30 - Save Best Score: 0.8137 Model\n",
      "Epoch 30 - Save Best Loss: 0.8137 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8071(0.8137) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 11m 22s) Loss: 0.5930(0.5930) Grad: 255426.2969  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6065(0.7005) Grad: 131534.0781  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9229(0.6992) Grad: 84580.3047  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7038(0.7038) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.6992  avg_val_loss: 0.8110  time: 130s\n",
      "Epoch 31 - avg_train_Score: 0.6992 avgScore: 0.8110\n",
      "Epoch 31 - Save Best Score: 0.8110 Model\n",
      "Epoch 31 - Save Best Loss: 0.8110 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8050(0.8110) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 11m 5s) Loss: 0.5112(0.5112) Grad: 194013.5781  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.6048(0.6946) Grad: 43699.1484  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6601(0.6935) Grad: 53786.1016  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7224(0.7224) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.6935  avg_val_loss: 0.8095  time: 131s\n",
      "Epoch 32 - avg_train_Score: 0.6935 avgScore: 0.8095\n",
      "Epoch 32 - Save Best Score: 0.8095 Model\n",
      "Epoch 32 - Save Best Loss: 0.8095 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7919(0.8095) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 13m 12s) Loss: 0.7682(0.7682) Grad: 205209.2969  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5231(0.6801) Grad: 86671.7891  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8557(0.6804) Grad: 93434.5000  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.7306(0.7306) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.6804  avg_val_loss: 0.8105  time: 130s\n",
      "Epoch 33 - avg_train_Score: 0.6804 avgScore: 0.8105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7952(0.8105) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.5619(0.5619) Grad: 178030.3906  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5291(0.6773) Grad: 133395.9688  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6723(0.6771) Grad: 107851.3750  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7254(0.7254) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.6771  avg_val_loss: 0.8084  time: 130s\n",
      "Epoch 34 - avg_train_Score: 0.6771 avgScore: 0.8084\n",
      "Epoch 34 - Save Best Score: 0.8084 Model\n",
      "Epoch 34 - Save Best Loss: 0.8084 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7850(0.8084) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 56s) Loss: 0.7343(0.7343) Grad: 174318.2344  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8697(0.6772) Grad: 105854.7031  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5290(0.6771) Grad: 121771.9453  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7220(0.7220) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7878(0.8093) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.6771  avg_val_loss: 0.8093  time: 130s\n",
      "Epoch 35 - avg_train_Score: 0.6771 avgScore: 0.8093\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 10m 41s) Loss: 0.6218(0.6218) Grad: 208452.9062  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7470(0.6731) Grad: 55499.4883  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2106(0.6769) Grad: 43298.0625  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.7196(0.7196) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.6769  avg_val_loss: 0.8090  time: 130s\n",
      "Epoch 36 - avg_train_Score: 0.6769 avgScore: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7797(0.8090) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 54s) Loss: 0.9418(0.9418) Grad: 164877.9062  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7792(0.6797) Grad: 102873.2500  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.5788(0.6769) Grad: 92108.2812  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7257(0.7257) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7865(0.8092) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.6769  avg_val_loss: 0.8092  time: 130s\n",
      "Epoch 37 - avg_train_Score: 0.6769 avgScore: 0.8092\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 10m 46s) Loss: 0.7280(0.7280) Grad: 242457.8125  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6585(0.6680) Grad: 83904.0547  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5332(0.6665) Grad: 93120.7344  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7130(0.7130) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.6665  avg_val_loss: 0.8071  time: 130s\n",
      "Epoch 38 - avg_train_Score: 0.6665 avgScore: 0.8071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.7888(0.8071) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Save Best Score: 0.8071 Model\n",
      "Epoch 38 - Save Best Loss: 0.8071 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 10m 52s) Loss: 0.5869(0.5869) Grad: 195575.5000  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5289(0.6667) Grad: 113739.7578  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6412(0.6686) Grad: 96991.1328  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7136(0.7136) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.6686  avg_val_loss: 0.8080  time: 130s\n",
      "Epoch 39 - avg_train_Score: 0.6686 avgScore: 0.8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7872(0.8080) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 10m 56s) Loss: 0.6399(0.6399) Grad: 259212.7031  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5934(0.6608) Grad: 40377.2539  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5052(0.6625) Grad: 46536.8008  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7187(0.7187) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.6625  avg_val_loss: 0.8076  time: 130s\n",
      "Epoch 40 - avg_train_Score: 0.6625 avgScore: 0.8076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7839(0.8076) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 0 result ==========\n",
      "score: 0.8076\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8675, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 4.9809(4.9809) Grad: 268638.4375  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 5.4710(5.3754) Grad: 48768.8594  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 4.9757(5.3557) Grad: 60325.4219  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 5.4394(5.4394) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 5.3557  avg_val_loss: 5.1308  time: 131s\n",
      "Epoch 1 - avg_train_Score: 5.3557 avgScore: 5.1308\n",
      "Epoch 1 - Save Best Score: 5.1308 Model\n",
      "Epoch 1 - Save Best Loss: 5.1308 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 3.5133(5.1308) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 5.9145(5.9145) Grad: 201811.2344  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 4.6922(4.9712) Grad: 118437.6719  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 5.6176(4.9574) Grad: 44366.0430  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 4.9994(4.9994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 4.9574  avg_val_loss: 4.6545  time: 130s\n",
      "Epoch 2 - avg_train_Score: 4.9574 avgScore: 4.6545\n",
      "Epoch 2 - Save Best Score: 4.6545 Model\n",
      "Epoch 2 - Save Best Loss: 4.6545 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 3.2056(4.6545) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 11m 18s) Loss: 4.8514(4.8514) Grad: 224870.8594  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 2.0874(2.9289) Grad: 15905.2324  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.5142(2.8315) Grad: 17726.7109  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 2.0147(2.0147) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 2.8315  avg_val_loss: 1.7268  time: 131s\n",
      "Epoch 3 - avg_train_Score: 2.8315 avgScore: 1.7268\n",
      "Epoch 3 - Save Best Score: 1.7268 Model\n",
      "Epoch 3 - Save Best Loss: 1.7268 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.4462(1.7268) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 11m 29s) Loss: 1.3915(1.3915) Grad: 396573.7188  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.4465(1.4434) Grad: 18779.4941  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.4321(1.4351) Grad: 20772.3906  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 1.1388(1.1388) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.4351  avg_val_loss: 1.0667  time: 130s\n",
      "Epoch 4 - avg_train_Score: 1.4351 avgScore: 1.0667\n",
      "Epoch 4 - Save Best Score: 1.0667 Model\n",
      "Epoch 4 - Save Best Loss: 1.0667 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.1240(1.0667) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 11m 25s) Loss: 1.9135(1.9135) Grad: 275008.8750  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0399(1.2590) Grad: 17489.8047  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2090(1.2570) Grad: 19741.0059  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 1.0843(1.0843) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.2570  avg_val_loss: 0.9930  time: 130s\n",
      "Epoch 5 - avg_train_Score: 1.2570 avgScore: 0.9930\n",
      "Epoch 5 - Save Best Score: 0.9930 Model\n",
      "Epoch 5 - Save Best Loss: 0.9930 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0031(0.9930) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 10m 57s) Loss: 1.1440(1.1440) Grad: 309915.5312  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.0009(1.1733) Grad: 14438.8203  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.2300(1.1727) Grad: 28822.6113  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 1.0230(1.0230) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9473(0.9549) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 1.1727  avg_val_loss: 0.9549  time: 131s\n",
      "Epoch 6 - avg_train_Score: 1.1727 avgScore: 0.9549\n",
      "Epoch 6 - Save Best Score: 0.9549 Model\n",
      "Epoch 6 - Save Best Loss: 0.9549 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 3s) Loss: 1.1021(1.1021) Grad: 332539.8125  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.3906(1.0796) Grad: 29191.5781  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2740(1.0780) Grad: 39902.6797  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.9913(0.9913) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 1.0780  avg_val_loss: 0.9270  time: 130s\n",
      "Epoch 7 - avg_train_Score: 1.0780 avgScore: 0.9270\n",
      "Epoch 7 - Save Best Score: 0.9270 Model\n",
      "Epoch 7 - Save Best Loss: 0.9270 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9035(0.9270) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 11m 10s) Loss: 0.9025(0.9025) Grad: 280861.2812  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7381(0.9937) Grad: 47952.2891  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.4331(0.9957) Grad: 17909.0918  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 49s) Loss: 0.9669(0.9669) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.9957  avg_val_loss: 0.9126  time: 131s\n",
      "Epoch 8 - avg_train_Score: 0.9957 avgScore: 0.9126\n",
      "Epoch 8 - Save Best Score: 0.9126 Model\n",
      "Epoch 8 - Save Best Loss: 0.9126 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8724(0.9126) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 10m 58s) Loss: 0.9974(0.9974) Grad: 221320.0000  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8898(1.0142) Grad: 42200.6953  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.0057(1.0141) Grad: 44879.5039  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.9458(0.9458) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 1.0141  avg_val_loss: 0.9102  time: 130s\n",
      "Epoch 9 - avg_train_Score: 1.0141 avgScore: 0.9102\n",
      "Epoch 9 - Save Best Score: 0.9102 Model\n",
      "Epoch 9 - Save Best Loss: 0.9102 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8473(0.9102) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.8611(0.8611) Grad: 329055.2500  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8395(0.9528) Grad: 36841.1641  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.0155(0.9550) Grad: 30535.8809  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.9630(0.9630) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.9550  avg_val_loss: 0.8988  time: 131s\n",
      "Epoch 10 - avg_train_Score: 0.9550 avgScore: 0.8988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8688(0.8988) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Save Best Score: 0.8988 Model\n",
      "Epoch 10 - Save Best Loss: 0.8988 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 10m 53s) Loss: 0.9498(0.9498) Grad: nan  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0515(0.9463) Grad: 24075.0000  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7502(0.9457) Grad: 32649.7812  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.9735(0.9735) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.9457  avg_val_loss: 0.8976  time: 131s\n",
      "Epoch 11 - avg_train_Score: 0.9457 avgScore: 0.8976\n",
      "Epoch 11 - Save Best Score: 0.8976 Model\n",
      "Epoch 11 - Save Best Loss: 0.8976 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8996(0.8976) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 11m 17s) Loss: 0.7364(0.7364) Grad: 281931.0625  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8258(0.8905) Grad: 63275.3125  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7911(0.8936) Grad: 34487.1172  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 51s) Loss: 0.9543(0.9543) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.8936  avg_val_loss: 0.8821  time: 131s\n",
      "Epoch 12 - avg_train_Score: 0.8936 avgScore: 0.8821\n",
      "Epoch 12 - Save Best Score: 0.8821 Model\n",
      "Epoch 12 - Save Best Loss: 0.8821 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9008(0.8821) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 0.7302(0.7302) Grad: 385124.1250  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8312(0.8766) Grad: 67992.6172  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.9836(0.8789) Grad: 75742.9375  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.9372(0.9372) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.8789  avg_val_loss: 0.8788  time: 131s\n",
      "Epoch 13 - avg_train_Score: 0.8789 avgScore: 0.8788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8942(0.8788) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Save Best Score: 0.8788 Model\n",
      "Epoch 13 - Save Best Loss: 0.8788 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 24s) Loss: 0.8093(0.8093) Grad: nan  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9236(0.8820) Grad: 34233.7500  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7861(0.8829) Grad: 33988.3242  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.9517(0.9517) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.8829  avg_val_loss: 0.8837  time: 130s\n",
      "Epoch 14 - avg_train_Score: 0.8829 avgScore: 0.8837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8712(0.8837) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 11m 16s) Loss: 0.8045(0.8045) Grad: 334002.4062  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8150(0.8697) Grad: 57531.6211  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6325(0.8710) Grad: 58220.7188  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.9538(0.9538) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.8710  avg_val_loss: 0.8736  time: 131s\n",
      "Epoch 15 - avg_train_Score: 0.8710 avgScore: 0.8736\n",
      "Epoch 15 - Save Best Score: 0.8736 Model\n",
      "Epoch 15 - Save Best Loss: 0.8736 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8689(0.8736) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 11m 0s) Loss: 0.6782(0.6782) Grad: 226660.6094  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.9148(0.8527) Grad: 24398.1641  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8651(0.8588) Grad: 18188.3535  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.9659(0.9659) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.8588  avg_val_loss: 0.8707  time: 131s\n",
      "Epoch 16 - avg_train_Score: 0.8588 avgScore: 0.8707\n",
      "Epoch 16 - Save Best Score: 0.8707 Model\n",
      "Epoch 16 - Save Best Loss: 0.8707 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8772(0.8707) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 11m 20s) Loss: 0.8544(0.8544) Grad: 292407.2812  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7613(0.8742) Grad: 62252.2734  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6506(0.8670) Grad: 44763.1797  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.9300(0.9300) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.8670  avg_val_loss: 0.8675  time: 130s\n",
      "Epoch 17 - avg_train_Score: 0.8670 avgScore: 0.8675\n",
      "Epoch 17 - Save Best Score: 0.8675 Model\n",
      "Epoch 17 - Save Best Loss: 0.8675 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8584(0.8675) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 48s) Loss: 0.7031(0.7031) Grad: 177779.2812  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0161(0.8180) Grad: 64291.9805  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8714(0.8181) Grad: 82093.2422  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 0.9228(0.9228) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.8181  avg_val_loss: 0.8595  time: 130s\n",
      "Epoch 18 - avg_train_Score: 0.8181 avgScore: 0.8595\n",
      "Epoch 18 - Save Best Score: 0.8595 Model\n",
      "Epoch 18 - Save Best Loss: 0.8595 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8312(0.8595) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 10m 59s) Loss: 0.7202(0.7202) Grad: 205087.3906  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9475(0.7839) Grad: 84497.6484  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6491(0.7875) Grad: 82960.2578  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.9164(0.9164) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.7875  avg_val_loss: 0.8529  time: 131s\n",
      "Epoch 19 - avg_train_Score: 0.7875 avgScore: 0.8529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8088(0.8529) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Save Best Score: 0.8529 Model\n",
      "Epoch 19 - Save Best Loss: 0.8529 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 3s) Loss: 0.7736(0.7736) Grad: 248247.9844  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0781(0.7967) Grad: 54938.1836  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7274(0.7980) Grad: 75515.5156  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.9100(0.9100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.7980  avg_val_loss: 0.8552  time: 130s\n",
      "Epoch 20 - avg_train_Score: 0.7980 avgScore: 0.8552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8087(0.8552) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 11m 31s) Loss: 0.7257(0.7257) Grad: 216289.8594  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8111(0.7743) Grad: 46983.4336  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7908(0.7763) Grad: 57355.5977  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 49s) Loss: 0.9076(0.9076) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.7763  avg_val_loss: 0.8551  time: 131s\n",
      "Epoch 21 - avg_train_Score: 0.7763 avgScore: 0.8551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8501(0.8551) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 11m 12s) Loss: 0.8030(0.8030) Grad: 258303.5312  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6790(0.7702) Grad: 53785.8242  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8211(0.7728) Grad: 58715.2266  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.9095(0.9095) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8209(0.8532) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.7728  avg_val_loss: 0.8532  time: 130s\n",
      "Epoch 22 - avg_train_Score: 0.7728 avgScore: 0.8532\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 11m 1s) Loss: 0.7014(0.7014) Grad: 311083.6250  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6684(0.7599) Grad: 78259.3750  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7105(0.7608) Grad: 52051.5586  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.9227(0.9227) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.7608  avg_val_loss: 0.8519  time: 131s\n",
      "Epoch 23 - avg_train_Score: 0.7608 avgScore: 0.8519\n",
      "Epoch 23 - Save Best Score: 0.8519 Model\n",
      "Epoch 23 - Save Best Loss: 0.8519 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8367(0.8519) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 51s) Loss: 0.7491(0.7491) Grad: 291189.4375  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5464(0.7316) Grad: 57772.3477  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6000(0.7341) Grad: 50756.6328  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.9181(0.9181) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.7341  avg_val_loss: 0.8490  time: 130s\n",
      "Epoch 24 - avg_train_Score: 0.7341 avgScore: 0.8490\n",
      "Epoch 24 - Save Best Score: 0.8490 Model\n",
      "Epoch 24 - Save Best Loss: 0.8490 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8418(0.8490) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 14m 7s) Loss: 0.8610(0.8610) Grad: 177367.7344  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.9270(0.7386) Grad: 55781.3125  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.5884(0.7384) Grad: 45487.7109  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.9197(0.9197) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.7384  avg_val_loss: 0.8497  time: 131s\n",
      "Epoch 25 - avg_train_Score: 0.7384 avgScore: 0.8497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8498(0.8497) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 11m 5s) Loss: 0.6127(0.6127) Grad: 191215.5781  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5859(0.7327) Grad: 51728.3594  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6424(0.7314) Grad: 89237.0547  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.9032(0.9032) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8355(0.8450) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.7314  avg_val_loss: 0.8450  time: 131s\n",
      "Epoch 26 - avg_train_Score: 0.7314 avgScore: 0.8450\n",
      "Epoch 26 - Save Best Score: 0.8450 Model\n",
      "Epoch 26 - Save Best Loss: 0.8450 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 0.5826(0.5826) Grad: 202220.2188  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6476(0.7236) Grad: 105508.9688  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5859(0.7223) Grad: 179627.7344  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 0.9129(0.9129) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.7223  avg_val_loss: 0.8457  time: 130s\n",
      "Epoch 27 - avg_train_Score: 0.7223 avgScore: 0.8457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8417(0.8457) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 10m 47s) Loss: 0.8268(0.8268) Grad: 197259.7500  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.6217(0.7121) Grad: 181547.7031  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6507(0.7138) Grad: 92315.9688  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.9114(0.9114) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.7138  avg_val_loss: 0.8432  time: 131s\n",
      "Epoch 28 - avg_train_Score: 0.7138 avgScore: 0.8432\n",
      "Epoch 28 - Save Best Score: 0.8432 Model\n",
      "Epoch 28 - Save Best Loss: 0.8432 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8453(0.8432) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 10m 51s) Loss: 0.5849(0.5849) Grad: 171886.0625  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7464(0.7099) Grad: 56378.9297  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8654(0.7085) Grad: 58020.7227  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.9024(0.9024) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.7085  avg_val_loss: 0.8413  time: 130s\n",
      "Epoch 29 - avg_train_Score: 0.7085 avgScore: 0.8413\n",
      "Epoch 29 - Save Best Score: 0.8413 Model\n",
      "Epoch 29 - Save Best Loss: 0.8413 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8303(0.8413) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 11m 3s) Loss: 0.6616(0.6616) Grad: 195696.2188  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6370(0.6993) Grad: 47560.0781  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8507(0.7004) Grad: 48812.7305  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8930(0.8930) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.7004  avg_val_loss: 0.8409  time: 130s\n",
      "Epoch 30 - avg_train_Score: 0.7004 avgScore: 0.8409\n",
      "Epoch 30 - Save Best Score: 0.8409 Model\n",
      "Epoch 30 - Save Best Loss: 0.8409 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8381(0.8409) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 0.5025(0.5025) Grad: 210029.1094  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5587(0.7024) Grad: 139163.4062  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6658(0.7034) Grad: 95993.6562  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.9014(0.9014) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.7034  avg_val_loss: 0.8411  time: 131s\n",
      "Epoch 31 - avg_train_Score: 0.7034 avgScore: 0.8411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8323(0.8411) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 11m 20s) Loss: 0.7952(0.7952) Grad: 226072.1875  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7691(0.6871) Grad: 44919.4844  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6447(0.6905) Grad: 43460.0625  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.9154(0.9154) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.6905  avg_val_loss: 0.8404  time: 131s\n",
      "Epoch 32 - avg_train_Score: 0.6905 avgScore: 0.8404\n",
      "Epoch 32 - Save Best Score: 0.8404 Model\n",
      "Epoch 32 - Save Best Loss: 0.8404 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8371(0.8404) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 11m 1s) Loss: 0.6018(0.6018) Grad: 180265.8750  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8700(0.6925) Grad: 46822.3516  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6381(0.6946) Grad: 58985.1016  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.8966(0.8966) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.6946  avg_val_loss: 0.8407  time: 130s\n",
      "Epoch 33 - avg_train_Score: 0.6946 avgScore: 0.8407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8545(0.8407) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 11m 43s) Loss: 0.6367(0.6367) Grad: 167590.7344  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5699(0.6708) Grad: 115598.4141  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5924(0.6724) Grad: 100814.4688  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8943(0.8943) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.6724  avg_val_loss: 0.8396  time: 130s\n",
      "Epoch 34 - avg_train_Score: 0.6724 avgScore: 0.8396\n",
      "Epoch 34 - Save Best Score: 0.8396 Model\n",
      "Epoch 34 - Save Best Loss: 0.8396 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8487(0.8396) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.5779(0.5779) Grad: 187922.2656  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6675(0.6874) Grad: 49697.3828  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7444(0.6904) Grad: 64216.4883  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8918(0.8918) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8503(0.8399) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.6904  avg_val_loss: 0.8399  time: 131s\n",
      "Epoch 35 - avg_train_Score: 0.6904 avgScore: 0.8399\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 11m 4s) Loss: 0.6719(0.6719) Grad: 179411.3281  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8540(0.6867) Grad: 60550.9570  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.0608(0.6864) Grad: 67467.3125  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.8988(0.8988) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.6864  avg_val_loss: 0.8391  time: 130s\n",
      "Epoch 36 - avg_train_Score: 0.6864 avgScore: 0.8391\n",
      "Epoch 36 - Save Best Score: 0.8391 Model\n",
      "Epoch 36 - Save Best Loss: 0.8391 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8398(0.8391) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 51s) Loss: 0.9093(0.9093) Grad: 210985.9219  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6012(0.6681) Grad: 122743.5078  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.5748(0.6697) Grad: 68706.0000  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 0.9078(0.9078) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.6697  avg_val_loss: 0.8386  time: 131s\n",
      "Epoch 37 - avg_train_Score: 0.6697 avgScore: 0.8386\n",
      "Epoch 37 - Save Best Score: 0.8386 Model\n",
      "Epoch 37 - Save Best Loss: 0.8386 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8475(0.8386) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 11m 26s) Loss: 0.6891(0.6891) Grad: 170179.7656  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6654(0.6580) Grad: 22187.3301  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5298(0.6575) Grad: 25662.9336  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8949(0.8949) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.6575  avg_val_loss: 0.8383  time: 130s\n",
      "Epoch 38 - avg_train_Score: 0.6575 avgScore: 0.8383\n",
      "Epoch 38 - Save Best Score: 0.8383 Model\n",
      "Epoch 38 - Save Best Loss: 0.8383 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8442(0.8383) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 10m 47s) Loss: 0.8415(0.8415) Grad: 208018.6406  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8022(0.6792) Grad: 76254.8828  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.1519(0.6803) Grad: 86896.3750  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.8915(0.8915) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.6803  avg_val_loss: 0.8378  time: 130s\n",
      "Epoch 39 - avg_train_Score: 0.6803 avgScore: 0.8378\n",
      "Epoch 39 - Save Best Score: 0.8378 Model\n",
      "Epoch 39 - Save Best Loss: 0.8378 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8424(0.8378) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.5470(0.5470) Grad: 165899.4219  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.4783(0.6593) Grad: 48505.1641  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6731(0.6582) Grad: 39098.5625  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.8920(0.8920) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.6582  avg_val_loss: 0.8367  time: 130s\n",
      "Epoch 40 - avg_train_Score: 0.6582 avgScore: 0.8367\n",
      "Epoch 40 - Save Best Score: 0.8367 Model\n",
      "Epoch 40 - Save Best Loss: 0.8367 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8439(0.8367) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 1 result ==========\n",
      "score: 0.8367\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 58s) Loss: 5.7958(5.7958) Grad: 308165.1875  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 5.3489(5.3166) Grad: 22416.1348  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 5.4593(5.3035) Grad: 47309.6953  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 5.0385(5.0385) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 5.3035  avg_val_loss: 5.1825  time: 130s\n",
      "Epoch 1 - avg_train_Score: 5.3035 avgScore: 5.1825\n",
      "Epoch 1 - Save Best Score: 5.1825 Model\n",
      "Epoch 1 - Save Best Loss: 5.1825 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 5.0031(5.1825) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 11m 12s) Loss: 5.3826(5.3826) Grad: 228357.5000  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 4.4945(4.9313) Grad: 39284.0977  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 4.4948(4.9132) Grad: 16896.8789  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 4.5145(4.5145) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 4.9132  avg_val_loss: 4.6683  time: 131s\n",
      "Epoch 2 - avg_train_Score: 4.9132 avgScore: 4.6683\n",
      "Epoch 2 - Save Best Score: 4.6683 Model\n",
      "Epoch 2 - Save Best Loss: 4.6683 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.5385(4.6683) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 11m 39s) Loss: 4.1563(4.1563) Grad: 414276.0000  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.9036(3.3050) Grad: 114589.7578  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 2.3152(3.2148) Grad: 22683.5215  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 2.5444(2.5444) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 3.2148  avg_val_loss: 2.7536  time: 131s\n",
      "Epoch 3 - avg_train_Score: 3.2148 avgScore: 2.7536\n",
      "Epoch 3 - Save Best Score: 2.7536 Model\n",
      "Epoch 3 - Save Best Loss: 2.7536 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 2.7960(2.7536) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 10m 50s) Loss: 1.8590(1.8590) Grad: 450205.6562  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.5248(1.6772) Grad: 11953.5293  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.2947(1.6840) Grad: 13830.8955  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.9928(0.9928) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.6840  avg_val_loss: 1.1748  time: 131s\n",
      "Epoch 4 - avg_train_Score: 1.6840 avgScore: 1.1748\n",
      "Epoch 4 - Save Best Score: 1.1748 Model\n",
      "Epoch 4 - Save Best Loss: 1.1748 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.1686(1.1748) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 11m 37s) Loss: 1.4476(1.4476) Grad: nan  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.3627(1.3969) Grad: 22739.2754  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.1702(1.3882) Grad: 24953.4277  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.8340(0.8340) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.3882  avg_val_loss: 1.0098  time: 131s\n",
      "Epoch 5 - avg_train_Score: 1.3882 avgScore: 1.0098\n",
      "Epoch 5 - Save Best Score: 1.0098 Model\n",
      "Epoch 5 - Save Best Loss: 1.0098 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 1.0495(1.0098) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 11m 4s) Loss: 1.1835(1.1835) Grad: 293310.8125  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2248(1.1515) Grad: 63683.0508  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.4137(1.1464) Grad: 44071.4570  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7982(0.7982) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 1.1464  avg_val_loss: 0.9551  time: 130s\n",
      "Epoch 6 - avg_train_Score: 1.1464 avgScore: 0.9551\n",
      "Epoch 6 - Save Best Score: 0.9551 Model\n",
      "Epoch 6 - Save Best Loss: 0.9551 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0142(0.9551) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 37s) Loss: 1.1372(1.1372) Grad: inf  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.2767(1.1191) Grad: 69025.5625  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.1013(1.1189) Grad: 30958.4629  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.8039(0.8039) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 1.1189  avg_val_loss: 0.9269  time: 131s\n",
      "Epoch 7 - avg_train_Score: 1.1189 avgScore: 0.9269\n",
      "Epoch 7 - Save Best Score: 0.9269 Model\n",
      "Epoch 7 - Save Best Loss: 0.9269 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.9466(0.9269) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 1.0347(1.0347) Grad: 275980.9688  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8988(1.0753) Grad: 52652.7422  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9785(1.0757) Grad: 32849.9805  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.8048(0.8048) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9696(0.9112) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 1.0757  avg_val_loss: 0.9112  time: 130s\n",
      "Epoch 8 - avg_train_Score: 1.0757 avgScore: 0.9112\n",
      "Epoch 8 - Save Best Score: 0.9112 Model\n",
      "Epoch 8 - Save Best Loss: 0.9112 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 11s) Loss: 0.9655(0.9655) Grad: 346117.0625  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8698(0.9902) Grad: 66222.5156  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.9234(0.9957) Grad: 80702.0547  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.7882(0.7882) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9410(0.8877) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.9957  avg_val_loss: 0.8877  time: 131s\n",
      "Epoch 9 - avg_train_Score: 0.9957 avgScore: 0.8877\n",
      "Epoch 9 - Save Best Score: 0.8877 Model\n",
      "Epoch 9 - Save Best Loss: 0.8877 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 3s) Loss: 0.9188(0.9188) Grad: 430974.2188  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9454(0.9654) Grad: 83833.3984  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8819(0.9656) Grad: 59562.3203  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7708(0.7708) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.9656  avg_val_loss: 0.8799  time: 131s\n",
      "Epoch 10 - avg_train_Score: 0.9656 avgScore: 0.8799\n",
      "Epoch 10 - Save Best Score: 0.8799 Model\n",
      "Epoch 10 - Save Best Loss: 0.8799 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9252(0.8799) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 11m 3s) Loss: 0.8238(0.8238) Grad: 273151.3125  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.2342(0.9389) Grad: 108485.8125  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.0815(0.9399) Grad: 92427.8906  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7635(0.7635) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.9399  avg_val_loss: 0.8744  time: 131s\n",
      "Epoch 11 - avg_train_Score: 0.9399 avgScore: 0.8744\n",
      "Epoch 11 - Save Best Score: 0.8744 Model\n",
      "Epoch 11 - Save Best Loss: 0.8744 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.9222(0.8744) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 1.1725(1.1725) Grad: 285013.6250  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.1925(0.9936) Grad: 34391.4805  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0278(0.9967) Grad: 190537.5625  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7630(0.7630) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.9967  avg_val_loss: 0.8741  time: 130s\n",
      "Epoch 12 - avg_train_Score: 0.9967 avgScore: 0.8741\n",
      "Epoch 12 - Save Best Score: 0.8741 Model\n",
      "Epoch 12 - Save Best Loss: 0.8741 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8980(0.8741) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 11m 8s) Loss: 1.1955(1.1955) Grad: 395008.9375  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.3423(0.9764) Grad: 20833.3418  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.1211(0.9777) Grad: 30614.0957  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7717(0.7717) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.9777  avg_val_loss: 0.8769  time: 131s\n",
      "Epoch 13 - avg_train_Score: 0.9777 avgScore: 0.8769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8897(0.8769) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 43s) Loss: 1.1782(1.1782) Grad: 254740.0469  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7890(0.9317) Grad: 50590.0352  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7820(0.9287) Grad: 33851.4102  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.7784(0.7784) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.9287  avg_val_loss: 0.8698  time: 130s\n",
      "Epoch 14 - avg_train_Score: 0.9287 avgScore: 0.8698\n",
      "Epoch 14 - Save Best Score: 0.8698 Model\n",
      "Epoch 14 - Save Best Loss: 0.8698 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9162(0.8698) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 11m 13s) Loss: 0.8249(0.8249) Grad: 304755.6875  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.6697(0.8780) Grad: 83794.1094  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.9501(0.8775) Grad: 80494.3984  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.7717(0.7717) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.8775  avg_val_loss: 0.8536  time: 131s\n",
      "Epoch 15 - avg_train_Score: 0.8775 avgScore: 0.8536\n",
      "Epoch 15 - Save Best Score: 0.8536 Model\n",
      "Epoch 15 - Save Best Loss: 0.8536 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9273(0.8536) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 11m 51s) Loss: 0.7037(0.7037) Grad: 232395.6094  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8629(0.8559) Grad: 70446.1406  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8391(0.8546) Grad: 70651.8516  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.7510(0.7510) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.8546  avg_val_loss: 0.8437  time: 131s\n",
      "Epoch 16 - avg_train_Score: 0.8546 avgScore: 0.8437\n",
      "Epoch 16 - Save Best Score: 0.8437 Model\n",
      "Epoch 16 - Save Best Loss: 0.8437 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8972(0.8437) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 11m 50s) Loss: 1.0446(1.0446) Grad: 187900.8750  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9422(0.8353) Grad: 114731.3125  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6476(0.8334) Grad: 69253.6484  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7259(0.7259) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.8334  avg_val_loss: 0.8438  time: 130s\n",
      "Epoch 17 - avg_train_Score: 0.8334 avgScore: 0.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8989(0.8438) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 12m 44s) Loss: 0.7444(0.7444) Grad: 238707.8906  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.7512(0.8224) Grad: 54506.4883  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6723(0.8233) Grad: 28852.4375  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.7288(0.7288) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.8233  avg_val_loss: 0.8434  time: 131s\n",
      "Epoch 18 - avg_train_Score: 0.8233 avgScore: 0.8434\n",
      "Epoch 18 - Save Best Score: 0.8434 Model\n",
      "Epoch 18 - Save Best Loss: 0.8434 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9543(0.8434) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 11m 24s) Loss: 0.7807(0.7807) Grad: 227914.3594  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6153(0.8176) Grad: 69034.7266  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0305(0.8206) Grad: 79125.6484  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 0.7196(0.7196) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.8206  avg_val_loss: 0.8369  time: 130s\n",
      "Epoch 19 - avg_train_Score: 0.8206 avgScore: 0.8369\n",
      "Epoch 19 - Save Best Score: 0.8369 Model\n",
      "Epoch 19 - Save Best Loss: 0.8369 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8815(0.8369) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 21s) Loss: 0.9465(0.9465) Grad: 283446.0312  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8639(0.8415) Grad: 39491.8516  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0762(0.8390) Grad: 44899.5000  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7486(0.7486) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.8390  avg_val_loss: 0.8460  time: 130s\n",
      "Epoch 20 - avg_train_Score: 0.8390 avgScore: 0.8460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8999(0.8460) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 11m 1s) Loss: 1.0185(1.0185) Grad: 237238.7500  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7656(0.8053) Grad: 50234.3359  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8645(0.8019) Grad: 109193.8203  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7414(0.7414) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.8019  avg_val_loss: 0.8393  time: 130s\n",
      "Epoch 21 - avg_train_Score: 0.8019 avgScore: 0.8393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9197(0.8393) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 10m 59s) Loss: 1.2546(1.2546) Grad: 235841.9375  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7315(0.7717) Grad: 42655.0742  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7047(0.7706) Grad: 66249.9531  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 53s) Loss: 0.7392(0.7392) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.7706  avg_val_loss: 0.8326  time: 131s\n",
      "Epoch 22 - avg_train_Score: 0.7706 avgScore: 0.8326\n",
      "Epoch 22 - Save Best Score: 0.8326 Model\n",
      "Epoch 22 - Save Best Loss: 0.8326 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8893(0.8326) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 11m 27s) Loss: 0.8879(0.8879) Grad: 334881.3750  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.7593(0.7752) Grad: 47693.0391  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6572(0.7798) Grad: 38055.8789  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7406(0.7406) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.7798  avg_val_loss: 0.8361  time: 131s\n",
      "Epoch 23 - avg_train_Score: 0.7798 avgScore: 0.8361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8995(0.8361) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 11m 43s) Loss: 0.8281(0.8281) Grad: 284836.4375  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.7829(0.7971) Grad: 39726.5508  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.1554(0.8003) Grad: 36739.2891  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7443(0.7443) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.8003  avg_val_loss: 0.8348  time: 131s\n",
      "Epoch 24 - avg_train_Score: 0.8003 avgScore: 0.8348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8474(0.8348) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 11m 36s) Loss: 0.6227(0.6227) Grad: 203167.1250  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.7662(0.7467) Grad: 96811.1875  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7434(0.7487) Grad: 59559.5391  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.7425(0.7425) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.7487  avg_val_loss: 0.8288  time: 131s\n",
      "Epoch 25 - avg_train_Score: 0.7487 avgScore: 0.8288\n",
      "Epoch 25 - Save Best Score: 0.8288 Model\n",
      "Epoch 25 - Save Best Loss: 0.8288 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8491(0.8288) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 0.8427(0.8427) Grad: 277571.1250  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6307(0.7456) Grad: 77872.2188  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6112(0.7440) Grad: 59395.6094  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7257(0.7257) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.7440  avg_val_loss: 0.8249  time: 131s\n",
      "Epoch 26 - avg_train_Score: 0.7440 avgScore: 0.8249\n",
      "Epoch 26 - Save Best Score: 0.8249 Model\n",
      "Epoch 26 - Save Best Loss: 0.8249 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8726(0.8249) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 0.7196(0.7196) Grad: 200946.0938  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.9538(0.7436) Grad: 121680.8672  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6071(0.7395) Grad: 56649.3047  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7277(0.7277) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.7395  avg_val_loss: 0.8225  time: 131s\n",
      "Epoch 27 - avg_train_Score: 0.7395 avgScore: 0.8225\n",
      "Epoch 27 - Save Best Score: 0.8225 Model\n",
      "Epoch 27 - Save Best Loss: 0.8225 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8654(0.8225) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 11m 57s) Loss: 0.7536(0.7536) Grad: 246859.2344  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0955(0.7480) Grad: 15562.2139  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6788(0.7457) Grad: 14901.9600  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.7356(0.7356) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.7457  avg_val_loss: 0.8238  time: 130s\n",
      "Epoch 28 - avg_train_Score: 0.7457 avgScore: 0.8238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8880(0.8238) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 11m 13s) Loss: 0.7424(0.7424) Grad: 255663.0156  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.7957(0.7319) Grad: 23678.4141  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6110(0.7338) Grad: 29510.6758  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7326(0.7326) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.7338  avg_val_loss: 0.8261  time: 131s\n",
      "Epoch 29 - avg_train_Score: 0.7338 avgScore: 0.8261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8707(0.8261) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.6980(0.6980) Grad: 186067.4531  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6017(0.7253) Grad: 53646.9492  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6874(0.7238) Grad: 49364.4141  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7281(0.7281) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.7238  avg_val_loss: 0.8231  time: 131s\n",
      "Epoch 30 - avg_train_Score: 0.7238 avgScore: 0.8231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8548(0.8231) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 11m 12s) Loss: 0.9361(0.9361) Grad: 366534.9375  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8133(0.7124) Grad: 67545.2812  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7793(0.7112) Grad: 47963.3555  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.7423(0.7423) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.7112  avg_val_loss: 0.8235  time: 130s\n",
      "Epoch 31 - avg_train_Score: 0.7112 avgScore: 0.8235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8711(0.8235) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 11m 39s) Loss: 0.7368(0.7368) Grad: 215561.1406  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6577(0.7032) Grad: 42810.3359  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6897(0.7016) Grad: 52116.9922  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.7496(0.7496) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.7016  avg_val_loss: 0.8202  time: 130s\n",
      "Epoch 32 - avg_train_Score: 0.7016 avgScore: 0.8202\n",
      "Epoch 32 - Save Best Score: 0.8202 Model\n",
      "Epoch 32 - Save Best Loss: 0.8202 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8796(0.8202) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 11m 2s) Loss: 0.6828(0.6828) Grad: 204786.9219  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6492(0.7115) Grad: 55671.6602  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.4756(0.7078) Grad: 45942.5156  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.7451(0.7451) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.7078  avg_val_loss: 0.8202  time: 130s\n",
      "Epoch 33 - avg_train_Score: 0.7078 avgScore: 0.8202\n",
      "Epoch 33 - Save Best Score: 0.8202 Model\n",
      "Epoch 33 - Save Best Loss: 0.8202 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8821(0.8202) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 11m 25s) Loss: 0.5729(0.5729) Grad: 171552.2188  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6225(0.7019) Grad: 73221.7031  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.4881(0.7013) Grad: 59954.2773  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7388(0.7388) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.7013  avg_val_loss: 0.8182  time: 131s\n",
      "Epoch 34 - avg_train_Score: 0.7013 avgScore: 0.8182\n",
      "Epoch 34 - Save Best Score: 0.8182 Model\n",
      "Epoch 34 - Save Best Loss: 0.8182 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8713(0.8182) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 11m 21s) Loss: 0.5381(0.5381) Grad: 234204.5938  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.6967(0.6914) Grad: 24353.4609  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6186(0.6941) Grad: 27897.4375  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7358(0.7358) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.6941  avg_val_loss: 0.8177  time: 131s\n",
      "Epoch 35 - avg_train_Score: 0.6941 avgScore: 0.8177\n",
      "Epoch 35 - Save Best Score: 0.8177 Model\n",
      "Epoch 35 - Save Best Loss: 0.8177 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8619(0.8177) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 11m 28s) Loss: 0.5774(0.5774) Grad: 227642.8281  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9067(0.6821) Grad: 79457.6328  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7645(0.6833) Grad: 57775.9531  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7274(0.7274) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.6833  avg_val_loss: 0.8162  time: 131s\n",
      "Epoch 36 - avg_train_Score: 0.6833 avgScore: 0.8162\n",
      "Epoch 36 - Save Best Score: 0.8162 Model\n",
      "Epoch 36 - Save Best Loss: 0.8162 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8744(0.8162) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 41s) Loss: 0.6972(0.6972) Grad: 155106.7188  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5829(0.6820) Grad: 51826.4727  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7589(0.6867) Grad: 218016.8125  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7391(0.7391) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.6867  avg_val_loss: 0.8170  time: 130s\n",
      "Epoch 37 - avg_train_Score: 0.6867 avgScore: 0.8170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8697(0.8170) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.7837(0.7837) Grad: 249967.5469  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7814(0.7023) Grad: 70427.6016  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6811(0.7008) Grad: 73988.7812  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7419(0.7419) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.7008  avg_val_loss: 0.8166  time: 130s\n",
      "Epoch 38 - avg_train_Score: 0.7008 avgScore: 0.8166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8620(0.8166) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 11m 8s) Loss: 0.8382(0.8382) Grad: 189679.0312  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.5230(0.6859) Grad: 23120.4492  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8640(0.6889) Grad: 27302.7539  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7423(0.7423) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.6889  avg_val_loss: 0.8155  time: 131s\n",
      "Epoch 39 - avg_train_Score: 0.6889 avgScore: 0.8155\n",
      "Epoch 39 - Save Best Score: 0.8155 Model\n",
      "Epoch 39 - Save Best Loss: 0.8155 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8677(0.8155) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 11m 59s) Loss: 0.6197(0.6197) Grad: 259184.6719  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.8592(0.6811) Grad: 26968.6270  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.5433(0.6781) Grad: 37894.5391  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7426(0.7426) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.6781  avg_val_loss: 0.8153  time: 131s\n",
      "Epoch 40 - avg_train_Score: 0.6781 avgScore: 0.8153\n",
      "Epoch 40 - Save Best Score: 0.8153 Model\n",
      "Epoch 40 - Save Best Loss: 0.8153 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8657(0.8153) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 2 result ==========\n",
      "score: 0.8153\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 11m 22s) Loss: 6.5961(6.5961) Grad: 304193.5312  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 4.8533(5.2838) Grad: 46108.7188  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 5.1542(5.2649) Grad: 23919.6191  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 5.2045(5.2045) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.6651(5.2230) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 5.2649  avg_val_loss: 5.2230  time: 130s\n",
      "Epoch 1 - avg_train_Score: 5.2649 avgScore: 5.2230\n",
      "Epoch 1 - Save Best Score: 5.2230 Model\n",
      "Epoch 1 - Save Best Loss: 5.2230 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 11m 49s) Loss: 5.2619(5.2619) Grad: 483019.8438  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 4.4668(4.9107) Grad: 24962.8848  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 4.7830(4.8883) Grad: 26237.9980  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 4.6978(4.6978) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 4.8883  avg_val_loss: 4.7252  time: 131s\n",
      "Epoch 2 - avg_train_Score: 4.8883 avgScore: 4.7252\n",
      "Epoch 2 - Save Best Score: 4.7252 Model\n",
      "Epoch 2 - Save Best Loss: 4.7252 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.1968(4.7252) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 44s) Loss: 5.2096(5.2096) Grad: 274703.0625  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 2.5450(3.4749) Grad: 83121.1719  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 2.6442(3.3711) Grad: 24671.9629  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 2.8962(2.8962) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 3.3711  avg_val_loss: 2.8120  time: 131s\n",
      "Epoch 3 - avg_train_Score: 3.3711 avgScore: 2.8120\n",
      "Epoch 3 - Save Best Score: 2.8120 Model\n",
      "Epoch 3 - Save Best Loss: 2.8120 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 2.4098(2.8120) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 11m 27s) Loss: 1.8293(1.8293) Grad: 318019.6250  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.4476(1.6995) Grad: 20306.5996  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2264(1.6798) Grad: 25361.6543  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 1.1538(1.1538) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.6798  avg_val_loss: 1.1991  time: 130s\n",
      "Epoch 4 - avg_train_Score: 1.6798 avgScore: 1.1991\n",
      "Epoch 4 - Save Best Score: 1.1991 Model\n",
      "Epoch 4 - Save Best Loss: 1.1991 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.1242(1.1991) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 11m 8s) Loss: 1.2639(1.2639) Grad: 268374.8125  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.4933(1.2411) Grad: 65433.7500  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.1483(1.2418) Grad: 84608.6172  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.9701(0.9701) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.2418  avg_val_loss: 1.0382  time: 131s\n",
      "Epoch 5 - avg_train_Score: 1.2418 avgScore: 1.0382\n",
      "Epoch 5 - Save Best Score: 1.0382 Model\n",
      "Epoch 5 - Save Best Loss: 1.0382 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0510(1.0382) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 11m 24s) Loss: 1.0710(1.0710) Grad: 362791.1875  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.1866(1.1412) Grad: 130558.2969  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.3354(1.1450) Grad: 140513.5469  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.9383(0.9383) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 1.1450  avg_val_loss: 0.9936  time: 131s\n",
      "Epoch 6 - avg_train_Score: 1.1450 avgScore: 0.9936\n",
      "Epoch 6 - Save Best Score: 0.9936 Model\n",
      "Epoch 6 - Save Best Loss: 0.9936 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0358(0.9936) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 0s) Loss: 1.2816(1.2816) Grad: inf  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.7250(1.1141) Grad: 50985.9688  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.1413(1.1171) Grad: 93983.4375  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.9152(0.9152) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 1.1171  avg_val_loss: 0.9689  time: 130s\n",
      "Epoch 7 - avg_train_Score: 1.1171 avgScore: 0.9689\n",
      "Epoch 7 - Save Best Score: 0.9689 Model\n",
      "Epoch 7 - Save Best Loss: 0.9689 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0259(0.9689) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 1.0147(1.0147) Grad: 610479.8125  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.1682(1.0517) Grad: 140806.3438  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.0369(1.0477) Grad: 231974.5469  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.9049(0.9049) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0084(0.9473) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 1.0477  avg_val_loss: 0.9473  time: 131s\n",
      "Epoch 8 - avg_train_Score: 1.0477 avgScore: 0.9473\n",
      "Epoch 8 - Save Best Score: 0.9473 Model\n",
      "Epoch 8 - Save Best Loss: 0.9473 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 10m 56s) Loss: 0.7625(0.7625) Grad: 315984.4062  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.0129(1.0675) Grad: 39509.4961  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.9707(1.0694) Grad: 66507.3516  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.9108(0.9108) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 1.0694  avg_val_loss: 0.9520  time: 131s\n",
      "Epoch 9 - avg_train_Score: 1.0694 avgScore: 0.9520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0468(0.9520) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 1.0702(1.0702) Grad: 471716.8750  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.2115(1.0334) Grad: 61528.5742  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.9368(1.0359) Grad: 36265.8750  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.9278(0.9278) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 1.0359  avg_val_loss: 0.9337  time: 131s\n",
      "Epoch 10 - avg_train_Score: 1.0359 avgScore: 0.9337\n",
      "Epoch 10 - Save Best Score: 0.9337 Model\n",
      "Epoch 10 - Save Best Loss: 0.9337 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9911(0.9337) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 11m 40s) Loss: 1.0884(1.0884) Grad: 386473.9062  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.7690(0.9941) Grad: 66859.4609  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8316(0.9951) Grad: 103545.2422  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.9237(0.9237) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.9951  avg_val_loss: 0.9121  time: 131s\n",
      "Epoch 11 - avg_train_Score: 0.9951 avgScore: 0.9121\n",
      "Epoch 11 - Save Best Score: 0.9121 Model\n",
      "Epoch 11 - Save Best Loss: 0.9121 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9676(0.9121) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 0.8492(0.8492) Grad: 209710.0781  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9580(0.9549) Grad: 61585.0391  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9677(0.9565) Grad: 65583.3438  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.9305(0.9305) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.9565  avg_val_loss: 0.9103  time: 130s\n",
      "Epoch 12 - avg_train_Score: 0.9565 avgScore: 0.9103\n",
      "Epoch 12 - Save Best Score: 0.9103 Model\n",
      "Epoch 12 - Save Best Loss: 0.9103 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9375(0.9103) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 10m 44s) Loss: 0.7680(0.7680) Grad: 258446.0156  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9472(0.9233) Grad: 63374.7188  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8919(0.9238) Grad: 132171.1250  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.9080(0.9080) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.9238  avg_val_loss: 0.9038  time: 130s\n",
      "Epoch 13 - avg_train_Score: 0.9238 avgScore: 0.9038\n",
      "Epoch 13 - Save Best Score: 0.9038 Model\n",
      "Epoch 13 - Save Best Loss: 0.9038 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.9614(0.9038) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 10s) Loss: 0.8478(0.8478) Grad: 253749.8594  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9165(0.8973) Grad: 108225.4766  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0530(0.8969) Grad: 72299.1719  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.9056(0.9056) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.8969  avg_val_loss: 0.8937  time: 130s\n",
      "Epoch 14 - avg_train_Score: 0.8969 avgScore: 0.8937\n",
      "Epoch 14 - Save Best Score: 0.8937 Model\n",
      "Epoch 14 - Save Best Loss: 0.8937 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8851(0.8937) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 51s) Loss: 1.1112(1.1112) Grad: 259469.6719  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.3990(0.8588) Grad: 188223.5625  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7057(0.8591) Grad: 150635.5000  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.8982(0.8982) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.8591  avg_val_loss: 0.8888  time: 130s\n",
      "Epoch 15 - avg_train_Score: 0.8591 avgScore: 0.8888\n",
      "Epoch 15 - Save Best Score: 0.8888 Model\n",
      "Epoch 15 - Save Best Loss: 0.8888 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8774(0.8888) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 10m 53s) Loss: 0.9208(0.9208) Grad: 356783.3438  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.9159(0.8911) Grad: 75253.9141  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.3037(0.8941) Grad: 96468.1172  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.9066(0.9066) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.8941  avg_val_loss: 0.8916  time: 131s\n",
      "Epoch 16 - avg_train_Score: 0.8941 avgScore: 0.8916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8578(0.8916) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 11m 1s) Loss: 0.8990(0.8990) Grad: 282947.1562  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8720(0.8521) Grad: 59453.3359  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7754(0.8497) Grad: 69152.8203  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 0.9028(0.9028) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.8497  avg_val_loss: 0.8818  time: 131s\n",
      "Epoch 17 - avg_train_Score: 0.8497 avgScore: 0.8818\n",
      "Epoch 17 - Save Best Score: 0.8818 Model\n",
      "Epoch 17 - Save Best Loss: 0.8818 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8876(0.8818) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 0.8359(0.8359) Grad: 243750.6406  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7262(0.8477) Grad: 66877.3203  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7404(0.8466) Grad: 85678.1250  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.8706(0.8706) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.8466  avg_val_loss: 0.8817  time: 130s\n",
      "Epoch 18 - avg_train_Score: 0.8466 avgScore: 0.8817\n",
      "Epoch 18 - Save Best Score: 0.8817 Model\n",
      "Epoch 18 - Save Best Loss: 0.8817 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8827(0.8817) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 10m 37s) Loss: 0.7806(0.7806) Grad: 498377.0938  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0880(0.8944) Grad: 34798.9023  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0685(0.8941) Grad: 52898.9023  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.9052(0.9052) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.8941  avg_val_loss: 0.8840  time: 130s\n",
      "Epoch 19 - avg_train_Score: 0.8941 avgScore: 0.8840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8917(0.8840) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 0s) Loss: 1.1221(1.1221) Grad: 239334.8281  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9969(0.8241) Grad: 61593.6289  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6585(0.8200) Grad: 56274.5273  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8960(0.8960) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.8200  avg_val_loss: 0.8750  time: 131s\n",
      "Epoch 20 - avg_train_Score: 0.8200 avgScore: 0.8750\n",
      "Epoch 20 - Save Best Score: 0.8750 Model\n",
      "Epoch 20 - Save Best Loss: 0.8750 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9076(0.8750) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 12m 2s) Loss: 0.7478(0.7478) Grad: 289911.5312  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7015(0.8121) Grad: 51322.3008  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8617(0.8134) Grad: 57995.4648  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 53s) Loss: 0.8960(0.8960) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.8134  avg_val_loss: 0.8676  time: 131s\n",
      "Epoch 21 - avg_train_Score: 0.8134 avgScore: 0.8676\n",
      "Epoch 21 - Save Best Score: 0.8676 Model\n",
      "Epoch 21 - Save Best Loss: 0.8676 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9092(0.8676) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 11m 17s) Loss: 0.8916(0.8916) Grad: inf  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0029(0.8017) Grad: 58109.4688  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8608(0.8013) Grad: 62377.6250  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.8749(0.8749) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.8013  avg_val_loss: 0.8651  time: 130s\n",
      "Epoch 22 - avg_train_Score: 0.8013 avgScore: 0.8651\n",
      "Epoch 22 - Save Best Score: 0.8651 Model\n",
      "Epoch 22 - Save Best Loss: 0.8651 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9056(0.8651) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 11m 38s) Loss: 0.5870(0.5870) Grad: 214945.0000  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6892(0.7850) Grad: 54447.0664  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6043(0.7833) Grad: 75554.1641  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.8829(0.8829) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8897(0.8694) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.7833  avg_val_loss: 0.8694  time: 130s\n",
      "Epoch 23 - avg_train_Score: 0.7833 avgScore: 0.8694\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 11m 5s) Loss: 0.9244(0.9244) Grad: 314159.7188  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9656(0.7765) Grad: 45908.3438  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8012(0.7774) Grad: 59870.7812  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.8702(0.8702) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.7774  avg_val_loss: 0.8698  time: 130s\n",
      "Epoch 24 - avg_train_Score: 0.7774 avgScore: 0.8698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8901(0.8698) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 10m 54s) Loss: 0.8187(0.8187) Grad: 208160.9375  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9457(0.7826) Grad: 166361.0625  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.2191(0.7823) Grad: 41096.1406  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8933(0.8933) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.7823  avg_val_loss: 0.8657  time: 130s\n",
      "Epoch 25 - avg_train_Score: 0.7823 avgScore: 0.8657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8872(0.8657) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 0.7200(0.7200) Grad: 260678.6719  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5971(0.7779) Grad: 70082.1719  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7242(0.7762) Grad: 59821.8867  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.8920(0.8920) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.7762  avg_val_loss: 0.8640  time: 130s\n",
      "Epoch 26 - avg_train_Score: 0.7762 avgScore: 0.8640\n",
      "Epoch 26 - Save Best Score: 0.8640 Model\n",
      "Epoch 26 - Save Best Loss: 0.8640 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9115(0.8640) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 51s) Loss: 0.7528(0.7528) Grad: 445478.2188  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.6753(0.7476) Grad: 216396.1250  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7399(0.7461) Grad: 47093.6328  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 0.8814(0.8814) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.7461  avg_val_loss: 0.8604  time: 131s\n",
      "Epoch 27 - avg_train_Score: 0.7461 avgScore: 0.8604\n",
      "Epoch 27 - Save Best Score: 0.8604 Model\n",
      "Epoch 27 - Save Best Loss: 0.8604 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9104(0.8604) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 0.6193(0.6193) Grad: 253014.1406  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5492(0.7494) Grad: 54039.1211  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6551(0.7497) Grad: 126301.2969  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.8474(0.8474) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.7497  avg_val_loss: 0.8566  time: 130s\n",
      "Epoch 28 - avg_train_Score: 0.7497 avgScore: 0.8566\n",
      "Epoch 28 - Save Best Score: 0.8566 Model\n",
      "Epoch 28 - Save Best Loss: 0.8566 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9106(0.8566) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 11m 1s) Loss: 0.6876(0.6876) Grad: 251378.1875  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.7486(0.7351) Grad: 61775.5703  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.5197(0.7344) Grad: 53754.6484  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8629(0.8629) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9071(0.8566) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.7344  avg_val_loss: 0.8566  time: 131s\n",
      "Epoch 29 - avg_train_Score: 0.7344 avgScore: 0.8566\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 10m 59s) Loss: 0.6799(0.6799) Grad: 210841.3750  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.5750(0.7383) Grad: 37513.4492  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8663(0.7387) Grad: 48997.1484  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8618(0.8618) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.7387  avg_val_loss: 0.8569  time: 131s\n",
      "Epoch 30 - avg_train_Score: 0.7387 avgScore: 0.8569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8862(0.8569) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 10m 51s) Loss: 0.6607(0.6607) Grad: 183774.6719  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6815(0.7306) Grad: 249145.6406  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7713(0.7324) Grad: 60709.9883  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 0.8694(0.8694) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.7324  avg_val_loss: 0.8551  time: 131s\n",
      "Epoch 31 - avg_train_Score: 0.7324 avgScore: 0.8551\n",
      "Epoch 31 - Save Best Score: 0.8551 Model\n",
      "Epoch 31 - Save Best Loss: 0.8551 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9117(0.8551) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 11m 18s) Loss: 0.6075(0.6075) Grad: 223694.7812  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7707(0.7107) Grad: 28830.6973  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5513(0.7143) Grad: 29256.6094  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.8799(0.8799) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.7143  avg_val_loss: 0.8552  time: 130s\n",
      "Epoch 32 - avg_train_Score: 0.7143 avgScore: 0.8552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8835(0.8552) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 10m 46s) Loss: 0.7027(0.7027) Grad: 419699.8438  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9971(0.7144) Grad: 71667.5703  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7757(0.7176) Grad: 119579.7734  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.8685(0.8685) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8856(0.8548) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.7176  avg_val_loss: 0.8548  time: 130s\n",
      "Epoch 33 - avg_train_Score: 0.7176 avgScore: 0.8548\n",
      "Epoch 33 - Save Best Score: 0.8548 Model\n",
      "Epoch 33 - Save Best Loss: 0.8548 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 11m 16s) Loss: 0.6337(0.6337) Grad: 181415.6250  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.5982(0.7046) Grad: 144455.5469  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.0883(0.7073) Grad: 103734.6094  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8715(0.8715) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.7073  avg_val_loss: 0.8539  time: 131s\n",
      "Epoch 34 - avg_train_Score: 0.7073 avgScore: 0.8539\n",
      "Epoch 34 - Save Best Score: 0.8539 Model\n",
      "Epoch 34 - Save Best Loss: 0.8539 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8985(0.8539) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 53s) Loss: 0.7159(0.7159) Grad: 199815.5000  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.4954(0.7142) Grad: 94704.2031  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.5894(0.7128) Grad: 84833.5000  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.8687(0.8687) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.7128  avg_val_loss: 0.8527  time: 131s\n",
      "Epoch 35 - avg_train_Score: 0.7128 avgScore: 0.8527\n",
      "Epoch 35 - Save Best Score: 0.8527 Model\n",
      "Epoch 35 - Save Best Loss: 0.8527 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8862(0.8527) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 11m 1s) Loss: 0.5268(0.5268) Grad: 210037.2344  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5302(0.7098) Grad: 26084.8320  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6490(0.7100) Grad: 39395.6328  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 0.8739(0.8739) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.7100  avg_val_loss: 0.8500  time: 131s\n",
      "Epoch 36 - avg_train_Score: 0.7100 avgScore: 0.8500\n",
      "Epoch 36 - Save Best Score: 0.8500 Model\n",
      "Epoch 36 - Save Best Loss: 0.8500 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8885(0.8500) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 11m 14s) Loss: 0.7498(0.7498) Grad: 213469.1719  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6461(0.7014) Grad: 30657.5898  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6024(0.7016) Grad: 88937.9297  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8768(0.8768) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.7016  avg_val_loss: 0.8478  time: 130s\n",
      "Epoch 37 - avg_train_Score: 0.7016 avgScore: 0.8478\n",
      "Epoch 37 - Save Best Score: 0.8478 Model\n",
      "Epoch 37 - Save Best Loss: 0.8478 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9042(0.8478) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 10m 52s) Loss: 0.5702(0.5702) Grad: 251067.7188  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5756(0.7032) Grad: 20370.9355  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0912(0.7005) Grad: 12292.6094  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.8716(0.8716) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.7005  avg_val_loss: 0.8495  time: 130s\n",
      "Epoch 38 - avg_train_Score: 0.7005 avgScore: 0.8495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9031(0.8495) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 12m 9s) Loss: 0.5181(0.5181) Grad: 234079.2656  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7635(0.6984) Grad: 125776.8750  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6269(0.6959) Grad: 70281.8203  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.8736(0.8736) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.6959  avg_val_loss: 0.8499  time: 130s\n",
      "Epoch 39 - avg_train_Score: 0.6959 avgScore: 0.8499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9044(0.8499) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 1.1086(1.1086) Grad: 194316.9219  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.5567(0.6879) Grad: 58991.2266  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7649(0.6890) Grad: 57138.0039  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 48s) Loss: 0.8715(0.8715) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.6890  avg_val_loss: 0.8503  time: 131s\n",
      "Epoch 40 - avg_train_Score: 0.6890 avgScore: 0.8503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9047(0.8503) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 3 result ==========\n",
      "score: 0.8503\n",
      "========== fold: 4 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 58s) Loss: 5.1548(5.1548) Grad: 406954.1875  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 4.9939(5.2986) Grad: 47024.8789  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 5.3489(5.2685) Grad: 47431.5586  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 5.4180(5.4180) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 5.2685  avg_val_loss: 5.2300  time: 131s\n",
      "Epoch 1 - avg_train_Score: 5.2685 avgScore: 5.2300\n",
      "Epoch 1 - Save Best Score: 5.2300 Model\n",
      "Epoch 1 - Save Best Loss: 5.2300 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 5.0972(5.2300) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 4.9625(4.9625) Grad: 335159.6562  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 5.2790(4.7326) Grad: 7420.3521  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 4.0884(4.6959) Grad: 79590.4141  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 47s) Loss: 4.5727(4.5727) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 4.6959  avg_val_loss: 4.4678  time: 130s\n",
      "Epoch 2 - avg_train_Score: 4.6959 avgScore: 4.4678\n",
      "Epoch 2 - Save Best Score: 4.4678 Model\n",
      "Epoch 2 - Save Best Loss: 4.4678 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 4.3657(4.4678) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 56s) Loss: 3.8783(3.8783) Grad: 202193.0625  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 2.5700(3.6094) Grad: 15072.0840  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 2.2473(3.5263) Grad: 14130.0137  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 2.7056(2.7056) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 3.5263  avg_val_loss: 2.8645  time: 131s\n",
      "Epoch 3 - avg_train_Score: 3.5263 avgScore: 2.8645\n",
      "Epoch 3 - Save Best Score: 2.8645 Model\n",
      "Epoch 3 - Save Best Loss: 2.8645 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 3.0175(2.8645) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 10m 59s) Loss: 2.2690(2.2690) Grad: inf  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2735(2.0625) Grad: 9435.3350  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.6473(2.0187) Grad: 9138.2041  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 1.1344(1.1344) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 2.0187  avg_val_loss: 1.2969  time: 131s\n",
      "Epoch 4 - avg_train_Score: 2.0187 avgScore: 1.2969\n",
      "Epoch 4 - Save Best Score: 1.2969 Model\n",
      "Epoch 4 - Save Best Loss: 1.2969 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.4571(1.2969) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 11m 1s) Loss: 1.1736(1.1736) Grad: 373107.1562  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.4227(1.3261) Grad: 197434.4844  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.0126(1.3217) Grad: 54129.9805  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 44s) Loss: 1.0046(1.0046) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.3217  avg_val_loss: 1.0665  time: 131s\n",
      "Epoch 5 - avg_train_Score: 1.3217 avgScore: 1.0665\n",
      "Epoch 5 - Save Best Score: 1.0665 Model\n",
      "Epoch 5 - Save Best Loss: 1.0665 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.2285(1.0665) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 11m 13s) Loss: 1.3219(1.3219) Grad: 297657.2188  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.2576(1.2416) Grad: 48259.8320  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0400(1.2407) Grad: 36869.6445  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.9852(0.9852) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 1.2407  avg_val_loss: 1.0076  time: 130s\n",
      "Epoch 6 - avg_train_Score: 1.2407 avgScore: 1.0076\n",
      "Epoch 6 - Save Best Score: 1.0076 Model\n",
      "Epoch 6 - Save Best Loss: 1.0076 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 1.1284(1.0076) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 17s) Loss: 1.9560(1.9560) Grad: 344012.0312  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.3071(1.2042) Grad: 17839.4941  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.2865(1.2061) Grad: 21928.1465  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.9669(0.9669) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0354(0.9617) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 1.2061  avg_val_loss: 0.9617  time: 131s\n",
      "Epoch 7 - avg_train_Score: 1.2061 avgScore: 0.9617\n",
      "Epoch 7 - Save Best Score: 0.9617 Model\n",
      "Epoch 7 - Save Best Loss: 0.9617 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 11m 28s) Loss: 1.3750(1.3750) Grad: 209680.0781  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8044(1.1086) Grad: 67064.8281  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.1438(1.1090) Grad: 72916.2969  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 49s) Loss: 0.8986(0.8986) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 1.1090  avg_val_loss: 0.9323  time: 131s\n",
      "Epoch 8 - avg_train_Score: 1.1090 avgScore: 0.9323\n",
      "Epoch 8 - Save Best Score: 0.9323 Model\n",
      "Epoch 8 - Save Best Loss: 0.9323 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 1.0050(0.9323) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 21s) Loss: 1.0236(1.0236) Grad: 292424.9062  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9216(1.0740) Grad: 52022.6250  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.2424(1.0753) Grad: 30927.2422  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.8485(0.8485) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 1.0753  avg_val_loss: 0.9191  time: 130s\n",
      "Epoch 9 - avg_train_Score: 1.0753 avgScore: 0.9191\n",
      "Epoch 9 - Save Best Score: 0.9191 Model\n",
      "Epoch 9 - Save Best Loss: 0.9191 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9718(0.9191) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 8s) Loss: 1.1234(1.1234) Grad: 256946.5312  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0885(1.0365) Grad: 36480.2734  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8671(1.0357) Grad: 33006.5352  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 0.8425(0.8425) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9173(0.8994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 1.0357  avg_val_loss: 0.8994  time: 131s\n",
      "Epoch 10 - avg_train_Score: 1.0357 avgScore: 0.8994\n",
      "Epoch 10 - Save Best Score: 0.8994 Model\n",
      "Epoch 10 - Save Best Loss: 0.8994 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 11m 38s) Loss: 0.8753(0.8753) Grad: 281294.4375  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.3919(0.9728) Grad: 96565.3203  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9674(0.9744) Grad: 129885.2344  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.8364(0.8364) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.9744  avg_val_loss: 0.8889  time: 130s\n",
      "Epoch 11 - avg_train_Score: 0.9744 avgScore: 0.8889\n",
      "Epoch 11 - Save Best Score: 0.8889 Model\n",
      "Epoch 11 - Save Best Loss: 0.8889 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9101(0.8889) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.9266(0.9266) Grad: 255910.4531  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 1.6135(0.9279) Grad: 51343.0039  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8150(0.9349) Grad: 86173.9766  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.8387(0.8387) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.9349  avg_val_loss: 0.8796  time: 131s\n",
      "Epoch 12 - avg_train_Score: 0.9349 avgScore: 0.8796\n",
      "Epoch 12 - Save Best Score: 0.8796 Model\n",
      "Epoch 12 - Save Best Loss: 0.8796 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.9549(0.8796) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 10m 44s) Loss: 1.0036(1.0036) Grad: 224089.5312  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.5464(0.9157) Grad: 121424.5156  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9746(0.9185) Grad: 181020.8281  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 0.8054(0.8054) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.9344(0.8720) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.9185  avg_val_loss: 0.8720  time: 130s\n",
      "Epoch 13 - avg_train_Score: 0.9185 avgScore: 0.8720\n",
      "Epoch 13 - Save Best Score: 0.8720 Model\n",
      "Epoch 13 - Save Best Loss: 0.8720 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 13s) Loss: 0.9871(0.9871) Grad: 376032.8438  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0454(0.9807) Grad: 32345.5176  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.8517(0.9812) Grad: 52950.9688  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.8401(0.8401) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.9812  avg_val_loss: 0.8735  time: 130s\n",
      "Epoch 14 - avg_train_Score: 0.9812 avgScore: 0.8735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8571(0.8735) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 46s) Loss: 1.0361(1.0361) Grad: 256987.5312  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8649(0.9452) Grad: 27371.4707  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0075(0.9437) Grad: 40180.9375  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8193(0.8193) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.9437  avg_val_loss: 0.8607  time: 130s\n",
      "Epoch 15 - avg_train_Score: 0.9437 avgScore: 0.8607\n",
      "Epoch 15 - Save Best Score: 0.8607 Model\n",
      "Epoch 15 - Save Best Loss: 0.8607 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8749(0.8607) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 11m 42s) Loss: 1.5037(1.5037) Grad: 215520.5469  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8395(0.9156) Grad: 36168.4766  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 1.0160(0.9131) Grad: 38682.9102  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.8423(0.8423) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.9131  avg_val_loss: 0.8572  time: 131s\n",
      "Epoch 16 - avg_train_Score: 0.9131 avgScore: 0.8572\n",
      "Epoch 16 - Save Best Score: 0.8572 Model\n",
      "Epoch 16 - Save Best Loss: 0.8572 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8964(0.8572) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 11m 18s) Loss: 1.1290(1.1290) Grad: 242104.6875  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7780(0.8823) Grad: 31954.6777  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7055(0.8815) Grad: 35356.7383  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.8196(0.8196) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8734(0.8466) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.8815  avg_val_loss: 0.8466  time: 130s\n",
      "Epoch 17 - avg_train_Score: 0.8815 avgScore: 0.8466\n",
      "Epoch 17 - Save Best Score: 0.8466 Model\n",
      "Epoch 17 - Save Best Loss: 0.8466 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 27s) Loss: 0.7427(0.7427) Grad: 347229.5938  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9897(0.8912) Grad: 36246.5469  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7819(0.8896) Grad: 29638.9668  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8287(0.8287) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.8896  avg_val_loss: 0.8470  time: 130s\n",
      "Epoch 18 - avg_train_Score: 0.8896 avgScore: 0.8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8791(0.8470) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 11m 29s) Loss: 1.2546(1.2546) Grad: 213355.3281  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6641(0.8357) Grad: 61833.9336  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8243(0.8369) Grad: 56650.0859  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.8267(0.8267) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8964(0.8407) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.8369  avg_val_loss: 0.8407  time: 131s\n",
      "Epoch 19 - avg_train_Score: 0.8369 avgScore: 0.8407\n",
      "Epoch 19 - Save Best Score: 0.8407 Model\n",
      "Epoch 19 - Save Best Loss: 0.8407 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 10m 43s) Loss: 0.6301(0.6301) Grad: 236611.5000  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6485(0.8099) Grad: 83091.4844  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6823(0.8112) Grad: 95630.3906  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.8234(0.8234) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.8112  avg_val_loss: 0.8382  time: 130s\n",
      "Epoch 20 - avg_train_Score: 0.8112 avgScore: 0.8382\n",
      "Epoch 20 - Save Best Score: 0.8382 Model\n",
      "Epoch 20 - Save Best Loss: 0.8382 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8939(0.8382) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 11m 39s) Loss: 0.7277(0.7277) Grad: 250605.7500  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9718(0.7934) Grad: 51986.3555  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.3739(0.7968) Grad: 60014.9922  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.7958(0.7958) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.7968  avg_val_loss: 0.8359  time: 130s\n",
      "Epoch 21 - avg_train_Score: 0.7968 avgScore: 0.8359\n",
      "Epoch 21 - Save Best Score: 0.8359 Model\n",
      "Epoch 21 - Save Best Loss: 0.8359 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8755(0.8359) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 10m 41s) Loss: 0.7849(0.7849) Grad: 195702.8594  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9486(0.8053) Grad: 27566.8359  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7953(0.8069) Grad: 30086.2871  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7930(0.7930) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.8069  avg_val_loss: 0.8354  time: 130s\n",
      "Epoch 22 - avg_train_Score: 0.8069 avgScore: 0.8354\n",
      "Epoch 22 - Save Best Score: 0.8354 Model\n",
      "Epoch 22 - Save Best Loss: 0.8354 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8578(0.8354) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 11m 38s) Loss: 0.9674(0.9674) Grad: 248000.7188  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6348(0.7843) Grad: 71380.7812  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7103(0.7841) Grad: 56400.3867  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.7808(0.7808) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.7841  avg_val_loss: 0.8315  time: 130s\n",
      "Epoch 23 - avg_train_Score: 0.7841 avgScore: 0.8315\n",
      "Epoch 23 - Save Best Score: 0.8315 Model\n",
      "Epoch 23 - Save Best Loss: 0.8315 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8219(0.8315) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 48s) Loss: 0.7360(0.7360) Grad: 210617.6719  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8601(0.7606) Grad: 62989.4727  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6726(0.7618) Grad: 69202.3594  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7840(0.7840) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.7618  avg_val_loss: 0.8276  time: 130s\n",
      "Epoch 24 - avg_train_Score: 0.7618 avgScore: 0.8276\n",
      "Epoch 24 - Save Best Score: 0.8276 Model\n",
      "Epoch 24 - Save Best Loss: 0.8276 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7930(0.8276) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 10m 37s) Loss: 0.6615(0.6615) Grad: 217851.9688  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7185(0.7577) Grad: 57864.4023  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.6630(0.7565) Grad: 60767.5117  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 45s) Loss: 0.7861(0.7861) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.7565  avg_val_loss: 0.8248  time: 131s\n",
      "Epoch 25 - avg_train_Score: 0.7565 avgScore: 0.8248\n",
      "Epoch 25 - Save Best Score: 0.8248 Model\n",
      "Epoch 25 - Save Best Loss: 0.8248 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7810(0.8248) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 11m 12s) Loss: 1.2799(1.2799) Grad: 176438.7031  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6439(0.7338) Grad: 126023.3438  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6998(0.7352) Grad: 105244.5391  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.8074(0.8074) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.7352  avg_val_loss: 0.8215  time: 130s\n",
      "Epoch 26 - avg_train_Score: 0.7352 avgScore: 0.8215\n",
      "Epoch 26 - Save Best Score: 0.8215 Model\n",
      "Epoch 26 - Save Best Loss: 0.8215 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7916(0.8215) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 14m 21s) Loss: 0.7523(0.7523) Grad: 211837.5781  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9229(0.7629) Grad: 85956.4141  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.7959(0.7629) Grad: 58319.6602  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.7861(0.7861) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.7629  avg_val_loss: 0.8249  time: 131s\n",
      "Epoch 27 - avg_train_Score: 0.7629 avgScore: 0.8249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8064(0.8249) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 10m 54s) Loss: 0.6755(0.6755) Grad: 240080.8750  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7662(0.7500) Grad: 54713.8281  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.9749(0.7476) Grad: 41665.1797  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7956(0.7956) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.7476  avg_val_loss: 0.8264  time: 130s\n",
      "Epoch 28 - avg_train_Score: 0.7476 avgScore: 0.8264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8104(0.8264) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 10m 59s) Loss: 0.8098(0.8098) Grad: 234903.0312  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7397(0.7232) Grad: 48943.9727  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.0187(0.7234) Grad: 52699.1328  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7875(0.7875) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.7234  avg_val_loss: 0.8237  time: 130s\n",
      "Epoch 29 - avg_train_Score: 0.7234 avgScore: 0.8237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7954(0.8237) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 11m 34s) Loss: 0.7213(0.7213) Grad: 203045.5625  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.9160(0.7190) Grad: 56094.5312  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.4949(0.7199) Grad: 49996.3320  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7742(0.7742) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.7199  avg_val_loss: 0.8205  time: 131s\n",
      "Epoch 30 - avg_train_Score: 0.7199 avgScore: 0.8205\n",
      "Epoch 30 - Save Best Score: 0.8205 Model\n",
      "Epoch 30 - Save Best Loss: 0.8205 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8191(0.8205) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 11m 27s) Loss: 0.5267(0.5267) Grad: 204686.6562  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5416(0.7160) Grad: 93990.4297  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.8067(0.7211) Grad: 58456.8203  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7802(0.7802) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.7211  avg_val_loss: 0.8191  time: 131s\n",
      "Epoch 31 - avg_train_Score: 0.7211 avgScore: 0.8191\n",
      "Epoch 31 - Save Best Score: 0.8191 Model\n",
      "Epoch 31 - Save Best Loss: 0.8191 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8168(0.8191) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 10m 46s) Loss: 0.8925(0.8925) Grad: 213271.3281  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6473(0.6964) Grad: 89477.1562  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5729(0.6959) Grad: 102968.2969  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.8000(0.8000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.6959  avg_val_loss: 0.8185  time: 130s\n",
      "Epoch 32 - avg_train_Score: 0.6959 avgScore: 0.8185\n",
      "Epoch 32 - Save Best Score: 0.8185 Model\n",
      "Epoch 32 - Save Best Loss: 0.8185 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8230(0.8185) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 11m 49s) Loss: 0.9077(0.9077) Grad: 245960.6250  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6945(0.7030) Grad: 60597.0898  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.7645(0.7019) Grad: 63555.1367  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7928(0.7928) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.7019  avg_val_loss: 0.8177  time: 130s\n",
      "Epoch 33 - avg_train_Score: 0.7019 avgScore: 0.8177\n",
      "Epoch 33 - Save Best Score: 0.8177 Model\n",
      "Epoch 33 - Save Best Loss: 0.8177 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8081(0.8177) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 12m 11s) Loss: 0.5502(0.5502) Grad: 202267.7344  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 1.0584(0.7063) Grad: 44936.2383  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.5586(0.7033) Grad: 44711.9648  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 50s) Loss: 0.7855(0.7855) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7960(0.8168) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.7033  avg_val_loss: 0.8168  time: 131s\n",
      "Epoch 34 - avg_train_Score: 0.7033 avgScore: 0.8168\n",
      "Epoch 34 - Save Best Score: 0.8168 Model\n",
      "Epoch 34 - Save Best Loss: 0.8168 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 11m 24s) Loss: 0.7995(0.7995) Grad: 225928.4688  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5837(0.6846) Grad: 95018.7422  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5522(0.6851) Grad: 93087.0625  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 50s) Loss: 0.7847(0.7847) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.6851  avg_val_loss: 0.8148  time: 130s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8028(0.8148) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 - avg_train_Score: 0.6851 avgScore: 0.8148\n",
      "Epoch 35 - Save Best Score: 0.8148 Model\n",
      "Epoch 35 - Save Best Loss: 0.8148 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 13m 21s) Loss: 0.6465(0.6465) Grad: 228447.1406  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 43s (remain 0m 8s) Loss: 0.6102(0.6912) Grad: 53243.6484  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 51s (remain 0m 0s) Loss: 0.5844(0.6932) Grad: 47828.3438  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7842(0.7842) \n",
      "EVAL: [67/68] Elapsed 0m 18s (remain 0m 0s) Loss: 0.8004(0.8131) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.6932  avg_val_loss: 0.8131  time: 131s\n",
      "Epoch 36 - avg_train_Score: 0.6932 avgScore: 0.8131\n",
      "Epoch 36 - Save Best Score: 0.8131 Model\n",
      "Epoch 36 - Save Best Loss: 0.8131 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 48s) Loss: 0.5684(0.5684) Grad: 265139.2812  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.6248(0.6738) Grad: 129979.8672  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.6510(0.6753) Grad: 168994.7344  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7752(0.7752) \n",
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7982(0.8120) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.6753  avg_val_loss: 0.8120  time: 130s\n",
      "Epoch 37 - avg_train_Score: 0.6753 avgScore: 0.8120\n",
      "Epoch 37 - Save Best Score: 0.8120 Model\n",
      "Epoch 37 - Save Best Loss: 0.8120 Model\n",
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 11m 21s) Loss: 0.5706(0.5706) Grad: 187156.7656  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.7406(0.6789) Grad: 116530.2578  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 1.1381(0.6809) Grad: 102210.8672  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 40s) Loss: 0.7714(0.7714) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.6809  avg_val_loss: 0.8109  time: 130s\n",
      "Epoch 38 - avg_train_Score: 0.6809 avgScore: 0.8109\n",
      "Epoch 38 - Save Best Score: 0.8109 Model\n",
      "Epoch 38 - Save Best Loss: 0.8109 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7991(0.8109) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 11m 7s) Loss: 0.6138(0.6138) Grad: 206391.0469  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.5051(0.6859) Grad: 149996.0469  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.4652(0.6857) Grad: 92938.7656  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.7696(0.7696) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.6857  avg_val_loss: 0.8118  time: 130s\n",
      "Epoch 39 - avg_train_Score: 0.6857 avgScore: 0.8118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.8023(0.8118) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_167072/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 11m 28s) Loss: 0.5417(0.5417) Grad: 252916.0312  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 42s (remain 0m 8s) Loss: 0.8665(0.6821) Grad: 56161.8633  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 50s (remain 0m 0s) Loss: 0.5815(0.6838) Grad: 45871.1953  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 43s) Loss: 0.7710(0.7710) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.6838  avg_val_loss: 0.8119  time: 130s\n",
      "Epoch 40 - avg_train_Score: 0.6838 avgScore: 0.8119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 19s (remain 0m 0s) Loss: 0.7939(0.8119) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 4 result ==========\n",
      "score: 0.8119\n",
      "========== CV ==========\n",
      "score: 0.8244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "            if CFG.objective_cv == 'binary':\n",
    "                outputs.append(torch.sigmoid(model(x)).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'multiclass':\n",
    "                outputs.append(torch.softmax(\n",
    "                    model(x), axis=1).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'regression':\n",
    "                outputs.append(model(x).to('cpu').numpy())\n",
    "\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "\n",
    "def test_fn(valid_loader, model, device):\n",
    "    preds = []\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        preds.append(y_preds)\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def inference():\n",
    "    test = pd.read_csv(CFG.comp_dataset_path +\n",
    "                       'test_features.csv')\n",
    "\n",
    "    test['base_path'] = CFG.comp_dataset_path + 'images/' + test['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in test['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    print(test.head(5))\n",
    "\n",
    "    valid_dataset = CustomDataset(\n",
    "        test, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = EnsembleModel()\n",
    "    folds = [0] if CFG.use_holdout else list(range(CFG.n_fold))\n",
    "    for fold in folds:\n",
    "        _model = CustomModel(CFG, pretrained=False)\n",
    "        _model.to(device)\n",
    "\n",
    "        model_path = CFG.model_dir + \\\n",
    "            f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth'\n",
    "        print('load', model_path)\n",
    "        state = torch.load(model_path)['model']\n",
    "        _model.load_state_dict(state)\n",
    "        _model.eval()\n",
    "\n",
    "        # _model = tta.ClassificationTTAWrapper(\n",
    "        #     _model, tta.aliases.five_crop_transform(256, 256))\n",
    "\n",
    "        model.add_model(_model)\n",
    "\n",
    "    preds = test_fn(valid_loader, model, device)\n",
    "\n",
    "    test[CFG.target_col] = preds\n",
    "    test.to_csv(CFG.submission_dir +\n",
    "                'submission_oof.csv', index=False)\n",
    "    test[CFG.target_col].to_csv(\n",
    "        CFG.submission_dir + f'submission_{CFG.exp_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-0.5.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa97b1331eeb4dcbaa1f10a119027db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     ID      vEgo      aEgo  steeringAngleDeg  \\\n",
      "0  012baccc145d400c896cb82065a93d42_120  3.374273 -0.019360        -34.008415   \n",
      "1  012baccc145d400c896cb82065a93d42_220  2.441048 -0.022754        307.860077   \n",
      "2  012baccc145d400c896cb82065a93d42_320  3.604152 -0.286239         10.774388   \n",
      "3  012baccc145d400c896cb82065a93d42_420  2.048902 -0.537628         61.045235   \n",
      "4  01d738e799d260a10f6324f78023b38f_120  2.201528 -1.898600          5.740093   \n",
      "\n",
      "   steeringTorque  brake  brakePressed  gas  gasPressed gearShifter  \\\n",
      "0            17.0    0.0         False  0.0       False       drive   \n",
      "1           295.0    0.0          True  0.0       False       drive   \n",
      "2          -110.0    0.0          True  0.0       False       drive   \n",
      "3           189.0    0.0          True  0.0       False       drive   \n",
      "4           -41.0    0.0          True  0.0       False       drive   \n",
      "\n",
      "   leftBlinker  rightBlinker  \\\n",
      "0        False         False   \n",
      "1        False         False   \n",
      "2        False         False   \n",
      "3         True         False   \n",
      "4        False         False   \n",
      "\n",
      "                                           base_path  \n",
      "0  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "1  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "2  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "3  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "4  ../raw/atmacup_18_dataset/images/01d738e799d26...  \n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_small/atmacup_18-models/swin_small_patch4_window7_224_fold0_last.pth\n",
      "pretrained: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167072/610043316.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path)['model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_small/atmacup_18-models/swin_small_patch4_window7_224_fold1_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_small/atmacup_18-models/swin_small_patch4_window7_224_fold2_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_small/atmacup_18-models/swin_small_patch4_window7_224_fold3_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_small/atmacup_18-models/swin_small_patch4_window7_224_fold4_last.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d73acdb0e4c49c8b97392ed8cb433f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
