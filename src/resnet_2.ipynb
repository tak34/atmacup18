{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from timm.utils.model_ema import ModelEmaV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set dataset path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'atmacup_18'  # comp名\n",
    "\n",
    "    comp_dataset_path = '../raw/atmacup_18_dataset/'\n",
    "\n",
    "    exp_name = 'atmacup_18_resnet2'\n",
    "\n",
    "    is_debug = False\n",
    "    use_gray_scale = False\n",
    "\n",
    "    model_in_chans = 9  # モデルの入力チャンネル数\n",
    "\n",
    "    # ============== file path =============\n",
    "    train_fold_dir = \"../proc/baseline/folds\"\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = 'resnet34d'\n",
    "\n",
    "    num_frames = 3  # model_in_chansの倍数\n",
    "    norm_in_chans = 1 if use_gray_scale else 3\n",
    "\n",
    "    use_torch_compile = False\n",
    "    use_ema = True\n",
    "    ema_decay = 0.995\n",
    "    # ============== training cfg =============\n",
    "    size = 224  # 224\n",
    "\n",
    "    batch_size = 64  # 32\n",
    "\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 20\n",
    "    if is_debug:\n",
    "        epochs = 2\n",
    "\n",
    "    # adamW warmupあり\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-3\n",
    "    if scheduler == 'GradualWarmupSchedulerV2':\n",
    "        lr /= warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    n_fold = 5\n",
    "    use_holdout = False\n",
    "    use_alldata = False\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    skf_col = 'class'\n",
    "    group_col = 'scene'\n",
    "    fold_type = 'gkf'\n",
    "\n",
    "    objective_cv = 'regression'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'minimize'  # 'maximize', 'minimize'\n",
    "    metrics = 'calc_mae_atmacup'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 18\n",
    "    target_col = ['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1', 'x_2', 'y_2',\n",
    "                  'z_2', 'x_3', 'y_3', 'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5']\n",
    "\n",
    "\n",
    "    # ============== ほぼ固定 =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'last'  # 'best'\n",
    "\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== set dataset path =============\n",
    "    if exp_name is not None:\n",
    "        print('set dataset path')\n",
    "\n",
    "        outputs_path = f'../proc/baseline/outputs/{exp_name}/'\n",
    "\n",
    "        submission_dir = outputs_path + 'submissions/'\n",
    "        submission_path = submission_dir + f'submission_{exp_name}.csv'\n",
    "\n",
    "        model_dir = outputs_path + \\\n",
    "            f'{comp_name}-models/'\n",
    "\n",
    "        figures_dir = outputs_path + 'figures/'\n",
    "\n",
    "        log_dir = outputs_path + 'logs/'\n",
    "        log_path = log_dir + f'{exp_name}.txt'\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        A.Downscale(p=0.25),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.ShiftScaleRotate(p=0.5),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        # A.CoarseDropout(max_holes=1, max_height=int(\n",
    "        #     size * 0.3), max_width=int(size * 0.3), p=0.5),\n",
    "        A.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA が利用可能か: True\n",
      "利用可能な CUDA デバイス数: 1\n",
      "現在の CUDA デバイス: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA が利用可能か:\", torch.cuda.is_available())\n",
    "print(\"利用可能な CUDA デバイス数:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"現在の CUDA デバイス:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "def get_fold(train, cfg):\n",
    "    if cfg.fold_type == 'kf':\n",
    "        Fold = KFold(n_splits=cfg.n_fold,\n",
    "                     shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.target_col])\n",
    "    elif cfg.fold_type == 'skf':\n",
    "        Fold = StratifiedKFold(n_splits=cfg.n_fold,\n",
    "                               shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.skf_col])\n",
    "    elif cfg.fold_type == 'gkf':\n",
    "        Fold = GroupKFold(n_splits=cfg.n_fold)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.group_col], groups)\n",
    "    elif cfg.fold_type == 'sgkf':\n",
    "        Fold = StratifiedGroupKFold(n_splits=cfg.n_fold,\n",
    "                                    shuffle=True, random_state=cfg.seed)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.skf_col], groups)\n",
    "    # elif fold_type == 'mskf':\n",
    "    #     Fold = MultilabelStratifiedKFold(\n",
    "    #         n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    #     kf = Fold.split(train, train[cfg.skf_col])\n",
    "\n",
    "    for n, (train_index, val_index) in enumerate(kf):\n",
    "        train.loc[val_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "    print(train.groupby('fold').size())\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_folds():\n",
    "    train_df = pd.read_csv(CFG.comp_dataset_path + 'train_features.csv')\n",
    "\n",
    "    train_df['scene'] = train_df['ID'].str.split('_').str[0]\n",
    "\n",
    "    print('group', CFG.group_col)\n",
    "    print(f'train len: {len(train_df)}')\n",
    "\n",
    "    train_df = get_fold(train_df, CFG)\n",
    "\n",
    "    # print(train_df.groupby(['fold', CFG.target_col]).size())\n",
    "    print(train_df['fold'].value_counts())\n",
    "\n",
    "    os.makedirs(CFG.train_fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(CFG.train_fold_dir +\n",
    "                    'train_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group scene\n",
      "train len: 43371\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "dtype: int64\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "make_train_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数固定\n",
    "def set_seed(seed=None, cudnn_deterministic=True):\n",
    "    if seed is None:\n",
    "        seed = 42\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def make_dirs(cfg):\n",
    "    for dir in [cfg.model_dir, cfg.figures_dir, cfg.submission_dir, cfg.log_dir]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def cfg_init(cfg, mode='train'):\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if mode == 'train':\n",
    "        make_dirs(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_init(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common_utils.logger import init_logger, wandb_init, AverageMeter, timeSince\n",
    "# from common_utils.settings import cfg_init\n",
    "\n",
    "def init_logger(log_file):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- exp_info -----------------\n",
      "2024年11月24日 01:53:52\n"
     ]
    }
   ],
   "source": [
    "Logger = init_logger(log_file=CFG.log_path)\n",
    "\n",
    "Logger.info('\\n\\n-------- exp_info -----------------')\n",
    "Logger.info(datetime.datetime.now().strftime('%Y年%m月%d日 %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    # return roc_auc_score(y_true, y_pred)\n",
    "    eval_func = eval(CFG.metrics)\n",
    "    return eval_func(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calc_mae_atmacup(y_true, y_pred):\n",
    "    abs_diff = np.abs(y_true - y_pred)  # 各予測の差分の絶対値を計算して\n",
    "    mae = np.mean(abs_diff.reshape(-1, ))  # 予測の差分の絶対値の平均を計算\n",
    "\n",
    "    return mae\n",
    "\n",
    "def get_result(result_df):\n",
    "\n",
    "    # preds = result_df['preds'].values\n",
    "\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "    preds = result_df[pred_cols].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    Logger.info(f'score: {score:<.4f}')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_traffic_light(image, id):\n",
    "    path = f'./datasets/atmacup_18/traffic_lights/{id}.json'\n",
    "    traffic_lights = json.load(open(path))\n",
    "\n",
    "    traffic_class = ['green',\n",
    "                     'straight', 'left', 'right', 'empty', 'other', 'yellow', 'red']\n",
    "    class_to_idx = {\n",
    "        cls: idx for idx, cls in enumerate(traffic_class)\n",
    "    }\n",
    "\n",
    "    for traffic_light in traffic_lights:\n",
    "        bbox = traffic_light['bbox']\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # int\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        point1 = (x1, y1)\n",
    "        point2 = (x2, y2)\n",
    "\n",
    "        idx = class_to_idx[traffic_light['class']]\n",
    "        color = 255 - int(255*(idx/len(traffic_class)))\n",
    "\n",
    "        cv2.rectangle(image, point1, point2, color=color, thickness=1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_image_for_cache(path):\n",
    "    if CFG.use_gray_scale:\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # image = cv2.resize(image, (CFG.size, CFG.size))\n",
    "\n",
    "    # 効かない\n",
    "    # image = draw_traffic_light(image, path.split('/')[-2])\n",
    "    return (path, image)\n",
    "\n",
    "\n",
    "def make_video_cache(paths):\n",
    "    debug = []\n",
    "    for idx in range(9):\n",
    "        color = 255 - int(255*(idx/9))\n",
    "        debug.append(color)\n",
    "    print(debug)\n",
    "\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        res = pool.imap_unordered(read_image_for_cache, paths)\n",
    "        res = tqdm(res)\n",
    "        res = list(res)\n",
    "\n",
    "    return dict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import ReplayCompose\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        # aug = A.Compose(cfg.train_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        # aug = A.Compose(cfg.valid_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, cfg, labels=None, transform=None):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.base_paths = df['base_path'].values\n",
    "        # self.labels = df[self.cfg.target_col].values\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def read_image_multiframe(self, idx):\n",
    "        base_path = self.base_paths[idx]\n",
    "\n",
    "        images = []\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "\n",
    "            image = self.cfg.video_cache[path]\n",
    "\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.read_image_multiframe(idx)\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image=image)['image']\n",
    "            replay = None\n",
    "            images = []\n",
    "            for img in image:\n",
    "                if replay is None:\n",
    "                    sample = self.transform(image=img)\n",
    "                    replay = sample['replay']\n",
    "                else:\n",
    "                    sample = ReplayCompose.replay(replay, image=img)\n",
    "                images.append(sample['image'])\n",
    "\n",
    "            image = torch.concat(images, dim=0)\n",
    "\n",
    "        if self.labels is None:\n",
    "            return image\n",
    "\n",
    "        if self.cfg.objective_cv == 'multiclass':\n",
    "            label = torch.tensor(self.labels[idx]).long()\n",
    "        else:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aug_video(train, cfg, plot_count=1):\n",
    "    transform = CFG.train_aug_list\n",
    "    transform = A.ReplayCompose(transform)\n",
    "\n",
    "    dataset = CustomDataset(\n",
    "        train, CFG, transform=transform)\n",
    "\n",
    "    for i in range(plot_count):\n",
    "        image = dataset.read_image_multiframe(i)\n",
    "\n",
    "        if cfg.use_gray_scale:\n",
    "            image = np.stack(image, axis=2)\n",
    "        else:\n",
    "            image = np.concatenate(image, axis=2)\n",
    "\n",
    "        aug_image = dataset[i]\n",
    "        # torch to numpy\n",
    "        aug_image = aug_image.permute(1, 2, 0).numpy()*255\n",
    "\n",
    "        for frame in range(image.shape[-1]):\n",
    "            if frame % 3 != 0:\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            if cfg.use_gray_scale:\n",
    "                axes[0].imshow(image[..., frame], cmap=\"gray\")\n",
    "                axes[1].imshow(aug_image[..., frame], cmap=\"gray\")\n",
    "            else:\n",
    "                axes[0].imshow(image[..., frame:frame+3].astype(int))\n",
    "                axes[1].imshow(aug_image[..., frame:frame+3].astype(int))\n",
    "            plt.savefig(cfg.figures_dir +\n",
    "                        f'aug_{i}_frame{frame}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # xの次元数が4（バッチ、チャネル、高さ、幅）であることを確認\n",
    "        if x.dim() != 4:\n",
    "            raise ValueError(f'Expected 4D input (got {x.dim()}D input)')\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "        # self.cfg = cfg\n",
    "\n",
    "        if model_name is None:\n",
    "            model_name = cfg.model_name\n",
    "\n",
    "        print(f'pretrained: {pretrained}')\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=0,\n",
    "            in_chans=cfg.model_in_chans)\n",
    "\n",
    "        self.n_features = self.model.num_features\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "\n",
    "        # nn.Dropout(0.5),\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.target_size)\n",
    "        )\n",
    "\n",
    "    def feature(self, image):\n",
    "\n",
    "        feature = self.model(image)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.feature(image)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(\n",
    "            optimizer, multiplier, total_epoch, after_scheduler)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [\n",
    "                        base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "def get_scheduler(cfg, optimizer):\n",
    "    if cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=cfg.factor, patience=cfg.patience, verbose=True, eps=cfg.eps)\n",
    "    elif cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer, T_max=cfg.epochs, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=cfg.T_0, T_mult=1, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'GradualWarmupSchedulerV2':\n",
    "        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, cfg.epochs, eta_min=1e-7)\n",
    "        scheduler = GradualWarmupSchedulerV2(\n",
    "            optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def scheduler_step(scheduler, avg_val_loss, epoch):\n",
    "    if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        scheduler.step(avg_val_loss)\n",
    "    elif isinstance(scheduler, CosineAnnealingLR):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, GradualWarmupSchedulerV2):\n",
    "        scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device,\n",
    "             model_ema=None):\n",
    "    \"\"\" 1epoch毎のtrain \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.use_amp)\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    preds_labels = []\n",
    "    start = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with autocast(CFG.use_amp):\n",
    "            y_preds = model(images)\n",
    "\n",
    "            if y_preds.size(1) == 1:\n",
    "                y_preds = y_preds.view(-1)\n",
    "\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.detach().to('cpu').numpy())\n",
    "\n",
    "        preds_labels.append(labels.detach().to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(preds_labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        if y_preds.size(1) == 1:\n",
    "            y_preds = y_preds.view(-1)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # binary\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(folds, fold):\n",
    "\n",
    "    Logger.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    if CFG.use_alldata:\n",
    "        train_folds = folds.copy().reset_index(drop=True)\n",
    "    else:\n",
    "        train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # train_folds = train_downsampling(train_folds)\n",
    "\n",
    "    train_labels = train_folds[CFG.target_col].values\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        train_folds, CFG, labels=train_labels, transform=get_transforms(data='train', cfg=CFG))\n",
    "    valid_dataset = CustomDataset(\n",
    "        valid_folds, CFG, labels=valid_labels, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                              )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "\n",
    "    model = CustomModel(CFG, pretrained=CFG.pretrained)\n",
    "    model.to(device)\n",
    "\n",
    "    if CFG.use_ema:\n",
    "        model_ema = ModelEmaV2(model, decay=CFG.ema_decay)\n",
    "    else:\n",
    "        model_ema = None\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr)\n",
    "    scheduler = get_scheduler(CFG, optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    if CFG.objective_cv == 'binary':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif CFG.objective_cv == 'multiclass':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif CFG.objective_cv == 'regression':\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "    if CFG.metric_direction == 'minimize':\n",
    "        best_score = np.inf\n",
    "    elif CFG.metric_direction == 'maximize':\n",
    "        best_score = -1\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    df_score = pd.DataFrame(columns=[\"train_loss\", 'train_score', 'val_loss', 'val_score'])\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss, train_preds, train_labels_epoch = train_fn(fold, train_loader, model,\n",
    "                                                             criterion, optimizer, epoch, scheduler, device, model_ema)\n",
    "        train_score = get_score(train_labels_epoch, train_preds)\n",
    "\n",
    "        # eval\n",
    "        if model_ema is not None:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model_ema.module, criterion, device)\n",
    "        else:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model, criterion, device)\n",
    "\n",
    "        scheduler_step(scheduler, avg_val_loss, epoch)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, valid_preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        # Logger.info(f'Epoch {epoch+1} - avgScore: {avg_score:.4f}')\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_Score: {train_score:.4f} avgScore: {score:.4f}')\n",
    "        \n",
    "        df_score.loc[epoch] = [avg_loss, train_score, avg_val_loss, score]\n",
    "\n",
    "        if CFG.metric_direction == 'minimize':\n",
    "            update_best = score < best_score\n",
    "        elif CFG.metric_direction == 'maximize':\n",
    "            update_best = score > best_score\n",
    "\n",
    "        if update_best:\n",
    "            best_loss = avg_val_loss\n",
    "            best_score = score\n",
    "\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "\n",
    "            if model_ema is not None:\n",
    "                torch.save({'model': model_ema.module.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "            else:\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save({'model': model.state_dict(),\n",
    "                'preds': valid_preds},\n",
    "               CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    \"\"\"\n",
    "    if model_ema is not None:\n",
    "        torch.save({'model': model_ema.module.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    else:\n",
    "        torch.save({'model': model.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "\n",
    "    check_point = torch.load(\n",
    "        CFG.model_dir + f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth', map_location=torch.device('cpu'))\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "\n",
    "    check_point_pred = check_point['preds']\n",
    "\n",
    "    # Columns must be same length as key 対策\n",
    "    if check_point_pred.ndim == 1:\n",
    "        check_point_pred = check_point_pred.reshape(-1, CFG.target_size)\n",
    "\n",
    "    print('check_point_pred shape', check_point_pred.shape)\n",
    "    valid_folds[pred_cols] = check_point_pred\n",
    "    return valid_folds, df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = pd.read_csv(CFG.train_fold_dir + 'train_folds.csv')\n",
    "    train['ori_idx'] = train.index\n",
    "\n",
    "    train['scene'] = train['ID'].str.split('_').str[0]\n",
    "\n",
    "    \"\"\"\n",
    "    if CFG.is_debug:\n",
    "        use_ids = train['scene'].unique()[:100]\n",
    "        train = train[train['scene'].isin(use_ids)].reset_index(drop=True)\n",
    "    \"\"\"\n",
    "\n",
    "    train['base_path'] = CFG.comp_dataset_path + 'images/' + train['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in train['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    # plot_aug_video(train, CFG, plot_count=10)\n",
    "\n",
    "    # train\n",
    "    oof_df = pd.DataFrame()\n",
    "    list_df_score = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold not in CFG.train_folds:\n",
    "            print(f'fold {fold} is skipped')\n",
    "            continue\n",
    "\n",
    "        _oof_df, _df_score = train_fold(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        list_df_score.append(_df_score)\n",
    "        Logger.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "\n",
    "        if CFG.use_holdout or CFG.use_alldata:\n",
    "            break\n",
    "\n",
    "    oof_df = oof_df.sort_values('ori_idx').reset_index(drop=True)\n",
    "\n",
    "    # CV result\n",
    "    Logger.info(\"========== CV ==========\")\n",
    "    score = get_result(oof_df)\n",
    "\n",
    "    # 学習曲線を可視化する\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    for df_score in list_df_score:\n",
    "        ax1.plot(df_score['val_loss'])\n",
    "        ax2.plot(df_score['val_score'])\n",
    "    ax1.set_title('Validation Loss')\n",
    "    ax2.set_title('Validation Score') \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax1.set_ylim([0, 1.5])\n",
    "    ax2.set_ylim([0, 1.5])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CFG.figures_dir + f'learning_curve_{CFG.exp_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # save result\n",
    "    oof_df.to_csv(CFG.submission_dir + 'oof_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-0.5.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e42d37ed61451aa0b0f71cfbff388a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 2s (remain 21m 4s) Loss: 6.0262(6.0262) Grad: 85801.1016  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.4493(2.3491) Grad: 387529.8125  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.5360(2.2752) Grad: 294086.5312  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 1.5865(1.5865) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.2752  avg_val_loss: 1.7777  time: 78s\n",
      "Epoch 1 - avg_train_Score: 2.2752 avgScore: 1.7777\n",
      "Epoch 1 - Save Best Score: 1.7777 Model\n",
      "Epoch 1 - Save Best Loss: 1.7777 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.6941(1.7777) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 15s) Loss: 1.3366(1.3366) Grad: 240240.2344  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.2177(1.2015) Grad: 127443.4453  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 1.3462(1.1963) Grad: 119637.0156  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 1.0776(1.0776) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1963  avg_val_loss: 1.0577  time: 76s\n",
      "Epoch 2 - avg_train_Score: 1.1963 avgScore: 1.0577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 1.0533(1.0577) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Save Best Score: 1.0577 Model\n",
      "Epoch 2 - Save Best Loss: 1.0577 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 44s) Loss: 1.0540(1.0540) Grad: 224408.5469  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.1572(1.2658) Grad: 186599.6406  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.8374(1.2554) Grad: 162324.6250  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 1.1148(1.1148) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2554  avg_val_loss: 1.0079  time: 78s\n",
      "Epoch 3 - avg_train_Score: 1.2554 avgScore: 1.0079\n",
      "Epoch 3 - Save Best Score: 1.0079 Model\n",
      "Epoch 3 - Save Best Loss: 1.0079 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0547(1.0079) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 11m 14s) Loss: 1.3161(1.3161) Grad: 182807.8906  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.2395(1.0632) Grad: 155879.5156  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9494(1.0631) Grad: 154049.3125  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.9867(0.9867) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9651(0.9362) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0631  avg_val_loss: 0.9362  time: 78s\n",
      "Epoch 4 - avg_train_Score: 1.0631 avgScore: 0.9362\n",
      "Epoch 4 - Save Best Score: 0.9362 Model\n",
      "Epoch 4 - Save Best Loss: 0.9362 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 10m 54s) Loss: 1.0229(1.0229) Grad: 164060.0312  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.0868(0.9709) Grad: 136227.7031  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9400(0.9710) Grad: 147903.8594  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.9907(0.9907) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9251(0.8963) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9710  avg_val_loss: 0.8963  time: 78s\n",
      "Epoch 5 - avg_train_Score: 0.9710 avgScore: 0.8963\n",
      "Epoch 5 - Save Best Score: 0.8963 Model\n",
      "Epoch 5 - Save Best Loss: 0.8963 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 10m 42s) Loss: 0.8419(0.8419) Grad: 166422.1250  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8733(0.8951) Grad: 142547.5312  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9062(0.8959) Grad: 132938.8906  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.9241(0.9241) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8959  avg_val_loss: 0.8529  time: 78s\n",
      "Epoch 6 - avg_train_Score: 0.8959 avgScore: 0.8529\n",
      "Epoch 6 - Save Best Score: 0.8529 Model\n",
      "Epoch 6 - Save Best Loss: 0.8529 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8827(0.8529) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 20s) Loss: 0.6408(0.6408) Grad: 92948.2500  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.7907(0.8181) Grad: 139256.1250  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.7673(0.8180) Grad: 138324.8281  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.8767(0.8767) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8180  avg_val_loss: 0.8144  time: 78s\n",
      "Epoch 7 - avg_train_Score: 0.8180 avgScore: 0.8144\n",
      "Epoch 7 - Save Best Score: 0.8144 Model\n",
      "Epoch 7 - Save Best Loss: 0.8144 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8443(0.8144) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 10m 39s) Loss: 0.8129(0.8129) Grad: 132884.2188  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8151(0.7524) Grad: 131422.0312  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.6157(0.7542) Grad: 102802.6094  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7929(0.7929) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7542  avg_val_loss: 0.7821  time: 79s\n",
      "Epoch 8 - avg_train_Score: 0.7542 avgScore: 0.7821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7739(0.7821) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Save Best Score: 0.7821 Model\n",
      "Epoch 8 - Save Best Loss: 0.7821 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 10m 47s) Loss: 0.5720(0.5720) Grad: 116953.7422  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7570(0.6896) Grad: 131040.6328  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6556(0.6892) Grad: 182708.1094  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7424(0.7424) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6892  avg_val_loss: 0.7523  time: 78s\n",
      "Epoch 9 - avg_train_Score: 0.6892 avgScore: 0.7523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7535(0.7523) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Save Best Score: 0.7523 Model\n",
      "Epoch 9 - Save Best Loss: 0.7523 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 13m 12s) Loss: 0.5527(0.5527) Grad: 102024.6562  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.5620(0.6177) Grad: 96161.3047  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.6640(0.6184) Grad: 119099.7891  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.6903(0.6903) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6184  avg_val_loss: 0.7341  time: 78s\n",
      "Epoch 10 - avg_train_Score: 0.6184 avgScore: 0.7341\n",
      "Epoch 10 - Save Best Score: 0.7341 Model\n",
      "Epoch 10 - Save Best Loss: 0.7341 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7727(0.7341) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 10m 31s) Loss: 0.5090(0.5090) Grad: 103513.6562  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.5859(0.5635) Grad: 82180.0078  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.4799(0.5639) Grad: 99730.8359  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6675(0.6675) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5639  avg_val_loss: 0.7216  time: 77s\n",
      "Epoch 11 - avg_train_Score: 0.5639 avgScore: 0.7216\n",
      "Epoch 11 - Save Best Score: 0.7216 Model\n",
      "Epoch 11 - Save Best Loss: 0.7216 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7795(0.7216) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 11m 33s) Loss: 0.5361(0.5361) Grad: 117075.4375  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6168(0.5132) Grad: 114855.6094  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4722(0.5164) Grad: 116993.8594  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6410(0.6410) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.5164  avg_val_loss: 0.7109  time: 78s\n",
      "Epoch 12 - avg_train_Score: 0.5164 avgScore: 0.7109\n",
      "Epoch 12 - Save Best Score: 0.7109 Model\n",
      "Epoch 12 - Save Best Loss: 0.7109 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7421(0.7109) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.4187(0.4187) Grad: 102626.1719  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.3902(0.4620) Grad: 87050.9219  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.5083(0.4625) Grad: 115612.4609  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6366(0.6366) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4625  avg_val_loss: 0.7063  time: 77s\n",
      "Epoch 13 - avg_train_Score: 0.4625 avgScore: 0.7063\n",
      "Epoch 13 - Save Best Score: 0.7063 Model\n",
      "Epoch 13 - Save Best Loss: 0.7063 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7228(0.7063) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 10m 52s) Loss: 0.4685(0.4685) Grad: 156725.7500  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4035(0.4208) Grad: 108160.6562  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3783(0.4200) Grad: 98613.7578  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.6492(0.6492) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4200  avg_val_loss: 0.7066  time: 77s\n",
      "Epoch 14 - avg_train_Score: 0.4200 avgScore: 0.7066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7330(0.7066) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 59s) Loss: 0.3460(0.3460) Grad: 84138.4688  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.4096(0.3801) Grad: 85926.8203  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.4040(0.3808) Grad: 88751.8672  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6564(0.6564) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7250(0.7072) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3808  avg_val_loss: 0.7072  time: 79s\n",
      "Epoch 15 - avg_train_Score: 0.3808 avgScore: 0.7072\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 10m 44s) Loss: 0.3436(0.3436) Grad: 76067.6641  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3119(0.3403) Grad: 80354.3359  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.3698(0.3401) Grad: 90196.0547  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6564(0.6564) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3401  avg_val_loss: 0.7031  time: 79s\n",
      "Epoch 16 - avg_train_Score: 0.3401 avgScore: 0.7031\n",
      "Epoch 16 - Save Best Score: 0.7031 Model\n",
      "Epoch 16 - Save Best Loss: 0.7031 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7156(0.7031) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 34s) Loss: 0.3214(0.3214) Grad: 78699.7266  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.3220(0.3074) Grad: 61285.2773  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.3058(0.3063) Grad: 85075.2500  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6523(0.6523) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.3063  avg_val_loss: 0.6994  time: 77s\n",
      "Epoch 17 - avg_train_Score: 0.3063 avgScore: 0.6994\n",
      "Epoch 17 - Save Best Score: 0.6994 Model\n",
      "Epoch 17 - Save Best Loss: 0.6994 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7025(0.6994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 12m 19s) Loss: 0.3292(0.3292) Grad: 64536.8047  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3241(0.2777) Grad: 64849.6406  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2807(0.2775) Grad: 71288.3984  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6531(0.6531) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2775  avg_val_loss: 0.6993  time: 78s\n",
      "Epoch 18 - avg_train_Score: 0.2775 avgScore: 0.6993\n",
      "Epoch 18 - Save Best Score: 0.6993 Model\n",
      "Epoch 18 - Save Best Loss: 0.6993 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6968(0.6993) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 0.2990(0.2990) Grad: 60661.1758  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.2106(0.2520) Grad: 68629.0000  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.2592(0.2520) Grad: 70999.5469  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6478(0.6478) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2520  avg_val_loss: 0.6994  time: 78s\n",
      "Epoch 19 - avg_train_Score: 0.2520 avgScore: 0.6994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7018(0.6994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 36s) Loss: 0.2274(0.2274) Grad: 78352.9688  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.2647(0.2337) Grad: 88788.9922  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.2156(0.2338) Grad: 86868.3438  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6574(0.6574) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7029(0.7005) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2338  avg_val_loss: 0.7005  time: 77s\n",
      "Epoch 20 - avg_train_Score: 0.2338 avgScore: 0.7005\n",
      "/tmp/ipykernel_65734/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 0 result ==========\n",
      "score: 0.7005\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8675, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 42s) Loss: 5.5422(5.5422) Grad: 93748.8594  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.4097(2.3507) Grad: 277868.6250  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 1.2716(2.2730) Grad: 289723.2500  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 2.1029(2.1029) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.2730  avg_val_loss: 1.8662  time: 78s\n",
      "Epoch 1 - avg_train_Score: 2.2730 avgScore: 1.8662\n",
      "Epoch 1 - Save Best Score: 1.8662 Model\n",
      "Epoch 1 - Save Best Loss: 1.8662 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.5915(1.8662) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 11m 7s) Loss: 1.2440(1.2440) Grad: 261736.3438  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9862(1.1791) Grad: 106646.4141  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 1.3233(1.1722) Grad: 131747.3750  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 1.0905(1.0905) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1722  avg_val_loss: 1.0017  time: 79s\n",
      "Epoch 2 - avg_train_Score: 1.1722 avgScore: 1.0017\n",
      "Epoch 2 - Save Best Score: 1.0017 Model\n",
      "Epoch 2 - Save Best Loss: 1.0017 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0060(1.0017) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 29s) Loss: 1.0924(1.0924) Grad: 297296.2188  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.0203(1.2265) Grad: 155300.6250  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 1.2341(1.2197) Grad: 233785.3594  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 1.0531(1.0531) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2197  avg_val_loss: 0.9566  time: 78s\n",
      "Epoch 3 - avg_train_Score: 1.2197 avgScore: 0.9566\n",
      "Epoch 3 - Save Best Score: 0.9566 Model\n",
      "Epoch 3 - Save Best Loss: 0.9566 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9423(0.9566) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 12m 44s) Loss: 1.6132(1.6132) Grad: 277334.9688  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9445(1.0557) Grad: 196049.8906  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.9444(1.0559) Grad: 172261.4531  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 1.0076(1.0076) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0559  avg_val_loss: 0.8994  time: 79s\n",
      "Epoch 4 - avg_train_Score: 1.0559 avgScore: 0.8994\n",
      "Epoch 4 - Save Best Score: 0.8994 Model\n",
      "Epoch 4 - Save Best Loss: 0.8994 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9067(0.8994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 11m 45s) Loss: 0.9131(0.9131) Grad: 144167.2344  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8234(0.9606) Grad: 153973.5000  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9855(0.9596) Grad: 169786.4844  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.9141(0.9141) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9596  avg_val_loss: 0.8558  time: 78s\n",
      "Epoch 5 - avg_train_Score: 0.9596 avgScore: 0.8558\n",
      "Epoch 5 - Save Best Score: 0.8558 Model\n",
      "Epoch 5 - Save Best Loss: 0.8558 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8654(0.8558) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 1.0068(1.0068) Grad: 167697.0312  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7881(0.8877) Grad: 131151.0625  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9208(0.8884) Grad: 153229.6406  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.8931(0.8931) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8884  avg_val_loss: 0.8192  time: 78s\n",
      "Epoch 6 - avg_train_Score: 0.8884 avgScore: 0.8192\n",
      "Epoch 6 - Save Best Score: 0.8192 Model\n",
      "Epoch 6 - Save Best Loss: 0.8192 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7866(0.8192) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 29s) Loss: 0.6706(0.6706) Grad: 121860.1875  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8779(0.8283) Grad: 119249.7812  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7165(0.8249) Grad: 121113.9922  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.8228(0.8228) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8249  avg_val_loss: 0.7768  time: 78s\n",
      "Epoch 7 - avg_train_Score: 0.8249 avgScore: 0.7768\n",
      "Epoch 7 - Save Best Score: 0.7768 Model\n",
      "Epoch 7 - Save Best Loss: 0.7768 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7423(0.7768) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 11m 2s) Loss: 0.8844(0.8844) Grad: 151396.6719  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7521(0.7542) Grad: 112140.5312  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7804(0.7541) Grad: 80475.3750  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7276(0.7276) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7541  avg_val_loss: 0.7426  time: 78s\n",
      "Epoch 8 - avg_train_Score: 0.7541 avgScore: 0.7426\n",
      "Epoch 8 - Save Best Score: 0.7426 Model\n",
      "Epoch 8 - Save Best Loss: 0.7426 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7280(0.7426) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 10m 58s) Loss: 0.8401(0.8401) Grad: 103396.6172  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7642(0.6895) Grad: 100841.0000  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6778(0.6904) Grad: 114215.6172  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7681(0.7681) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6904  avg_val_loss: 0.7222  time: 78s\n",
      "Epoch 9 - avg_train_Score: 0.6904 avgScore: 0.7222\n",
      "Epoch 9 - Save Best Score: 0.7222 Model\n",
      "Epoch 9 - Save Best Loss: 0.7222 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6948(0.7222) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 2s) Loss: 0.6548(0.6548) Grad: 91740.8438  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.5067(0.6338) Grad: 107180.4219  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.6469(0.6330) Grad: 90619.9297  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7288(0.7288) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6330  avg_val_loss: 0.7032  time: 79s\n",
      "Epoch 10 - avg_train_Score: 0.6330 avgScore: 0.7032\n",
      "Epoch 10 - Save Best Score: 0.7032 Model\n",
      "Epoch 10 - Save Best Loss: 0.7032 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6595(0.7032) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 10m 48s) Loss: 0.6226(0.6226) Grad: 89598.5625  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6783(0.5684) Grad: 94298.7500  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6194(0.5694) Grad: 83632.1875  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7090(0.7090) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5694  avg_val_loss: 0.6902  time: 78s\n",
      "Epoch 11 - avg_train_Score: 0.5694 avgScore: 0.6902\n",
      "Epoch 11 - Save Best Score: 0.6902 Model\n",
      "Epoch 11 - Save Best Loss: 0.6902 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6330(0.6902) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 11m 19s) Loss: 0.4971(0.4971) Grad: 102933.3984  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4214(0.5186) Grad: 83479.2109  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5229(0.5185) Grad: 102624.1250  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6909(0.6909) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6022(0.6760) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.5185  avg_val_loss: 0.6760  time: 78s\n",
      "Epoch 12 - avg_train_Score: 0.5185 avgScore: 0.6760\n",
      "Epoch 12 - Save Best Score: 0.6760 Model\n",
      "Epoch 12 - Save Best Loss: 0.6760 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 11m 35s) Loss: 0.4615(0.4615) Grad: 88228.8438  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4641(0.4708) Grad: 97467.2422  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5045(0.4704) Grad: 109356.6484  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6678(0.6678) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4704  avg_val_loss: 0.6713  time: 79s\n",
      "Epoch 13 - avg_train_Score: 0.4704 avgScore: 0.6713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6183(0.6713) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Save Best Score: 0.6713 Model\n",
      "Epoch 13 - Save Best Loss: 0.6713 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 22s) Loss: 0.4607(0.4607) Grad: 87754.2734  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.4328(0.4257) Grad: 74759.2500  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.4478(0.4261) Grad: 88092.1484  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6666(0.6666) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4261  avg_val_loss: 0.6617  time: 79s\n",
      "Epoch 14 - avg_train_Score: 0.4261 avgScore: 0.6617\n",
      "Epoch 14 - Save Best Score: 0.6617 Model\n",
      "Epoch 14 - Save Best Loss: 0.6617 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6129(0.6617) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 11m 20s) Loss: 0.4129(0.4129) Grad: 91727.3125  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.3379(0.3797) Grad: 67058.9375  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.3883(0.3797) Grad: 73078.3984  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6580(0.6580) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3797  avg_val_loss: 0.6538  time: 79s\n",
      "Epoch 15 - avg_train_Score: 0.3797 avgScore: 0.6538\n",
      "Epoch 15 - Save Best Score: 0.6538 Model\n",
      "Epoch 15 - Save Best Loss: 0.6538 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6139(0.6538) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 12m 17s) Loss: 0.3707(0.3707) Grad: 71671.8438  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3666(0.3407) Grad: 88087.2188  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3104(0.3415) Grad: 97720.5156  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6573(0.6573) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3415  avg_val_loss: 0.6540  time: 78s\n",
      "Epoch 16 - avg_train_Score: 0.3415 avgScore: 0.6540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6150(0.6540) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 40s) Loss: 0.3479(0.3479) Grad: 94684.0078  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2739(0.3079) Grad: 62665.3164  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2657(0.3077) Grad: 57488.3359  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6599(0.6599) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6125(0.6536) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.3077  avg_val_loss: 0.6536  time: 78s\n",
      "Epoch 17 - avg_train_Score: 0.3077 avgScore: 0.6536\n",
      "Epoch 17 - Save Best Score: 0.6536 Model\n",
      "Epoch 17 - Save Best Loss: 0.6536 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 36s) Loss: 0.2848(0.2848) Grad: 81146.4062  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.2547(0.2795) Grad: 54356.2891  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2590(0.2785) Grad: 63029.9492  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.6658(0.6658) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2785  avg_val_loss: 0.6534  time: 80s\n",
      "Epoch 18 - avg_train_Score: 0.2785 avgScore: 0.6534\n",
      "Epoch 18 - Save Best Score: 0.6534 Model\n",
      "Epoch 18 - Save Best Loss: 0.6534 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6107(0.6534) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 13m 17s) Loss: 0.2274(0.2274) Grad: 71896.7500  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.2021(0.2525) Grad: 70325.7422  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2508(0.2529) Grad: 56083.2930  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6635(0.6635) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2529  avg_val_loss: 0.6537  time: 79s\n",
      "Epoch 19 - avg_train_Score: 0.2529 avgScore: 0.6537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6137(0.6537) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 48s) Loss: 0.2450(0.2450) Grad: 56906.6484  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.2579(0.2370) Grad: 57415.3555  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2461(0.2368) Grad: 62346.7422  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6566(0.6566) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2368  avg_val_loss: 0.6528  time: 79s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6176(0.6528) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 - avg_train_Score: 0.2368 avgScore: 0.6528\n",
      "Epoch 20 - Save Best Score: 0.6528 Model\n",
      "Epoch 20 - Save Best Loss: 0.6528 Model\n",
      "/tmp/ipykernel_65734/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 1 result ==========\n",
      "score: 0.6528\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 11m 4s) Loss: 6.3834(6.3834) Grad: 86569.3047  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.6034(2.3697) Grad: 153017.9375  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 1.4015(2.2885) Grad: 151472.1406  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 1.9620(1.9620) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.2885  avg_val_loss: 2.0968  time: 78s\n",
      "Epoch 1 - avg_train_Score: 2.2885 avgScore: 2.0968\n",
      "Epoch 1 - Save Best Score: 2.0968 Model\n",
      "Epoch 1 - Save Best Loss: 2.0968 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 2.0806(2.0968) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 12m 1s) Loss: 1.4459(1.4459) Grad: 288156.7188  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.9661(1.1691) Grad: 136438.4062  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 1.2400(1.1693) Grad: 144437.9375  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.8897(0.8897) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1693  avg_val_loss: 1.0227  time: 79s\n",
      "Epoch 2 - avg_train_Score: 1.1693 avgScore: 1.0227\n",
      "Epoch 2 - Save Best Score: 1.0227 Model\n",
      "Epoch 2 - Save Best Loss: 1.0227 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.1442(1.0227) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 11m 32s) Loss: 1.1098(1.1098) Grad: 277529.8438  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.9721(1.2701) Grad: 201191.5000  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 1.0844(1.2621) Grad: 227967.5469  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7664(0.7664) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2621  avg_val_loss: 0.9861  time: 77s\n",
      "Epoch 3 - avg_train_Score: 1.2621 avgScore: 0.9861\n",
      "Epoch 3 - Save Best Score: 0.9861 Model\n",
      "Epoch 3 - Save Best Loss: 0.9861 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0732(0.9861) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 11m 38s) Loss: 1.1423(1.1423) Grad: 190770.0781  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9696(1.0936) Grad: 140543.4375  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9751(1.0903) Grad: 127394.3047  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7603(0.7603) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0903  avg_val_loss: 0.9101  time: 78s\n",
      "Epoch 4 - avg_train_Score: 1.0903 avgScore: 0.9101\n",
      "Epoch 4 - Save Best Score: 0.9101 Model\n",
      "Epoch 4 - Save Best Loss: 0.9101 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 1.0023(0.9101) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 14m 22s) Loss: 0.8854(0.8854) Grad: 133684.7812  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8210(0.9719) Grad: 117060.8906  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.0447(0.9736) Grad: 152600.1562  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.7099(0.7099) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9017(0.8553) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9736  avg_val_loss: 0.8553  time: 78s\n",
      "Epoch 5 - avg_train_Score: 0.9736 avgScore: 0.8553\n",
      "Epoch 5 - Save Best Score: 0.8553 Model\n",
      "Epoch 5 - Save Best Loss: 0.8553 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 11m 47s) Loss: 0.8994(0.8994) Grad: 128844.9688  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.0868(0.8865) Grad: 176458.1094  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.8036(0.8854) Grad: 127920.3750  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6692(0.6692) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8854  avg_val_loss: 0.8160  time: 77s\n",
      "Epoch 6 - avg_train_Score: 0.8854 avgScore: 0.8160\n",
      "Epoch 6 - Save Best Score: 0.8160 Model\n",
      "Epoch 6 - Save Best Loss: 0.8160 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9215(0.8160) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 12m 15s) Loss: 0.8790(0.8790) Grad: 132771.3438  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.8732(0.8089) Grad: 113172.3203  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.7731(0.8099) Grad: 107170.6719  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6459(0.6459) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8099  avg_val_loss: 0.7755  time: 79s\n",
      "Epoch 7 - avg_train_Score: 0.8099 avgScore: 0.7755\n",
      "Epoch 7 - Save Best Score: 0.7755 Model\n",
      "Epoch 7 - Save Best Loss: 0.7755 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8190(0.7755) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 11m 0s) Loss: 0.8822(0.8822) Grad: 99250.7109  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.7291(0.7372) Grad: 121113.8750  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.7367(0.7376) Grad: 79986.0625  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6326(0.6326) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7376  avg_val_loss: 0.7478  time: 77s\n",
      "Epoch 8 - avg_train_Score: 0.7376 avgScore: 0.7478\n",
      "Epoch 8 - Save Best Score: 0.7478 Model\n",
      "Epoch 8 - Save Best Loss: 0.7478 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7499(0.7478) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 5s) Loss: 0.6388(0.6388) Grad: 117678.9688  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7117(0.6636) Grad: 106863.9375  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.6400(0.6653) Grad: 111701.5703  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6434(0.6434) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6653  avg_val_loss: 0.7184  time: 77s\n",
      "Epoch 9 - avg_train_Score: 0.6653 avgScore: 0.7184\n",
      "Epoch 9 - Save Best Score: 0.7184 Model\n",
      "Epoch 9 - Save Best Loss: 0.7184 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7221(0.7184) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.6312(0.6312) Grad: 117245.4062  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5677(0.6138) Grad: 85876.3828  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5480(0.6134) Grad: 96460.9375  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.6379(0.6379) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6134  avg_val_loss: 0.6998  time: 78s\n",
      "Epoch 10 - avg_train_Score: 0.6134 avgScore: 0.6998\n",
      "Epoch 10 - Save Best Score: 0.6998 Model\n",
      "Epoch 10 - Save Best Loss: 0.6998 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6420(0.6998) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 11m 7s) Loss: 0.5363(0.5363) Grad: 91036.5391  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6475(0.5540) Grad: 129739.6328  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5798(0.5532) Grad: 98385.3516  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6247(0.6247) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5532  avg_val_loss: 0.6883  time: 78s\n",
      "Epoch 11 - avg_train_Score: 0.5532 avgScore: 0.6883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6758(0.6883) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Save Best Score: 0.6883 Model\n",
      "Epoch 11 - Save Best Loss: 0.6883 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 11m 34s) Loss: 0.4583(0.4583) Grad: 85837.7891  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4826(0.4986) Grad: 79349.4922  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5045(0.4992) Grad: 126606.5312  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6239(0.6239) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4992  avg_val_loss: 0.6806  time: 78s\n",
      "Epoch 12 - avg_train_Score: 0.4992 avgScore: 0.6806\n",
      "Epoch 12 - Save Best Score: 0.6806 Model\n",
      "Epoch 12 - Save Best Loss: 0.6806 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6415(0.6806) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 12m 0s) Loss: 0.5064(0.5064) Grad: 89620.8203  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.4759(0.4557) Grad: 94185.7891  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.4428(0.4551) Grad: 80927.9531  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.5984(0.5984) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4551  avg_val_loss: 0.6742  time: 77s\n",
      "Epoch 13 - avg_train_Score: 0.4551 avgScore: 0.6742\n",
      "Epoch 13 - Save Best Score: 0.6742 Model\n",
      "Epoch 13 - Save Best Loss: 0.6742 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6747(0.6742) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 8s) Loss: 0.4004(0.4004) Grad: 72887.3281  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4912(0.4125) Grad: 72245.3750  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3620(0.4121) Grad: 77828.6953  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.5938(0.5938) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4121  avg_val_loss: 0.6677  time: 78s\n",
      "Epoch 14 - avg_train_Score: 0.4121 avgScore: 0.6677\n",
      "Epoch 14 - Save Best Score: 0.6677 Model\n",
      "Epoch 14 - Save Best Loss: 0.6677 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6667(0.6677) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 11m 14s) Loss: 0.3535(0.3535) Grad: 86393.8125  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2966(0.3727) Grad: 83777.0312  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3648(0.3732) Grad: 87184.0547  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.5772(0.5772) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3732  avg_val_loss: 0.6651  time: 78s\n",
      "Epoch 15 - avg_train_Score: 0.3732 avgScore: 0.6651\n",
      "Epoch 15 - Save Best Score: 0.6651 Model\n",
      "Epoch 15 - Save Best Loss: 0.6651 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6653(0.6651) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 11m 28s) Loss: 0.3771(0.3771) Grad: 68186.5000  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3315(0.3358) Grad: 64967.6758  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3510(0.3358) Grad: 91969.2812  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.5776(0.5776) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3358  avg_val_loss: 0.6604  time: 78s\n",
      "Epoch 16 - avg_train_Score: 0.3358 avgScore: 0.6604\n",
      "Epoch 16 - Save Best Score: 0.6604 Model\n",
      "Epoch 16 - Save Best Loss: 0.6604 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6553(0.6604) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 11m 37s) Loss: 0.2861(0.2861) Grad: 80142.7734  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.3128(0.3011) Grad: 75372.1719  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.3160(0.3016) Grad: 88657.8359  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.5867(0.5867) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.3016  avg_val_loss: 0.6599  time: 79s\n",
      "Epoch 17 - avg_train_Score: 0.3016 avgScore: 0.6599\n",
      "Epoch 17 - Save Best Score: 0.6599 Model\n",
      "Epoch 17 - Save Best Loss: 0.6599 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6604(0.6599) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 4s) Loss: 0.2846(0.2846) Grad: 104085.9688  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2489(0.2755) Grad: 93134.0938  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2473(0.2756) Grad: 62674.3086  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.5860(0.5860) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2756  avg_val_loss: 0.6587  time: 78s\n",
      "Epoch 18 - avg_train_Score: 0.2756 avgScore: 0.6587\n",
      "Epoch 18 - Save Best Score: 0.6587 Model\n",
      "Epoch 18 - Save Best Loss: 0.6587 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6571(0.6587) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.2431(0.2431) Grad: 51892.4844  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2416(0.2501) Grad: 68119.3984  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2918(0.2500) Grad: 101742.5156  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.5889(0.5889) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2500  avg_val_loss: 0.6575  time: 78s\n",
      "Epoch 19 - avg_train_Score: 0.2500 avgScore: 0.6575\n",
      "Epoch 19 - Save Best Score: 0.6575 Model\n",
      "Epoch 19 - Save Best Loss: 0.6575 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6468(0.6575) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 17s) Loss: 0.2646(0.2646) Grad: 86872.9609  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2291(0.2326) Grad: 54347.5469  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2991(0.2324) Grad: 59645.0703  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.5893(0.5893) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2324  avg_val_loss: 0.6570  time: 79s\n",
      "Epoch 20 - avg_train_Score: 0.2324 avgScore: 0.6570\n",
      "Epoch 20 - Save Best Score: 0.6570 Model\n",
      "Epoch 20 - Save Best Loss: 0.6570 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6396(0.6570) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 2 result ==========\n",
      "score: 0.6570\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 12m 8s) Loss: 6.6057(6.6057) Grad: 95085.3516  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.5225(2.4253) Grad: 300924.0625  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.2960(2.3430) Grad: 274162.5938  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 2.2212(2.2212) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.3430  avg_val_loss: 2.1810  time: 78s\n",
      "Epoch 1 - avg_train_Score: 2.3430 avgScore: 2.1810\n",
      "Epoch 1 - Save Best Score: 2.1810 Model\n",
      "Epoch 1 - Save Best Loss: 2.1810 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.9440(2.1810) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 40s) Loss: 0.8538(0.8538) Grad: 249628.5781  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.9193(1.1843) Grad: 116176.1641  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.9782(1.1795) Grad: 113489.9141  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.9390(0.9390) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1795  avg_val_loss: 1.0661  time: 77s\n",
      "Epoch 2 - avg_train_Score: 1.1795 avgScore: 1.0661\n",
      "Epoch 2 - Save Best Score: 1.0661 Model\n",
      "Epoch 2 - Save Best Loss: 1.0661 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0743(1.0661) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 11m 7s) Loss: 1.0409(1.0409) Grad: 248897.7188  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.2567(1.2300) Grad: 194354.1250  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 1.3798(1.2227) Grad: 177787.8281  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.8832(0.8832) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2227  avg_val_loss: 1.0123  time: 78s\n",
      "Epoch 3 - avg_train_Score: 1.2227 avgScore: 1.0123\n",
      "Epoch 3 - Save Best Score: 1.0123 Model\n",
      "Epoch 3 - Save Best Loss: 1.0123 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0438(1.0123) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 11m 29s) Loss: 1.0381(1.0381) Grad: 163649.2812  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.1045(1.0674) Grad: 135987.5156  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.8805(1.0646) Grad: 165587.9688  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7771(0.7771) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0646  avg_val_loss: 0.9423  time: 78s\n",
      "Epoch 4 - avg_train_Score: 1.0646 avgScore: 0.9423\n",
      "Epoch 4 - Save Best Score: 0.9423 Model\n",
      "Epoch 4 - Save Best Loss: 0.9423 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0264(0.9423) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 12m 16s) Loss: 0.9128(0.9128) Grad: 157478.3906  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.8055(0.9555) Grad: 120610.3281  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.9691(0.9562) Grad: 165529.5938  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7364(0.7364) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9526(0.8952) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9562  avg_val_loss: 0.8952  time: 78s\n",
      "Epoch 5 - avg_train_Score: 0.9562 avgScore: 0.8952\n",
      "Epoch 5 - Save Best Score: 0.8952 Model\n",
      "Epoch 5 - Save Best Loss: 0.8952 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 10m 44s) Loss: 0.7676(0.7676) Grad: 172879.7812  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.0960(0.8933) Grad: 177319.2031  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7747(0.8949) Grad: 118065.1328  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7574(0.7574) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8949  avg_val_loss: 0.8438  time: 78s\n",
      "Epoch 6 - avg_train_Score: 0.8949 avgScore: 0.8438\n",
      "Epoch 6 - Save Best Score: 0.8438 Model\n",
      "Epoch 6 - Save Best Loss: 0.8438 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9300(0.8438) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 10m 47s) Loss: 0.8199(0.8199) Grad: 154032.1719  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.7629(0.8090) Grad: 114189.0000  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.7452(0.8085) Grad: 116937.3203  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7039(0.7039) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8085  avg_val_loss: 0.8094  time: 77s\n",
      "Epoch 7 - avg_train_Score: 0.8085 avgScore: 0.8094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8673(0.8094) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Save Best Score: 0.8094 Model\n",
      "Epoch 7 - Save Best Loss: 0.8094 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 10m 51s) Loss: 0.9351(0.9351) Grad: 141784.6875  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6696(0.7423) Grad: 161418.4062  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7081(0.7434) Grad: 97591.2734  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.6639(0.6639) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7434  avg_val_loss: 0.7734  time: 78s\n",
      "Epoch 8 - avg_train_Score: 0.7434 avgScore: 0.7734\n",
      "Epoch 8 - Save Best Score: 0.7734 Model\n",
      "Epoch 8 - Save Best Loss: 0.7734 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8616(0.7734) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 10m 48s) Loss: 0.5511(0.5511) Grad: 90712.6953  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.7245(0.6737) Grad: 100852.9453  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.7583(0.6731) Grad: 124023.1875  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6476(0.6476) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6731  avg_val_loss: 0.7518  time: 79s\n",
      "Epoch 9 - avg_train_Score: 0.6731 avgScore: 0.7518\n",
      "Epoch 9 - Save Best Score: 0.7518 Model\n",
      "Epoch 9 - Save Best Loss: 0.7518 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7932(0.7518) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 0.6091(0.6091) Grad: 116173.3359  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6099(0.6209) Grad: 106530.0547  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5937(0.6215) Grad: 100283.3281  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.5876(0.5876) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6215  avg_val_loss: 0.7323  time: 78s\n",
      "Epoch 10 - avg_train_Score: 0.6215 avgScore: 0.7323\n",
      "Epoch 10 - Save Best Score: 0.7323 Model\n",
      "Epoch 10 - Save Best Loss: 0.7323 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7783(0.7323) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 0.6560(0.6560) Grad: 83069.9141  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.5632(0.5663) Grad: 118910.7109  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.5495(0.5664) Grad: 84724.0625  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6115(0.6115) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7607(0.7235) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5664  avg_val_loss: 0.7235  time: 79s\n",
      "Epoch 11 - avg_train_Score: 0.5664 avgScore: 0.7235\n",
      "Epoch 11 - Save Best Score: 0.7235 Model\n",
      "Epoch 11 - Save Best Loss: 0.7235 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 11m 34s) Loss: 0.4714(0.4714) Grad: 116863.1250  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4697(0.5125) Grad: 94100.3516  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5485(0.5123) Grad: 89107.0859  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.5933(0.5933) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.5123  avg_val_loss: 0.7140  time: 78s\n",
      "Epoch 12 - avg_train_Score: 0.5123 avgScore: 0.7140\n",
      "Epoch 12 - Save Best Score: 0.7140 Model\n",
      "Epoch 12 - Save Best Loss: 0.7140 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7749(0.7140) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 0.4293(0.4293) Grad: 81849.7500  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5231(0.4658) Grad: 84932.4453  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4275(0.4653) Grad: 72175.6094  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.5882(0.5882) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4653  avg_val_loss: 0.7076  time: 78s\n",
      "Epoch 13 - avg_train_Score: 0.4653 avgScore: 0.7076\n",
      "Epoch 13 - Save Best Score: 0.7076 Model\n",
      "Epoch 13 - Save Best Loss: 0.7076 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7603(0.7076) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 21s) Loss: 0.4052(0.4052) Grad: 90220.7422  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.3434(0.4175) Grad: 76561.1719  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.4471(0.4179) Grad: 92170.1719  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.5929(0.5929) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.4179  avg_val_loss: 0.7031  time: 79s\n",
      "Epoch 14 - avg_train_Score: 0.4179 avgScore: 0.7031\n",
      "Epoch 14 - Save Best Score: 0.7031 Model\n",
      "Epoch 14 - Save Best Loss: 0.7031 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7384(0.7031) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 11m 22s) Loss: 0.4133(0.4133) Grad: 105208.3203  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.3821(0.3799) Grad: 79883.8828  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.4354(0.3788) Grad: 89150.1484  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.5903(0.5903) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3788  avg_val_loss: 0.6999  time: 80s\n",
      "Epoch 15 - avg_train_Score: 0.3788 avgScore: 0.6999\n",
      "Epoch 15 - Save Best Score: 0.6999 Model\n",
      "Epoch 15 - Save Best Loss: 0.6999 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7345(0.6999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 10m 48s) Loss: 0.3812(0.3812) Grad: 68232.5312  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3568(0.3372) Grad: 70057.0547  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4025(0.3372) Grad: 92864.4531  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6122(0.6122) \n",
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7310(0.6969) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3372  avg_val_loss: 0.6969  time: 78s\n",
      "Epoch 16 - avg_train_Score: 0.3372 avgScore: 0.6969\n",
      "Epoch 16 - Save Best Score: 0.6969 Model\n",
      "Epoch 16 - Save Best Loss: 0.6969 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 11m 14s) Loss: 0.2960(0.2960) Grad: 76869.3594  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.3357(0.3060) Grad: 82889.1953  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.2586(0.3059) Grad: 69461.2188  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6029(0.6029) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.3059  avg_val_loss: 0.6977  time: 77s\n",
      "Epoch 17 - avg_train_Score: 0.3059 avgScore: 0.6977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7255(0.6977) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 10m 41s) Loss: 0.2329(0.2329) Grad: 75356.0703  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2740(0.2754) Grad: 73611.1328  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.2380(0.2753) Grad: 62922.4219  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6070(0.6070) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2753  avg_val_loss: 0.6958  time: 78s\n",
      "Epoch 18 - avg_train_Score: 0.2753 avgScore: 0.6958\n",
      "Epoch 18 - Save Best Score: 0.6958 Model\n",
      "Epoch 18 - Save Best Loss: 0.6958 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7306(0.6958) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 11m 22s) Loss: 0.2226(0.2226) Grad: 67998.5391  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2376(0.2526) Grad: 56342.7695  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2539(0.2528) Grad: 78541.6406  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.6060(0.6060) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2528  avg_val_loss: 0.6937  time: 78s\n",
      "Epoch 19 - avg_train_Score: 0.2528 avgScore: 0.6937\n",
      "Epoch 19 - Save Best Score: 0.6937 Model\n",
      "Epoch 19 - Save Best Loss: 0.6937 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7199(0.6937) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 10m 51s) Loss: 0.2218(0.2218) Grad: 96544.5547  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.2377(0.2346) Grad: 65475.6172  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2225(0.2343) Grad: 70039.1562  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6037(0.6037) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2343  avg_val_loss: 0.6949  time: 78s\n",
      "Epoch 20 - avg_train_Score: 0.2343 avgScore: 0.6949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7161(0.6949) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 3 result ==========\n",
      "score: 0.6949\n",
      "========== fold: 4 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 11m 22s) Loss: 5.0194(5.0194) Grad: 89192.4375  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.4088(2.2759) Grad: 299051.1250  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.1771(2.1982) Grad: 255671.9688  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 1.5896(1.5896) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.1982  avg_val_loss: 1.6927  time: 78s\n",
      "Epoch 1 - avg_train_Score: 2.1982 avgScore: 1.6927\n",
      "Epoch 1 - Save Best Score: 1.6927 Model\n",
      "Epoch 1 - Save Best Loss: 1.6927 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.7458(1.6927) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 54s) Loss: 1.2662(1.2662) Grad: 276338.9688  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.2742(1.1546) Grad: 299073.5000  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.1416(1.1497) Grad: 255686.3750  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 1.1432(1.1432) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.1497  avg_val_loss: 1.0337  time: 77s\n",
      "Epoch 2 - avg_train_Score: 1.1497 avgScore: 1.0337\n",
      "Epoch 2 - Save Best Score: 1.0337 Model\n",
      "Epoch 2 - Save Best Loss: 1.0337 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0991(1.0337) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 46s) Loss: 0.9429(0.9429) Grad: 221479.9375  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.4492(1.2942) Grad: 230534.3906  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 1.3084(1.2843) Grad: 224438.0156  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 1.1307(1.1307) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2843  avg_val_loss: 1.0329  time: 78s\n",
      "Epoch 3 - avg_train_Score: 1.2843 avgScore: 1.0329\n",
      "Epoch 3 - Save Best Score: 1.0329 Model\n",
      "Epoch 3 - Save Best Loss: 1.0329 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.1956(1.0329) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 10m 44s) Loss: 1.2381(1.2381) Grad: 204336.6406  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.0460(1.0694) Grad: 172953.5625  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.1061(1.0658) Grad: 168102.0781  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.9373(0.9373) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0658  avg_val_loss: 0.9215  time: 78s\n",
      "Epoch 4 - avg_train_Score: 1.0658 avgScore: 0.9215\n",
      "Epoch 4 - Save Best Score: 0.9215 Model\n",
      "Epoch 4 - Save Best Loss: 0.9215 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0784(0.9215) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 11m 34s) Loss: 1.0576(1.0576) Grad: 175305.0156  LR: 0.000958  \n",
      "Epoch: [5][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 1.0305(0.9569) Grad: 163258.4062  LR: 0.000958  \n",
      "Epoch: [5][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.9060(0.9576) Grad: 119342.1016  LR: 0.000958  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.8409(0.8409) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9576  avg_val_loss: 0.8605  time: 77s\n",
      "Epoch 5 - avg_train_Score: 0.9576 avgScore: 0.8605\n",
      "Epoch 5 - Save Best Score: 0.8605 Model\n",
      "Epoch 5 - Save Best Loss: 0.8605 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9537(0.8605) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 12m 27s) Loss: 0.9224(0.9224) Grad: 136059.6250  LR: 0.000916  \n",
      "Epoch: [6][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7797(0.8696) Grad: 122305.2812  LR: 0.000916  \n",
      "Epoch: [6][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7172(0.8664) Grad: 131898.3281  LR: 0.000916  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.7621(0.7621) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8664  avg_val_loss: 0.8028  time: 78s\n",
      "Epoch 6 - avg_train_Score: 0.8664 avgScore: 0.8028\n",
      "Epoch 6 - Save Best Score: 0.8028 Model\n",
      "Epoch 6 - Save Best Loss: 0.8028 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8796(0.8028) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 11m 14s) Loss: 0.9083(0.9083) Grad: 128560.9609  LR: 0.000865  \n",
      "Epoch: [7][500/542] Elapsed 1m 5s (remain 0m 5s) Loss: 0.9164(0.7787) Grad: 118571.1250  LR: 0.000865  \n",
      "Epoch: [7][541/542] Elapsed 1m 10s (remain 0m 0s) Loss: 0.8054(0.7784) Grad: 129166.4531  LR: 0.000865  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7012(0.7012) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7784  avg_val_loss: 0.7628  time: 80s\n",
      "Epoch 7 - avg_train_Score: 0.7784 avgScore: 0.7628\n",
      "Epoch 7 - Save Best Score: 0.7628 Model\n",
      "Epoch 7 - Save Best Loss: 0.7628 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8360(0.7628) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 0.6016(0.6016) Grad: 113237.5547  LR: 0.000805  \n",
      "Epoch: [8][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6499(0.7063) Grad: 78511.2734  LR: 0.000805  \n",
      "Epoch: [8][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7420(0.7050) Grad: 99481.2812  LR: 0.000805  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.6699(0.6699) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7050  avg_val_loss: 0.7314  time: 78s\n",
      "Epoch 8 - avg_train_Score: 0.7050 avgScore: 0.7314\n",
      "Epoch 8 - Save Best Score: 0.7314 Model\n",
      "Epoch 8 - Save Best Loss: 0.7314 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7927(0.7314) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 28s) Loss: 0.8306(0.8306) Grad: 115679.5781  LR: 0.000738  \n",
      "Epoch: [9][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8989(0.6455) Grad: 107567.5547  LR: 0.000738  \n",
      "Epoch: [9][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4993(0.6453) Grad: 89437.5078  LR: 0.000738  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.6615(0.6615) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6453  avg_val_loss: 0.7044  time: 78s\n",
      "Epoch 9 - avg_train_Score: 0.6453 avgScore: 0.7044\n",
      "Epoch 9 - Save Best Score: 0.7044 Model\n",
      "Epoch 9 - Save Best Loss: 0.7044 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7290(0.7044) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 3s) Loss: 0.5125(0.5125) Grad: 84379.3828  LR: 0.000666  \n",
      "Epoch: [10][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7029(0.5853) Grad: 85150.3125  LR: 0.000666  \n",
      "Epoch: [10][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5990(0.5836) Grad: 116297.1875  LR: 0.000666  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6318(0.6318) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.5836  avg_val_loss: 0.6959  time: 79s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7440(0.6959) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_Score: 0.5836 avgScore: 0.6959\n",
      "Epoch 10 - Save Best Score: 0.6959 Model\n",
      "Epoch 10 - Save Best Loss: 0.6959 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 11m 23s) Loss: 0.4721(0.4721) Grad: 74305.3828  LR: 0.000589  \n",
      "Epoch: [11][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6212(0.5294) Grad: 107160.4062  LR: 0.000589  \n",
      "Epoch: [11][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4891(0.5298) Grad: 76248.7422  LR: 0.000589  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6021(0.6021) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5298  avg_val_loss: 0.6860  time: 79s\n",
      "Epoch 11 - avg_train_Score: 0.5298 avgScore: 0.6860\n",
      "Epoch 11 - Save Best Score: 0.6860 Model\n",
      "Epoch 11 - Save Best Loss: 0.6860 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7230(0.6860) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 11m 1s) Loss: 0.4145(0.4145) Grad: 84038.8516  LR: 0.000511  \n",
      "Epoch: [12][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5045(0.4839) Grad: 90930.9688  LR: 0.000511  \n",
      "Epoch: [12][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4387(0.4833) Grad: 71712.3516  LR: 0.000511  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6014(0.6014) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.4833  avg_val_loss: 0.6806  time: 78s\n",
      "Epoch 12 - avg_train_Score: 0.4833 avgScore: 0.6806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7287(0.6806) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Save Best Score: 0.6806 Model\n",
      "Epoch 12 - Save Best Loss: 0.6806 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 10m 47s) Loss: 0.4055(0.4055) Grad: 90731.1797  LR: 0.000432  \n",
      "Epoch: [13][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4253(0.4390) Grad: 61310.6445  LR: 0.000432  \n",
      "Epoch: [13][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4066(0.4392) Grad: 87624.7656  LR: 0.000432  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.5758(0.5758) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7203(0.6745) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.4392  avg_val_loss: 0.6745  time: 78s\n",
      "Epoch 13 - avg_train_Score: 0.4392 avgScore: 0.6745\n",
      "Epoch 13 - Save Best Score: 0.6745 Model\n",
      "Epoch 13 - Save Best Loss: 0.6745 Model\n",
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 2s) Loss: 0.3595(0.3595) Grad: 109274.1953  LR: 0.000356  \n",
      "Epoch: [14][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3800(0.3971) Grad: 120426.7812  LR: 0.000356  \n",
      "Epoch: [14][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3796(0.3962) Grad: 66228.6328  LR: 0.000356  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.5689(0.5689) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.3962  avg_val_loss: 0.6701  time: 78s\n",
      "Epoch 14 - avg_train_Score: 0.3962 avgScore: 0.6701\n",
      "Epoch 14 - Save Best Score: 0.6701 Model\n",
      "Epoch 14 - Save Best Loss: 0.6701 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7472(0.6701) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 11m 13s) Loss: 0.3449(0.3449) Grad: 82363.8828  LR: 0.000283  \n",
      "Epoch: [15][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3642(0.3581) Grad: 82942.4141  LR: 0.000283  \n",
      "Epoch: [15][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3802(0.3574) Grad: 80416.9297  LR: 0.000283  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.5453(0.5453) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.3574  avg_val_loss: 0.6682  time: 78s\n",
      "Epoch 15 - avg_train_Score: 0.3574 avgScore: 0.6682\n",
      "Epoch 15 - Save Best Score: 0.6682 Model\n",
      "Epoch 15 - Save Best Loss: 0.6682 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7224(0.6682) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 11m 46s) Loss: 0.3853(0.3853) Grad: 92109.7422  LR: 0.000216  \n",
      "Epoch: [16][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2671(0.3238) Grad: 55728.7617  LR: 0.000216  \n",
      "Epoch: [16][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2504(0.3237) Grad: 68244.8750  LR: 0.000216  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.5574(0.5574) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.3237  avg_val_loss: 0.6674  time: 79s\n",
      "Epoch 16 - avg_train_Score: 0.3237 avgScore: 0.6674\n",
      "Epoch 16 - Save Best Score: 0.6674 Model\n",
      "Epoch 16 - Save Best Loss: 0.6674 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7288(0.6674) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 11m 10s) Loss: 0.3127(0.3127) Grad: 63778.8320  LR: 0.000156  \n",
      "Epoch: [17][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3123(0.2937) Grad: 59922.0547  LR: 0.000156  \n",
      "Epoch: [17][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3302(0.2937) Grad: 93067.9688  LR: 0.000156  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.5474(0.5474) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.2937  avg_val_loss: 0.6642  time: 78s\n",
      "Epoch 17 - avg_train_Score: 0.2937 avgScore: 0.6642\n",
      "Epoch 17 - Save Best Score: 0.6642 Model\n",
      "Epoch 17 - Save Best Loss: 0.6642 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7253(0.6642) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 12m 13s) Loss: 0.2878(0.2878) Grad: 62260.7070  LR: 0.000104  \n",
      "Epoch: [18][500/542] Elapsed 1m 2s (remain 0m 5s) Loss: 0.2542(0.2642) Grad: 69415.9766  LR: 0.000104  \n",
      "Epoch: [18][541/542] Elapsed 1m 7s (remain 0m 0s) Loss: 0.2433(0.2642) Grad: 74045.8984  LR: 0.000104  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.5558(0.5558) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.2642  avg_val_loss: 0.6647  time: 77s\n",
      "Epoch 18 - avg_train_Score: 0.2642 avgScore: 0.6647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7283(0.6647) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 11m 29s) Loss: 0.2473(0.2473) Grad: 64324.1758  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.2456(0.2436) Grad: 79774.3828  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2557(0.2437) Grad: 52377.5742  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.5588(0.5588) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.2437  avg_val_loss: 0.6633  time: 79s\n",
      "Epoch 19 - avg_train_Score: 0.2437 avgScore: 0.6633\n",
      "Epoch 19 - Save Best Score: 0.6633 Model\n",
      "Epoch 19 - Save Best Loss: 0.6633 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7260(0.6633) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_65734/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 44s) Loss: 0.2360(0.2360) Grad: 70792.1953  LR: 0.000031  \n",
      "Epoch: [20][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2061(0.2266) Grad: 62656.3984  LR: 0.000031  \n",
      "Epoch: [20][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2014(0.2263) Grad: 59692.6562  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.5577(0.5577) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.2263  avg_val_loss: 0.6629  time: 78s\n",
      "Epoch 20 - avg_train_Score: 0.2263 avgScore: 0.6629\n",
      "Epoch 20 - Save Best Score: 0.6629 Model\n",
      "Epoch 20 - Save Best Loss: 0.6629 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7275(0.6629) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 4 result ==========\n",
      "score: 0.6629\n",
      "========== CV ==========\n",
      "score: 0.6736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "            if CFG.objective_cv == 'binary':\n",
    "                outputs.append(torch.sigmoid(model(x)).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'multiclass':\n",
    "                outputs.append(torch.softmax(\n",
    "                    model(x), axis=1).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'regression':\n",
    "                outputs.append(model(x).to('cpu').numpy())\n",
    "\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "\n",
    "def test_fn(valid_loader, model, device):\n",
    "    preds = []\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        preds.append(y_preds)\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def inference():\n",
    "    test = pd.read_csv(CFG.comp_dataset_path +\n",
    "                       'test_features.csv')\n",
    "\n",
    "    test['base_path'] = CFG.comp_dataset_path + 'images/' + test['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in test['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    print(test.head(5))\n",
    "\n",
    "    valid_dataset = CustomDataset(\n",
    "        test, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = EnsembleModel()\n",
    "    folds = [0] if CFG.use_holdout else list(range(CFG.n_fold))\n",
    "    for fold in folds:\n",
    "        _model = CustomModel(CFG, pretrained=False)\n",
    "        _model.to(device)\n",
    "\n",
    "        model_path = CFG.model_dir + \\\n",
    "            f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth'\n",
    "        print('load', model_path)\n",
    "        state = torch.load(model_path)['model']\n",
    "        _model.load_state_dict(state)\n",
    "        _model.eval()\n",
    "\n",
    "        # _model = tta.ClassificationTTAWrapper(\n",
    "        #     _model, tta.aliases.five_crop_transform(256, 256))\n",
    "\n",
    "        model.add_model(_model)\n",
    "\n",
    "    preds = test_fn(valid_loader, model, device)\n",
    "\n",
    "    test[CFG.target_col] = preds\n",
    "    test.to_csv(CFG.submission_dir +\n",
    "                'submission_oof.csv', index=False)\n",
    "    test[CFG.target_col].to_csv(\n",
    "        CFG.submission_dir + f'submission_{CFG.exp_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-0.5.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15057ecbe4b54e579cf4d22ee63eebbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     ID      vEgo      aEgo  steeringAngleDeg  \\\n",
      "0  012baccc145d400c896cb82065a93d42_120  3.374273 -0.019360        -34.008415   \n",
      "1  012baccc145d400c896cb82065a93d42_220  2.441048 -0.022754        307.860077   \n",
      "2  012baccc145d400c896cb82065a93d42_320  3.604152 -0.286239         10.774388   \n",
      "3  012baccc145d400c896cb82065a93d42_420  2.048902 -0.537628         61.045235   \n",
      "4  01d738e799d260a10f6324f78023b38f_120  2.201528 -1.898600          5.740093   \n",
      "\n",
      "   steeringTorque  brake  brakePressed  gas  gasPressed gearShifter  \\\n",
      "0            17.0    0.0         False  0.0       False       drive   \n",
      "1           295.0    0.0          True  0.0       False       drive   \n",
      "2          -110.0    0.0          True  0.0       False       drive   \n",
      "3           189.0    0.0          True  0.0       False       drive   \n",
      "4           -41.0    0.0          True  0.0       False       drive   \n",
      "\n",
      "   leftBlinker  rightBlinker  \\\n",
      "0        False         False   \n",
      "1        False         False   \n",
      "2        False         False   \n",
      "3         True         False   \n",
      "4        False         False   \n",
      "\n",
      "                                           base_path  \n",
      "0  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "1  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "2  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "3  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "4  ../raw/atmacup_18_dataset/images/01d738e799d26...  \n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_resnet2/atmacup_18-models/resnet34d_fold0_last.pth\n",
      "pretrained: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65734/610043316.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path)['model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../proc/baseline/outputs/atmacup_18_resnet2/atmacup_18-models/resnet34d_fold1_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_resnet2/atmacup_18-models/resnet34d_fold2_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_resnet2/atmacup_18-models/resnet34d_fold3_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_resnet2/atmacup_18-models/resnet34d_fold4_last.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdb41e3a1124dcc9a100568bb1d7e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
