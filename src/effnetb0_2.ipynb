{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from timm.utils.model_ema import ModelEmaV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set dataset path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'atmacup_18'  # comp名\n",
    "\n",
    "    comp_dataset_path = '../raw/atmacup_18_dataset/'\n",
    "\n",
    "    exp_name = 'atmacup_18_cnn_effnetb0_2'\n",
    "\n",
    "    is_debug = False\n",
    "    use_gray_scale = False\n",
    "\n",
    "    model_in_chans = 9  # モデルの入力チャンネル数\n",
    "\n",
    "    # ============== file path =============\n",
    "    train_fold_dir = \"../proc/baseline/folds/\"\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = \"tf_efficientnet_b0_ns\"\n",
    "\n",
    "    num_frames = 3  # model_in_chansの倍数\n",
    "    norm_in_chans = 1 if use_gray_scale else 3\n",
    "\n",
    "    use_torch_compile = False\n",
    "    use_ema = True\n",
    "    ema_decay = 0.995\n",
    "    # ============== training cfg =============\n",
    "    size = 224  # 224\n",
    "\n",
    "    batch_size = 64  # 32\n",
    "\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 40\n",
    "    if is_debug:\n",
    "        epochs = 2\n",
    "\n",
    "    # adamW warmupあり\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-3\n",
    "    if scheduler == 'GradualWarmupSchedulerV2':\n",
    "        lr /= warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    n_fold = 5\n",
    "    use_holdout = False\n",
    "    use_alldata = False\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    skf_col = 'class'\n",
    "    group_col = 'scene'\n",
    "    fold_type = 'gkf'\n",
    "\n",
    "    objective_cv = 'regression'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'minimize'  # 'maximize', 'minimize'\n",
    "    metrics = 'calc_mae_atmacup'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 18\n",
    "    target_col = ['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1', 'x_2', 'y_2',\n",
    "                  'z_2', 'x_3', 'y_3', 'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5']\n",
    "\n",
    "\n",
    "    # ============== ほぼ固定 =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'last'  # 'best'\n",
    "\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== set dataset path =============\n",
    "    if exp_name is not None:\n",
    "        print('set dataset path')\n",
    "\n",
    "        outputs_path = f'../proc/baseline/outputs/{exp_name}/'\n",
    "\n",
    "        submission_dir = outputs_path + 'submissions/'\n",
    "        submission_path = submission_dir + f'submission_{exp_name}.csv'\n",
    "\n",
    "        model_dir = outputs_path + \\\n",
    "            f'{comp_name}-models/'\n",
    "\n",
    "        figures_dir = outputs_path + 'figures/'\n",
    "\n",
    "        log_dir = outputs_path + 'logs/'\n",
    "        log_path = log_dir + f'{exp_name}.txt'\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.ShiftScaleRotate(p=0.5),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        # A.CoarseDropout(max_holes=1, max_height=int(\n",
    "        #     size * 0.3), max_width=int(size * 0.3), p=0.5),\n",
    "\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA が利用可能か: True\n",
      "利用可能な CUDA デバイス数: 1\n",
      "現在の CUDA デバイス: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA が利用可能か:\", torch.cuda.is_available())\n",
    "print(\"利用可能な CUDA デバイス数:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"現在の CUDA デバイス:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "def get_fold(train, cfg):\n",
    "    if cfg.fold_type == 'kf':\n",
    "        Fold = KFold(n_splits=cfg.n_fold,\n",
    "                     shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.target_col])\n",
    "    elif cfg.fold_type == 'skf':\n",
    "        Fold = StratifiedKFold(n_splits=cfg.n_fold,\n",
    "                               shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.skf_col])\n",
    "    elif cfg.fold_type == 'gkf':\n",
    "        Fold = GroupKFold(n_splits=cfg.n_fold)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.group_col], groups)\n",
    "    elif cfg.fold_type == 'sgkf':\n",
    "        Fold = StratifiedGroupKFold(n_splits=cfg.n_fold,\n",
    "                                    shuffle=True, random_state=cfg.seed)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.skf_col], groups)\n",
    "    # elif fold_type == 'mskf':\n",
    "    #     Fold = MultilabelStratifiedKFold(\n",
    "    #         n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    #     kf = Fold.split(train, train[cfg.skf_col])\n",
    "\n",
    "    for n, (train_index, val_index) in enumerate(kf):\n",
    "        train.loc[val_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "    print(train.groupby('fold').size())\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_folds():\n",
    "    train_df = pd.read_csv(CFG.comp_dataset_path + 'train_features.csv')\n",
    "\n",
    "    train_df['scene'] = train_df['ID'].str.split('_').str[0]\n",
    "\n",
    "    print('group', CFG.group_col)\n",
    "    print(f'train len: {len(train_df)}')\n",
    "\n",
    "    train_df = get_fold(train_df, CFG)\n",
    "\n",
    "    # print(train_df.groupby(['fold', CFG.target_col]).size())\n",
    "    print(train_df['fold'].value_counts())\n",
    "\n",
    "    os.makedirs(CFG.train_fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(CFG.train_fold_dir +\n",
    "                    'train_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group scene\n",
      "train len: 43371\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "dtype: int64\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "make_train_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数固定\n",
    "def set_seed(seed=None, cudnn_deterministic=True):\n",
    "    if seed is None:\n",
    "        seed = 42\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def make_dirs(cfg):\n",
    "    for dir in [cfg.model_dir, cfg.figures_dir, cfg.submission_dir, cfg.log_dir]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def cfg_init(cfg, mode='train'):\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if mode == 'train':\n",
    "        make_dirs(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_init(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common_utils.logger import init_logger, wandb_init, AverageMeter, timeSince\n",
    "# from common_utils.settings import cfg_init\n",
    "\n",
    "def init_logger(log_file):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- exp_info -----------------\n",
      "2024年11月23日 15:43:41\n"
     ]
    }
   ],
   "source": [
    "Logger = init_logger(log_file=CFG.log_path)\n",
    "\n",
    "Logger.info('\\n\\n-------- exp_info -----------------')\n",
    "Logger.info(datetime.datetime.now().strftime('%Y年%m月%d日 %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    # return roc_auc_score(y_true, y_pred)\n",
    "    eval_func = eval(CFG.metrics)\n",
    "    return eval_func(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calc_mae_atmacup(y_true, y_pred):\n",
    "    abs_diff = np.abs(y_true - y_pred)  # 各予測の差分の絶対値を計算して\n",
    "    mae = np.mean(abs_diff.reshape(-1, ))  # 予測の差分の絶対値の平均を計算\n",
    "\n",
    "    return mae\n",
    "\n",
    "def get_result(result_df):\n",
    "\n",
    "    # preds = result_df['preds'].values\n",
    "\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "    preds = result_df[pred_cols].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    Logger.info(f'score: {score:<.4f}')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_traffic_light(image, id):\n",
    "    path = f'./datasets/atmacup_18/traffic_lights/{id}.json'\n",
    "    traffic_lights = json.load(open(path))\n",
    "\n",
    "    traffic_class = ['green',\n",
    "                     'straight', 'left', 'right', 'empty', 'other', 'yellow', 'red']\n",
    "    class_to_idx = {\n",
    "        cls: idx for idx, cls in enumerate(traffic_class)\n",
    "    }\n",
    "\n",
    "    for traffic_light in traffic_lights:\n",
    "        bbox = traffic_light['bbox']\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # int\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        point1 = (x1, y1)\n",
    "        point2 = (x2, y2)\n",
    "\n",
    "        idx = class_to_idx[traffic_light['class']]\n",
    "        color = 255 - int(255*(idx/len(traffic_class)))\n",
    "\n",
    "        cv2.rectangle(image, point1, point2, color=color, thickness=1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_image_for_cache(path):\n",
    "    if CFG.use_gray_scale:\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # image = cv2.resize(image, (CFG.size, CFG.size))\n",
    "\n",
    "    # 効かない\n",
    "    # image = draw_traffic_light(image, path.split('/')[-2])\n",
    "    return (path, image)\n",
    "\n",
    "\n",
    "def make_video_cache(paths):\n",
    "    debug = []\n",
    "    for idx in range(9):\n",
    "        color = 255 - int(255*(idx/9))\n",
    "        debug.append(color)\n",
    "    print(debug)\n",
    "\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        res = pool.imap_unordered(read_image_for_cache, paths)\n",
    "        res = tqdm(res)\n",
    "        res = list(res)\n",
    "\n",
    "    return dict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import ReplayCompose\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        # aug = A.Compose(cfg.train_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        # aug = A.Compose(cfg.valid_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, cfg, labels=None, transform=None):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.base_paths = df['base_path'].values\n",
    "        # self.labels = df[self.cfg.target_col].values\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def read_image_multiframe(self, idx):\n",
    "        base_path = self.base_paths[idx]\n",
    "\n",
    "        images = []\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "\n",
    "            image = self.cfg.video_cache[path]\n",
    "\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.read_image_multiframe(idx)\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image=image)['image']\n",
    "            replay = None\n",
    "            images = []\n",
    "            for img in image:\n",
    "                if replay is None:\n",
    "                    sample = self.transform(image=img)\n",
    "                    replay = sample['replay']\n",
    "                else:\n",
    "                    sample = ReplayCompose.replay(replay, image=img)\n",
    "                images.append(sample['image'])\n",
    "\n",
    "            image = torch.concat(images, dim=0)\n",
    "\n",
    "        if self.labels is None:\n",
    "            return image\n",
    "\n",
    "        if self.cfg.objective_cv == 'multiclass':\n",
    "            label = torch.tensor(self.labels[idx]).long()\n",
    "        else:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aug_video(train, cfg, plot_count=1):\n",
    "    transform = CFG.train_aug_list\n",
    "    transform = A.ReplayCompose(transform)\n",
    "\n",
    "    dataset = CustomDataset(\n",
    "        train, CFG, transform=transform)\n",
    "\n",
    "    for i in range(plot_count):\n",
    "        image = dataset.read_image_multiframe(i)\n",
    "\n",
    "        if cfg.use_gray_scale:\n",
    "            image = np.stack(image, axis=2)\n",
    "        else:\n",
    "            image = np.concatenate(image, axis=2)\n",
    "\n",
    "        aug_image = dataset[i]\n",
    "        # torch to numpy\n",
    "        aug_image = aug_image.permute(1, 2, 0).numpy()*255\n",
    "\n",
    "        for frame in range(image.shape[-1]):\n",
    "            if frame % 3 != 0:\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            if cfg.use_gray_scale:\n",
    "                axes[0].imshow(image[..., frame], cmap=\"gray\")\n",
    "                axes[1].imshow(aug_image[..., frame], cmap=\"gray\")\n",
    "            else:\n",
    "                axes[0].imshow(image[..., frame:frame+3].astype(int))\n",
    "                axes[1].imshow(aug_image[..., frame:frame+3].astype(int))\n",
    "            plt.savefig(cfg.figures_dir +\n",
    "                        f'aug_{i}_frame{frame}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "        # self.cfg = cfg\n",
    "\n",
    "        if model_name is None:\n",
    "            model_name = cfg.model_name\n",
    "\n",
    "        print(f'pretrained: {pretrained}')\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=0,\n",
    "            in_chans=cfg.model_in_chans)\n",
    "\n",
    "        self.n_features = self.model.num_features\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "        self.pooling = GeM()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(self.n_features, self.target_size)\n",
    "\n",
    "        # # nn.Dropout(0.5),\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(self.n_features, self.target_size)\n",
    "        # )\n",
    "\n",
    "    def forward(self, image):\n",
    "        # 特徴量抽出\n",
    "        features = self.model(image)  # [batch_size, channels, height, width]\n",
    "        \n",
    "        # 入力が4次元でない場合は4次元に変換\n",
    "        if features.dim() != 4:\n",
    "            features = features.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # GeM プーリング (4D入力を想定)\n",
    "        pooled_features = self.pooling(features)  # [batch_size, channels, 1, 1]\n",
    "        \n",
    "        # 平坦化\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)  # [batch_size, channels]\n",
    "        \n",
    "        # ドロップアウトと全結合層\n",
    "        pooled_features = self.dropout(pooled_features)\n",
    "        output = self.linear(pooled_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(\n",
    "            optimizer, multiplier, total_epoch, after_scheduler)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [\n",
    "                        base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "def get_scheduler(cfg, optimizer):\n",
    "    if cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=cfg.factor, patience=cfg.patience, verbose=True, eps=cfg.eps)\n",
    "    elif cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer, T_max=cfg.epochs, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=cfg.T_0, T_mult=1, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'GradualWarmupSchedulerV2':\n",
    "        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, cfg.epochs, eta_min=1e-7)\n",
    "        scheduler = GradualWarmupSchedulerV2(\n",
    "            optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def scheduler_step(scheduler, avg_val_loss, epoch):\n",
    "    if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        scheduler.step(avg_val_loss)\n",
    "    elif isinstance(scheduler, CosineAnnealingLR):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, GradualWarmupSchedulerV2):\n",
    "        scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device,\n",
    "             model_ema=None):\n",
    "    \"\"\" 1epoch毎のtrain \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.use_amp)\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    preds_labels = []\n",
    "    start = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with autocast(CFG.use_amp):\n",
    "            y_preds = model(images)\n",
    "\n",
    "            if y_preds.size(1) == 1:\n",
    "                y_preds = y_preds.view(-1)\n",
    "\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.detach().to('cpu').numpy())\n",
    "\n",
    "        preds_labels.append(labels.detach().to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(preds_labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        if y_preds.size(1) == 1:\n",
    "            y_preds = y_preds.view(-1)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # binary\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(folds, fold):\n",
    "\n",
    "    Logger.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    if CFG.use_alldata:\n",
    "        train_folds = folds.copy().reset_index(drop=True)\n",
    "    else:\n",
    "        train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # train_folds = train_downsampling(train_folds)\n",
    "\n",
    "    train_labels = train_folds[CFG.target_col].values\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        train_folds, CFG, labels=train_labels, transform=get_transforms(data='train', cfg=CFG))\n",
    "    valid_dataset = CustomDataset(\n",
    "        valid_folds, CFG, labels=valid_labels, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                              )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "\n",
    "    model = CustomModel(CFG, pretrained=CFG.pretrained)\n",
    "    model.to(device)\n",
    "\n",
    "    if CFG.use_ema:\n",
    "        model_ema = ModelEmaV2(model, decay=CFG.ema_decay)\n",
    "    else:\n",
    "        model_ema = None\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr)\n",
    "    scheduler = get_scheduler(CFG, optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    if CFG.objective_cv == 'binary':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif CFG.objective_cv == 'multiclass':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif CFG.objective_cv == 'regression':\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "    if CFG.metric_direction == 'minimize':\n",
    "        best_score = np.inf\n",
    "    elif CFG.metric_direction == 'maximize':\n",
    "        best_score = -1\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    df_score = pd.DataFrame(columns=[\"train_loss\", 'train_score', 'val_loss', 'val_score'])\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss, train_preds, train_labels_epoch = train_fn(fold, train_loader, model,\n",
    "                                                             criterion, optimizer, epoch, scheduler, device, model_ema)\n",
    "        train_score = get_score(train_labels_epoch, train_preds)\n",
    "\n",
    "        # eval\n",
    "        if model_ema is not None:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model_ema.module, criterion, device)\n",
    "        else:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model, criterion, device)\n",
    "\n",
    "        scheduler_step(scheduler, avg_val_loss, epoch)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, valid_preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        # Logger.info(f'Epoch {epoch+1} - avgScore: {avg_score:.4f}')\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_Score: {train_score:.4f} avgScore: {score:.4f}')\n",
    "        \n",
    "        df_score.loc[epoch] = [avg_loss, train_score, avg_val_loss, score]\n",
    "\n",
    "        if CFG.metric_direction == 'minimize':\n",
    "            update_best = score < best_score\n",
    "        elif CFG.metric_direction == 'maximize':\n",
    "            update_best = score > best_score\n",
    "\n",
    "        if update_best:\n",
    "            best_loss = avg_val_loss\n",
    "            best_score = score\n",
    "\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "\n",
    "            if model_ema is not None:\n",
    "                torch.save({'model': model_ema.module.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "            else:\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save({'model': model.state_dict(),\n",
    "                'preds': valid_preds},\n",
    "               CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    \"\"\"\n",
    "    if model_ema is not None:\n",
    "        torch.save({'model': model_ema.module.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    else:\n",
    "        torch.save({'model': model.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "\n",
    "    check_point = torch.load(\n",
    "        CFG.model_dir + f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth', map_location=torch.device('cpu'))\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "\n",
    "    check_point_pred = check_point['preds']\n",
    "\n",
    "    # Columns must be same length as key 対策\n",
    "    if check_point_pred.ndim == 1:\n",
    "        check_point_pred = check_point_pred.reshape(-1, CFG.target_size)\n",
    "\n",
    "    print('check_point_pred shape', check_point_pred.shape)\n",
    "    valid_folds[pred_cols] = check_point_pred\n",
    "    return valid_folds, df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = pd.read_csv(CFG.train_fold_dir + 'train_folds.csv')\n",
    "    train['ori_idx'] = train.index\n",
    "\n",
    "    train['scene'] = train['ID'].str.split('_').str[0]\n",
    "\n",
    "    \"\"\"\n",
    "    if CFG.is_debug:\n",
    "        use_ids = train['scene'].unique()[:100]\n",
    "        train = train[train['scene'].isin(use_ids)].reset_index(drop=True)\n",
    "    \"\"\"\n",
    "\n",
    "    train['base_path'] = CFG.comp_dataset_path + 'images/' + train['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in train['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    # plot_aug_video(train, CFG, plot_count=10)\n",
    "\n",
    "    # train\n",
    "    oof_df = pd.DataFrame()\n",
    "    list_df_score = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold not in CFG.train_folds:\n",
    "            print(f'fold {fold} is skipped')\n",
    "            continue\n",
    "\n",
    "        _oof_df, _df_score = train_fold(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        list_df_score.append(_df_score)\n",
    "        Logger.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "\n",
    "        if CFG.use_holdout or CFG.use_alldata:\n",
    "            break\n",
    "\n",
    "    oof_df = oof_df.sort_values('ori_idx').reset_index(drop=True)\n",
    "\n",
    "    # CV result\n",
    "    Logger.info(\"========== CV ==========\")\n",
    "    score = get_result(oof_df)\n",
    "\n",
    "    # 学習曲線を可視化する\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    for df_score in list_df_score:\n",
    "        ax1.plot(df_score['val_loss'])\n",
    "        ax2.plot(df_score['val_score'])\n",
    "    ax1.set_title('Validation Loss')\n",
    "    ax2.set_title('Validation Score') \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax1.set_ylim([0, 1.5])\n",
    "    ax2.set_ylim([0, 1.5])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CFG.figures_dir + f'learning_curve_{CFG.exp_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # save result\n",
    "    oof_df.to_csv(CFG.submission_dir + 'oof_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-0.5.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb26ab1da3954ba08cb61bda5d3d38d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 2s (remain 20m 47s) Loss: 4.6440(4.6440) Grad: 36808.4414  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.4431(2.6139) Grad: 50074.9648  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 1.3171(2.5218) Grad: 50151.9766  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 55s) Loss: 1.6953(1.6953) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.5218  avg_val_loss: 1.9804  time: 65s\n",
      "Epoch 1 - avg_train_Score: 2.5218 avgScore: 1.9804\n",
      "Epoch 1 - Save Best Score: 1.9804 Model\n",
      "Epoch 1 - Save Best Loss: 1.9804 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.8032(1.9804) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 2s) Loss: 1.1678(1.1678) Grad: 175606.4688  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.2258(1.2180) Grad: 204455.8906  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.1074(1.2181) Grad: 286562.8125  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 1.0278(1.0278) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.2181  avg_val_loss: 1.1286  time: 64s\n",
      "Epoch 2 - avg_train_Score: 1.2181 avgScore: 1.1286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.1026(1.1286) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Save Best Score: 1.1286 Model\n",
      "Epoch 2 - Save Best Loss: 1.1286 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 17s) Loss: 1.1131(1.1131) Grad: 227516.2656  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.3454(1.2622) Grad: 77600.2109  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.3570(1.2624) Grad: 61474.7383  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 1.0146(1.0146) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2624  avg_val_loss: 1.0402  time: 64s\n",
      "Epoch 3 - avg_train_Score: 1.2624 avgScore: 1.0402\n",
      "Epoch 3 - Save Best Score: 1.0402 Model\n",
      "Epoch 3 - Save Best Loss: 1.0402 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0566(1.0402) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 9m 59s) Loss: 1.0287(1.0287) Grad: 149230.7969  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9019(1.0442) Grad: 100882.2422  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9857(1.0411) Grad: 132991.1406  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.8814(0.8814) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0411  avg_val_loss: 0.9051  time: 63s\n",
      "Epoch 4 - avg_train_Score: 1.0411 avgScore: 0.9051\n",
      "Epoch 4 - Save Best Score: 0.9051 Model\n",
      "Epoch 4 - Save Best Loss: 0.9051 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8810(0.9051) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 10m 43s) Loss: 0.8973(0.8973) Grad: 114373.8984  LR: 0.000989  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9500(0.9478) Grad: 59806.6328  LR: 0.000989  \n",
      "Epoch: [5][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8534(0.9517) Grad: 51541.8086  LR: 0.000989  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.8992(0.8992) \n",
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8938(0.8768) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9517  avg_val_loss: 0.8768  time: 64s\n",
      "Epoch 5 - avg_train_Score: 0.9517 avgScore: 0.8768\n",
      "Epoch 5 - Save Best Score: 0.8768 Model\n",
      "Epoch 5 - Save Best Loss: 0.8768 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 10m 21s) Loss: 1.0000(1.0000) Grad: 105455.8203  LR: 0.000979  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8567(0.9195) Grad: 42370.6250  LR: 0.000979  \n",
      "Epoch: [6][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8212(0.9191) Grad: 44358.0469  LR: 0.000979  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.8003(0.8003) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.9191  avg_val_loss: 0.8299  time: 64s\n",
      "Epoch 6 - avg_train_Score: 0.9191 avgScore: 0.8299\n",
      "Epoch 6 - Save Best Score: 0.8299 Model\n",
      "Epoch 6 - Save Best Loss: 0.8299 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8115(0.8299) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 10m 11s) Loss: 0.5960(0.5960) Grad: 81369.9531  LR: 0.000965  \n",
      "Epoch: [7][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.7711(0.7488) Grad: 105568.1562  LR: 0.000965  \n",
      "Epoch: [7][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5198(0.7500) Grad: 89017.3828  LR: 0.000965  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.7607(0.7607) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7500  avg_val_loss: 0.7870  time: 63s\n",
      "Epoch 7 - avg_train_Score: 0.7500 avgScore: 0.7870\n",
      "Epoch 7 - Save Best Score: 0.7870 Model\n",
      "Epoch 7 - Save Best Loss: 0.7870 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7842(0.7870) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 9m 54s) Loss: 0.7746(0.7746) Grad: 85745.6406  LR: 0.000949  \n",
      "Epoch: [8][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.6663(0.6870) Grad: 87906.3281  LR: 0.000949  \n",
      "Epoch: [8][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.7735(0.6904) Grad: 119526.2344  LR: 0.000949  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7306(0.7306) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7773(0.7768) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.6904  avg_val_loss: 0.7768  time: 65s\n",
      "Epoch 8 - avg_train_Score: 0.6904 avgScore: 0.7768\n",
      "Epoch 8 - Save Best Score: 0.7768 Model\n",
      "Epoch 8 - Save Best Loss: 0.7768 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 10m 14s) Loss: 0.7243(0.7243) Grad: 109128.5781  LR: 0.000929  \n",
      "Epoch: [9][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6568(0.6510) Grad: 119510.2031  LR: 0.000929  \n",
      "Epoch: [9][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.7381(0.6527) Grad: 83153.4531  LR: 0.000929  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7548(0.7548) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6527  avg_val_loss: 0.7664  time: 64s\n",
      "Epoch 9 - avg_train_Score: 0.6527 avgScore: 0.7664\n",
      "Epoch 9 - Save Best Score: 0.7664 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7295(0.7664) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Save Best Loss: 0.7664 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 10m 31s) Loss: 0.7465(0.7465) Grad: 124026.6328  LR: 0.000908  \n",
      "Epoch: [10][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6392(0.6211) Grad: 111800.0781  LR: 0.000908  \n",
      "Epoch: [10][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5369(0.6217) Grad: 86383.1797  LR: 0.000908  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7387(0.7387) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6217  avg_val_loss: 0.7595  time: 64s\n",
      "Epoch 10 - avg_train_Score: 0.6217 avgScore: 0.7595\n",
      "Epoch 10 - Save Best Score: 0.7595 Model\n",
      "Epoch 10 - Save Best Loss: 0.7595 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7057(0.7595) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 0.5302(0.5302) Grad: 80515.7422  LR: 0.000883  \n",
      "Epoch: [11][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.5350(0.5955) Grad: 74191.6953  LR: 0.000883  \n",
      "Epoch: [11][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.5908(0.5960) Grad: 90468.4922  LR: 0.000883  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7346(0.7346) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.5960  avg_val_loss: 0.7509  time: 65s\n",
      "Epoch 11 - avg_train_Score: 0.5960 avgScore: 0.7509\n",
      "Epoch 11 - Save Best Score: 0.7509 Model\n",
      "Epoch 11 - Save Best Loss: 0.7509 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7289(0.7509) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 11m 16s) Loss: 0.5774(0.5774) Grad: 95589.2422  LR: 0.000857  \n",
      "Epoch: [12][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.6651(0.5735) Grad: 93225.6797  LR: 0.000857  \n",
      "Epoch: [12][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.6024(0.5746) Grad: 91200.3750  LR: 0.000857  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7434(0.7434) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7245(0.7447) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.5746  avg_val_loss: 0.7447  time: 65s\n",
      "Epoch 12 - avg_train_Score: 0.5746 avgScore: 0.7447\n",
      "Epoch 12 - Save Best Score: 0.7447 Model\n",
      "Epoch 12 - Save Best Loss: 0.7447 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 10m 3s) Loss: 0.5314(0.5314) Grad: 84852.7969  LR: 0.000828  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5132(0.5452) Grad: 80597.9688  LR: 0.000828  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5792(0.5465) Grad: 89625.9766  LR: 0.000828  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7258(0.7258) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.5465  avg_val_loss: 0.7384  time: 64s\n",
      "Epoch 13 - avg_train_Score: 0.5465 avgScore: 0.7384\n",
      "Epoch 13 - Save Best Score: 0.7384 Model\n",
      "Epoch 13 - Save Best Loss: 0.7384 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7012(0.7384) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 10m 21s) Loss: 0.5288(0.5288) Grad: 88826.8984  LR: 0.000797  \n",
      "Epoch: [14][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4558(0.5172) Grad: 85117.0938  LR: 0.000797  \n",
      "Epoch: [14][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5279(0.5193) Grad: 78223.4766  LR: 0.000797  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7387(0.7387) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5193  avg_val_loss: 0.7374  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.5193 avgScore: 0.7374\n",
      "Epoch 14 - Save Best Score: 0.7374 Model\n",
      "Epoch 14 - Save Best Loss: 0.7374 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6962(0.7374) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.6452(0.6452) Grad: 96460.8125  LR: 0.000764  \n",
      "Epoch: [15][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4405(0.4962) Grad: 76807.2734  LR: 0.000764  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4845(0.4968) Grad: 94676.7266  LR: 0.000764  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7334(0.7334) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.4968  avg_val_loss: 0.7340  time: 65s\n",
      "Epoch 15 - avg_train_Score: 0.4968 avgScore: 0.7340\n",
      "Epoch 15 - Save Best Score: 0.7340 Model\n",
      "Epoch 15 - Save Best Loss: 0.7340 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6868(0.7340) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 10m 17s) Loss: 0.5005(0.5005) Grad: 78732.4141  LR: 0.000730  \n",
      "Epoch: [16][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.5897(0.4735) Grad: 81357.7891  LR: 0.000730  \n",
      "Epoch: [16][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.4682(0.4735) Grad: 84494.9141  LR: 0.000730  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7314(0.7314) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4735  avg_val_loss: 0.7315  time: 65s\n",
      "Epoch 16 - avg_train_Score: 0.4735 avgScore: 0.7315\n",
      "Epoch 16 - Save Best Score: 0.7315 Model\n",
      "Epoch 16 - Save Best Loss: 0.7315 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7256(0.7315) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 36s) Loss: 0.4522(0.4522) Grad: 98364.6250  LR: 0.000694  \n",
      "Epoch: [17][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4371(0.4532) Grad: 90121.7109  LR: 0.000694  \n",
      "Epoch: [17][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5375(0.4540) Grad: 86904.4297  LR: 0.000694  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7203(0.7203) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7119(0.7309) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4540  avg_val_loss: 0.7309  time: 64s\n",
      "Epoch 17 - avg_train_Score: 0.4540 avgScore: 0.7309\n",
      "Epoch 17 - Save Best Score: 0.7309 Model\n",
      "Epoch 17 - Save Best Loss: 0.7309 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 13s) Loss: 0.5273(0.5273) Grad: 94393.5234  LR: 0.000657  \n",
      "Epoch: [18][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3788(0.4315) Grad: 66616.3438  LR: 0.000657  \n",
      "Epoch: [18][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4130(0.4329) Grad: 88995.6797  LR: 0.000657  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7326(0.7326) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4329  avg_val_loss: 0.7300  time: 65s\n",
      "Epoch 18 - avg_train_Score: 0.4329 avgScore: 0.7300\n",
      "Epoch 18 - Save Best Score: 0.7300 Model\n",
      "Epoch 18 - Save Best Loss: 0.7300 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7183(0.7300) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.4490(0.4490) Grad: 72375.6484  LR: 0.000620  \n",
      "Epoch: [19][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4125(0.4141) Grad: 73464.8281  LR: 0.000620  \n",
      "Epoch: [19][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3959(0.4141) Grad: 84283.2734  LR: 0.000620  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7042(0.7042) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7235(0.7257) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4141  avg_val_loss: 0.7257  time: 63s\n",
      "Epoch 19 - avg_train_Score: 0.4141 avgScore: 0.7257\n",
      "Epoch 19 - Save Best Score: 0.7257 Model\n",
      "Epoch 19 - Save Best Loss: 0.7257 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 10m 37s) Loss: 0.3042(0.3042) Grad: 71378.5156  LR: 0.000581  \n",
      "Epoch: [20][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.3596(0.3993) Grad: 63292.3242  LR: 0.000581  \n",
      "Epoch: [20][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.4135(0.3996) Grad: 77182.3438  LR: 0.000581  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.6802(0.6802) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.3996  avg_val_loss: 0.7241  time: 65s\n",
      "Epoch 20 - avg_train_Score: 0.3996 avgScore: 0.7241\n",
      "Epoch 20 - Save Best Score: 0.7241 Model\n",
      "Epoch 20 - Save Best Loss: 0.7241 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7091(0.7241) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 10m 14s) Loss: 0.3488(0.3488) Grad: 71413.1016  LR: 0.000542  \n",
      "Epoch: [21][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3812(0.3823) Grad: 65786.5391  LR: 0.000542  \n",
      "Epoch: [21][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3359(0.3812) Grad: 66636.2031  LR: 0.000542  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6794(0.6794) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7041(0.7230) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3812  avg_val_loss: 0.7230  time: 64s\n",
      "Epoch 21 - avg_train_Score: 0.3812 avgScore: 0.7230\n",
      "Epoch 21 - Save Best Score: 0.7230 Model\n",
      "Epoch 21 - Save Best Loss: 0.7230 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 10m 31s) Loss: 0.3804(0.3804) Grad: 66359.7344  LR: 0.000503  \n",
      "Epoch: [22][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3747(0.3661) Grad: 68550.9766  LR: 0.000503  \n",
      "Epoch: [22][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3331(0.3657) Grad: 68286.1016  LR: 0.000503  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.6801(0.6801) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3657  avg_val_loss: 0.7219  time: 63s\n",
      "Epoch 22 - avg_train_Score: 0.3657 avgScore: 0.7219\n",
      "Epoch 22 - Save Best Score: 0.7219 Model\n",
      "Epoch 22 - Save Best Loss: 0.7219 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7044(0.7219) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 10m 33s) Loss: 0.3410(0.3410) Grad: 71730.0625  LR: 0.000464  \n",
      "Epoch: [23][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.3457(0.3510) Grad: 67754.3906  LR: 0.000464  \n",
      "Epoch: [23][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.3137(0.3506) Grad: 63338.2500  LR: 0.000464  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6796(0.6796) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3506  avg_val_loss: 0.7193  time: 65s\n",
      "Epoch 23 - avg_train_Score: 0.3506 avgScore: 0.7193\n",
      "Epoch 23 - Save Best Score: 0.7193 Model\n",
      "Epoch 23 - Save Best Loss: 0.7193 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7171(0.7193) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 15s) Loss: 0.3426(0.3426) Grad: 75375.7422  LR: 0.000425  \n",
      "Epoch: [24][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2927(0.3362) Grad: 61326.9141  LR: 0.000425  \n",
      "Epoch: [24][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.3498(0.3357) Grad: 71010.5234  LR: 0.000425  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.6866(0.6866) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3357  avg_val_loss: 0.7167  time: 65s\n",
      "Epoch 24 - avg_train_Score: 0.3357 avgScore: 0.7167\n",
      "Epoch 24 - Save Best Score: 0.7167 Model\n",
      "Epoch 24 - Save Best Loss: 0.7167 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7021(0.7167) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 10m 23s) Loss: 0.3616(0.3616) Grad: 63545.4844  LR: 0.000386  \n",
      "Epoch: [25][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.3578(0.3218) Grad: 84876.4609  LR: 0.000386  \n",
      "Epoch: [25][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.3063(0.3220) Grad: 62018.9375  LR: 0.000386  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6891(0.6891) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7040(0.7184) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3220  avg_val_loss: 0.7184  time: 65s\n",
      "Epoch 25 - avg_train_Score: 0.3220 avgScore: 0.7184\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 10m 1s) Loss: 0.3059(0.3059) Grad: 58682.4023  LR: 0.000348  \n",
      "Epoch: [26][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3134(0.3072) Grad: 64053.7148  LR: 0.000348  \n",
      "Epoch: [26][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3485(0.3074) Grad: 62382.6680  LR: 0.000348  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.7006(0.7006) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3074  avg_val_loss: 0.7195  time: 65s\n",
      "Epoch 26 - avg_train_Score: 0.3074 avgScore: 0.7195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6944(0.7195) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 50s) Loss: 0.3195(0.3195) Grad: 69334.1328  LR: 0.000311  \n",
      "Epoch: [27][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2905(0.2958) Grad: 56123.2266  LR: 0.000311  \n",
      "Epoch: [27][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2931(0.2962) Grad: 64596.3281  LR: 0.000311  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6927(0.6927) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.2962  avg_val_loss: 0.7174  time: 65s\n",
      "Epoch 27 - avg_train_Score: 0.2962 avgScore: 0.7174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6925(0.7174) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 10m 33s) Loss: 0.2730(0.2730) Grad: 58345.5977  LR: 0.000276  \n",
      "Epoch: [28][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2801(0.2831) Grad: 62533.7578  LR: 0.000276  \n",
      "Epoch: [28][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3047(0.2836) Grad: 73388.1094  LR: 0.000276  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6910(0.6910) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7000(0.7171) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.2836  avg_val_loss: 0.7171  time: 64s\n",
      "Epoch 28 - avg_train_Score: 0.2836 avgScore: 0.7171\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 0.3002(0.3002) Grad: 60790.1055  LR: 0.000242  \n",
      "Epoch: [29][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2720(0.2730) Grad: 55000.9219  LR: 0.000242  \n",
      "Epoch: [29][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3104(0.2735) Grad: 58912.8242  LR: 0.000242  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6894(0.6894) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2735  avg_val_loss: 0.7170  time: 64s\n",
      "Epoch 29 - avg_train_Score: 0.2735 avgScore: 0.7170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6958(0.7170) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.3430(0.3430) Grad: 69942.9453  LR: 0.000209  \n",
      "Epoch: [30][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2680(0.2648) Grad: 60700.7930  LR: 0.000209  \n",
      "Epoch: [30][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2550(0.2641) Grad: 68083.0156  LR: 0.000209  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.6966(0.6966) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2641  avg_val_loss: 0.7179  time: 64s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6962(0.7179) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 - avg_train_Score: 0.2641 avgScore: 0.7179\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 10m 37s) Loss: 0.2281(0.2281) Grad: 54999.8633  LR: 0.000178  \n",
      "Epoch: [31][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2718(0.2550) Grad: 72356.7344  LR: 0.000178  \n",
      "Epoch: [31][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2650(0.2553) Grad: 55181.7773  LR: 0.000178  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6982(0.6982) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7093(0.7172) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2553  avg_val_loss: 0.7172  time: 65s\n",
      "Epoch 31 - avg_train_Score: 0.2553 avgScore: 0.7172\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 10m 21s) Loss: 0.2419(0.2419) Grad: 70506.1406  LR: 0.000149  \n",
      "Epoch: [32][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.1882(0.2446) Grad: 58448.4766  LR: 0.000149  \n",
      "Epoch: [32][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2356(0.2443) Grad: 60573.1055  LR: 0.000149  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6926(0.6926) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2443  avg_val_loss: 0.7145  time: 65s\n",
      "Epoch 32 - avg_train_Score: 0.2443 avgScore: 0.7145\n",
      "Epoch 32 - Save Best Score: 0.7145 Model\n",
      "Epoch 32 - Save Best Loss: 0.7145 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6964(0.7145) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.2811(0.2811) Grad: 61679.8203  LR: 0.000122  \n",
      "Epoch: [33][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2828(0.2355) Grad: 65720.0156  LR: 0.000122  \n",
      "Epoch: [33][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2425(0.2356) Grad: 58293.7461  LR: 0.000122  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6810(0.6810) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6836(0.7150) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2356  avg_val_loss: 0.7150  time: 65s\n",
      "Epoch 33 - avg_train_Score: 0.2356 avgScore: 0.7150\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 9m 47s) Loss: 0.2036(0.2036) Grad: 66666.6719  LR: 0.000098  \n",
      "Epoch: [34][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2225(0.2300) Grad: 54909.9453  LR: 0.000098  \n",
      "Epoch: [34][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2247(0.2300) Grad: 64658.7266  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6894(0.6894) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2300  avg_val_loss: 0.7151  time: 64s\n",
      "Epoch 34 - avg_train_Score: 0.2300 avgScore: 0.7151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6854(0.7151) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 37s) Loss: 0.2069(0.2069) Grad: 52024.3711  LR: 0.000076  \n",
      "Epoch: [35][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2188(0.2230) Grad: 48466.2539  LR: 0.000076  \n",
      "Epoch: [35][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2550(0.2234) Grad: 67251.5859  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.6879(0.6879) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2234  avg_val_loss: 0.7164  time: 64s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6880(0.7164) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 - avg_train_Score: 0.2234 avgScore: 0.7164\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 10m 30s) Loss: 0.2312(0.2312) Grad: 73780.1641  LR: 0.000057  \n",
      "Epoch: [36][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2114(0.2176) Grad: 48443.0117  LR: 0.000057  \n",
      "Epoch: [36][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2352(0.2183) Grad: 56723.0547  LR: 0.000057  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.6911(0.6911) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2183  avg_val_loss: 0.7157  time: 65s\n",
      "Epoch 36 - avg_train_Score: 0.2183 avgScore: 0.7157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6887(0.7157) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.2388(0.2388) Grad: 55596.6797  LR: 0.000040  \n",
      "Epoch: [37][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2244(0.2133) Grad: 56716.3789  LR: 0.000040  \n",
      "Epoch: [37][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2195(0.2134) Grad: 45637.5430  LR: 0.000040  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 17s) Loss: 0.6877(0.6877) \n",
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6875(0.7152) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2134  avg_val_loss: 0.7152  time: 64s\n",
      "Epoch 37 - avg_train_Score: 0.2134 avgScore: 0.7152\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 9m 50s) Loss: 0.2403(0.2403) Grad: 58351.6406  LR: 0.000027  \n",
      "Epoch: [38][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.1978(0.2093) Grad: 60237.5078  LR: 0.000027  \n",
      "Epoch: [38][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2256(0.2089) Grad: 47993.4219  LR: 0.000027  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6885(0.6885) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2089  avg_val_loss: 0.7156  time: 64s\n",
      "Epoch 38 - avg_train_Score: 0.2089 avgScore: 0.7156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6883(0.7156) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 9m 57s) Loss: 0.2205(0.2205) Grad: 57221.4219  LR: 0.000016  \n",
      "Epoch: [39][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2238(0.2065) Grad: 67416.2578  LR: 0.000016  \n",
      "Epoch: [39][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2009(0.2068) Grad: 48193.7617  LR: 0.000016  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6914(0.6914) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2068  avg_val_loss: 0.7154  time: 64s\n",
      "Epoch 39 - avg_train_Score: 0.2068 avgScore: 0.7154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6907(0.7154) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.1886(0.1886) Grad: 45675.5781  LR: 0.000008  \n",
      "Epoch: [40][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2182(0.2045) Grad: 57748.2344  LR: 0.000008  \n",
      "Epoch: [40][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2120(0.2046) Grad: 49050.7617  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6893(0.6893) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2046  avg_val_loss: 0.7155  time: 63s\n",
      "Epoch 40 - avg_train_Score: 0.2046 avgScore: 0.7155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6890(0.7155) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 0 result ==========\n",
      "score: 0.7155\n",
      "========== fold: 1 training ==========\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8675, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 10s) Loss: 5.2215(5.2215) Grad: 41037.3906  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.5930(2.7719) Grad: 56324.9648  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.7804(2.6668) Grad: 72845.6328  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 2.2321(2.2321) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.6668  avg_val_loss: 2.0465  time: 64s\n",
      "Epoch 1 - avg_train_Score: 2.6668 avgScore: 2.0465\n",
      "Epoch 1 - Save Best Score: 2.0465 Model\n",
      "Epoch 1 - Save Best Loss: 2.0465 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.8037(2.0465) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 10s) Loss: 1.3109(1.3109) Grad: 197247.7656  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.3590(1.2550) Grad: 106185.2500  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.1748(1.2552) Grad: 100550.3359  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 1.1782(1.1782) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.2552  avg_val_loss: 1.1002  time: 64s\n",
      "Epoch 2 - avg_train_Score: 1.2552 avgScore: 1.1002\n",
      "Epoch 2 - Save Best Score: 1.1002 Model\n",
      "Epoch 2 - Save Best Loss: 1.1002 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0961(1.1002) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 5s) Loss: 0.9559(0.9559) Grad: 192668.5469  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.1919(1.2300) Grad: 76618.4375  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.1293(1.2292) Grad: 58513.2656  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 1.0689(1.0689) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2292  avg_val_loss: 1.0123  time: 63s\n",
      "Epoch 3 - avg_train_Score: 1.2292 avgScore: 1.0123\n",
      "Epoch 3 - Save Best Score: 1.0123 Model\n",
      "Epoch 3 - Save Best Loss: 1.0123 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0283(1.0123) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 1.1669(1.1669) Grad: 118265.9766  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0308(1.1051) Grad: 54926.6914  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.0177(1.1016) Grad: 61865.6289  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.9758(0.9758) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.1016  avg_val_loss: 0.9060  time: 64s\n",
      "Epoch 4 - avg_train_Score: 1.1016 avgScore: 0.9060\n",
      "Epoch 4 - Save Best Score: 0.9060 Model\n",
      "Epoch 4 - Save Best Loss: 0.9060 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9530(0.9060) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 9m 54s) Loss: 1.0523(1.0523) Grad: 123287.3984  LR: 0.000989  \n",
      "Epoch: [5][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.8910(0.8916) Grad: 131620.9219  LR: 0.000989  \n",
      "Epoch: [5][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.8601(0.8907) Grad: 93466.3984  LR: 0.000989  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.9139(0.9139) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.8907  avg_val_loss: 0.8511  time: 63s\n",
      "Epoch 5 - avg_train_Score: 0.8907 avgScore: 0.8511\n",
      "Epoch 5 - Save Best Score: 0.8511 Model\n",
      "Epoch 5 - Save Best Loss: 0.8511 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8776(0.8511) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.9942(0.9942) Grad: 106931.1328  LR: 0.000979  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9586(0.8187) Grad: 115785.1094  LR: 0.000979  \n",
      "Epoch: [6][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.9182(0.8184) Grad: 119685.1641  LR: 0.000979  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.9042(0.9042) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8371(0.8183) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8184  avg_val_loss: 0.8183  time: 64s\n",
      "Epoch 6 - avg_train_Score: 0.8184 avgScore: 0.8183\n",
      "Epoch 6 - Save Best Score: 0.8183 Model\n",
      "Epoch 6 - Save Best Loss: 0.8183 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 9m 59s) Loss: 0.8035(0.8035) Grad: 106851.5547  LR: 0.000965  \n",
      "Epoch: [7][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6836(0.7670) Grad: 90357.8906  LR: 0.000965  \n",
      "Epoch: [7][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.7591(0.7669) Grad: 115535.5859  LR: 0.000965  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.8825(0.8825) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7669  avg_val_loss: 0.8024  time: 63s\n",
      "Epoch 7 - avg_train_Score: 0.7669 avgScore: 0.8024\n",
      "Epoch 7 - Save Best Score: 0.8024 Model\n",
      "Epoch 7 - Save Best Loss: 0.8024 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7990(0.8024) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 10m 18s) Loss: 0.7107(0.7107) Grad: 106000.2188  LR: 0.000949  \n",
      "Epoch: [8][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.8463(0.7362) Grad: 143117.8594  LR: 0.000949  \n",
      "Epoch: [8][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.7442(0.7366) Grad: 107914.8438  LR: 0.000949  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.8650(0.8650) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7366  avg_val_loss: 0.7995  time: 63s\n",
      "Epoch 8 - avg_train_Score: 0.7366 avgScore: 0.7995\n",
      "Epoch 8 - Save Best Score: 0.7995 Model\n",
      "Epoch 8 - Save Best Loss: 0.7995 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7639(0.7995) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 0.5888(0.5888) Grad: 103705.0469  LR: 0.000929  \n",
      "Epoch: [9][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8954(0.6926) Grad: 118615.0781  LR: 0.000929  \n",
      "Epoch: [9][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6801(0.6957) Grad: 109189.3750  LR: 0.000929  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.8564(0.8564) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6957  avg_val_loss: 0.7848  time: 64s\n",
      "Epoch 9 - avg_train_Score: 0.6957 avgScore: 0.7848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7582(0.7848) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Save Best Score: 0.7848 Model\n",
      "Epoch 9 - Save Best Loss: 0.7848 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 10m 5s) Loss: 0.5730(0.5730) Grad: 101392.5000  LR: 0.000908  \n",
      "Epoch: [10][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7232(0.6651) Grad: 97062.6406  LR: 0.000908  \n",
      "Epoch: [10][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6220(0.6642) Grad: 98735.6641  LR: 0.000908  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.8622(0.8622) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6642  avg_val_loss: 0.7759  time: 64s\n",
      "Epoch 10 - avg_train_Score: 0.6642 avgScore: 0.7759\n",
      "Epoch 10 - Save Best Score: 0.7759 Model\n",
      "Epoch 10 - Save Best Loss: 0.7759 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7317(0.7759) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 10m 12s) Loss: 0.6228(0.6228) Grad: 102301.1328  LR: 0.000883  \n",
      "Epoch: [11][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7291(0.6197) Grad: 97437.7891  LR: 0.000883  \n",
      "Epoch: [11][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7138(0.6236) Grad: 84721.8672  LR: 0.000883  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.8461(0.8461) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.6236  avg_val_loss: 0.7717  time: 64s\n",
      "Epoch 11 - avg_train_Score: 0.6236 avgScore: 0.7717\n",
      "Epoch 11 - Save Best Score: 0.7717 Model\n",
      "Epoch 11 - Save Best Loss: 0.7717 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7036(0.7717) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 10m 43s) Loss: 0.5501(0.5501) Grad: 86484.6484  LR: 0.000857  \n",
      "Epoch: [12][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5931(0.5954) Grad: 78461.4141  LR: 0.000857  \n",
      "Epoch: [12][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6325(0.5966) Grad: 83218.3516  LR: 0.000857  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.8513(0.8513) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.5966  avg_val_loss: 0.7680  time: 64s\n",
      "Epoch 12 - avg_train_Score: 0.5966 avgScore: 0.7680\n",
      "Epoch 12 - Save Best Score: 0.7680 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7196(0.7680) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Save Best Loss: 0.7680 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 10m 3s) Loss: 0.5725(0.5725) Grad: 84752.8203  LR: 0.000828  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5085(0.5698) Grad: 80902.3594  LR: 0.000828  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5441(0.5695) Grad: 86557.0547  LR: 0.000828  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.8114(0.8114) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.5695  avg_val_loss: 0.7604  time: 64s\n",
      "Epoch 13 - avg_train_Score: 0.5695 avgScore: 0.7604\n",
      "Epoch 13 - Save Best Score: 0.7604 Model\n",
      "Epoch 13 - Save Best Loss: 0.7604 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6924(0.7604) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 10m 10s) Loss: 0.6471(0.6471) Grad: 93217.8750  LR: 0.000797  \n",
      "Epoch: [14][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.5806(0.5348) Grad: 87012.1094  LR: 0.000797  \n",
      "Epoch: [14][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5666(0.5356) Grad: 94535.5391  LR: 0.000797  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.8206(0.8206) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5356  avg_val_loss: 0.7527  time: 63s\n",
      "Epoch 14 - avg_train_Score: 0.5356 avgScore: 0.7527\n",
      "Epoch 14 - Save Best Score: 0.7527 Model\n",
      "Epoch 14 - Save Best Loss: 0.7527 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7219(0.7527) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 11s) Loss: 0.5272(0.5272) Grad: 74152.9688  LR: 0.000764  \n",
      "Epoch: [15][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5059(0.5156) Grad: 85284.2500  LR: 0.000764  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5672(0.5147) Grad: 92360.0703  LR: 0.000764  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7930(0.7930) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.5147  avg_val_loss: 0.7498  time: 64s\n",
      "Epoch 15 - avg_train_Score: 0.5147 avgScore: 0.7498\n",
      "Epoch 15 - Save Best Score: 0.7498 Model\n",
      "Epoch 15 - Save Best Loss: 0.7498 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7321(0.7498) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 10m 8s) Loss: 0.4549(0.4549) Grad: 85409.2734  LR: 0.000730  \n",
      "Epoch: [16][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.5531(0.4912) Grad: 85227.9688  LR: 0.000730  \n",
      "Epoch: [16][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.4772(0.4926) Grad: 91948.7734  LR: 0.000730  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7957(0.7957) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4926  avg_val_loss: 0.7430  time: 63s\n",
      "Epoch 16 - avg_train_Score: 0.4926 avgScore: 0.7430\n",
      "Epoch 16 - Save Best Score: 0.7430 Model\n",
      "Epoch 16 - Save Best Loss: 0.7430 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7021(0.7430) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 12s) Loss: 0.4405(0.4405) Grad: 82325.1484  LR: 0.000694  \n",
      "Epoch: [17][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4513(0.4740) Grad: 76341.5781  LR: 0.000694  \n",
      "Epoch: [17][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4615(0.4738) Grad: 76899.2812  LR: 0.000694  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7860(0.7860) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4738  avg_val_loss: 0.7429  time: 64s\n",
      "Epoch 17 - avg_train_Score: 0.4738 avgScore: 0.7429\n",
      "Epoch 17 - Save Best Score: 0.7429 Model\n",
      "Epoch 17 - Save Best Loss: 0.7429 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7144(0.7429) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 10m 27s) Loss: 0.3830(0.3830) Grad: 74187.1562  LR: 0.000657  \n",
      "Epoch: [18][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5753(0.4523) Grad: 90654.2891  LR: 0.000657  \n",
      "Epoch: [18][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3950(0.4532) Grad: 75026.9766  LR: 0.000657  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7752(0.7752) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4532  avg_val_loss: 0.7378  time: 64s\n",
      "Epoch 18 - avg_train_Score: 0.4532 avgScore: 0.7378\n",
      "Epoch 18 - Save Best Score: 0.7378 Model\n",
      "Epoch 18 - Save Best Loss: 0.7378 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7364(0.7378) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.4202(0.4202) Grad: 82559.3828  LR: 0.000620  \n",
      "Epoch: [19][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4974(0.4312) Grad: 81275.7734  LR: 0.000620  \n",
      "Epoch: [19][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4640(0.4316) Grad: 77658.4688  LR: 0.000620  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.8049(0.8049) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4316  avg_val_loss: 0.7355  time: 64s\n",
      "Epoch 19 - avg_train_Score: 0.4316 avgScore: 0.7355\n",
      "Epoch 19 - Save Best Score: 0.7355 Model\n",
      "Epoch 19 - Save Best Loss: 0.7355 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7120(0.7355) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 10m 6s) Loss: 0.3611(0.3611) Grad: 71350.1641  LR: 0.000581  \n",
      "Epoch: [20][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.3963(0.4158) Grad: 68550.6953  LR: 0.000581  \n",
      "Epoch: [20][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.3387(0.4152) Grad: 67266.4531  LR: 0.000581  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.8020(0.8020) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.4152  avg_val_loss: 0.7344  time: 65s\n",
      "Epoch 20 - avg_train_Score: 0.4152 avgScore: 0.7344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6970(0.7344) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Save Best Score: 0.7344 Model\n",
      "Epoch 20 - Save Best Loss: 0.7344 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 10m 31s) Loss: 0.4224(0.4224) Grad: 77603.2188  LR: 0.000542  \n",
      "Epoch: [21][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4464(0.3944) Grad: 86642.1250  LR: 0.000542  \n",
      "Epoch: [21][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3472(0.3939) Grad: 73492.9453  LR: 0.000542  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.8073(0.8073) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3939  avg_val_loss: 0.7318  time: 63s\n",
      "Epoch 21 - avg_train_Score: 0.3939 avgScore: 0.7318\n",
      "Epoch 21 - Save Best Score: 0.7318 Model\n",
      "Epoch 21 - Save Best Loss: 0.7318 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7081(0.7318) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 10m 29s) Loss: 0.3908(0.3908) Grad: 68450.0781  LR: 0.000503  \n",
      "Epoch: [22][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.3461(0.3750) Grad: 63216.0352  LR: 0.000503  \n",
      "Epoch: [22][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.3706(0.3753) Grad: 70176.2500  LR: 0.000503  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7999(0.7999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3753  avg_val_loss: 0.7339  time: 65s\n",
      "Epoch 22 - avg_train_Score: 0.3753 avgScore: 0.7339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6905(0.7339) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 10m 10s) Loss: 0.3435(0.3435) Grad: 59884.1602  LR: 0.000464  \n",
      "Epoch: [23][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3374(0.3654) Grad: 64426.1055  LR: 0.000464  \n",
      "Epoch: [23][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3591(0.3651) Grad: 58769.5078  LR: 0.000464  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.8056(0.8056) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3651  avg_val_loss: 0.7279  time: 64s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6851(0.7279) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 - avg_train_Score: 0.3651 avgScore: 0.7279\n",
      "Epoch 23 - Save Best Score: 0.7279 Model\n",
      "Epoch 23 - Save Best Loss: 0.7279 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 21s) Loss: 0.3528(0.3528) Grad: 67334.8672  LR: 0.000425  \n",
      "Epoch: [24][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3192(0.3477) Grad: 67386.2344  LR: 0.000425  \n",
      "Epoch: [24][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3423(0.3480) Grad: 68549.4297  LR: 0.000425  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7949(0.7949) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3480  avg_val_loss: 0.7292  time: 65s\n",
      "Epoch 24 - avg_train_Score: 0.3480 avgScore: 0.7292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7063(0.7292) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 10m 6s) Loss: 0.3298(0.3298) Grad: 80011.8906  LR: 0.000386  \n",
      "Epoch: [25][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3145(0.3326) Grad: 69006.1172  LR: 0.000386  \n",
      "Epoch: [25][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3271(0.3329) Grad: 68315.4844  LR: 0.000386  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7863(0.7863) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3329  avg_val_loss: 0.7283  time: 65s\n",
      "Epoch 25 - avg_train_Score: 0.3329 avgScore: 0.7283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6909(0.7283) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 10m 0s) Loss: 0.3626(0.3626) Grad: 68681.2891  LR: 0.000348  \n",
      "Epoch: [26][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2662(0.3217) Grad: 64653.6719  LR: 0.000348  \n",
      "Epoch: [26][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3459(0.3221) Grad: 63545.4922  LR: 0.000348  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7658(0.7658) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3221  avg_val_loss: 0.7271  time: 63s\n",
      "Epoch 26 - avg_train_Score: 0.3221 avgScore: 0.7271\n",
      "Epoch 26 - Save Best Score: 0.7271 Model\n",
      "Epoch 26 - Save Best Loss: 0.7271 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7049(0.7271) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 1s) Loss: 0.3317(0.3317) Grad: 80109.0625  LR: 0.000311  \n",
      "Epoch: [27][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2816(0.3084) Grad: 67536.9297  LR: 0.000311  \n",
      "Epoch: [27][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3209(0.3087) Grad: 55292.5312  LR: 0.000311  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7742(0.7742) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.3087  avg_val_loss: 0.7268  time: 64s\n",
      "Epoch 27 - avg_train_Score: 0.3087 avgScore: 0.7268\n",
      "Epoch 27 - Save Best Score: 0.7268 Model\n",
      "Epoch 27 - Save Best Loss: 0.7268 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7227(0.7268) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 9m 59s) Loss: 0.2997(0.2997) Grad: 63045.9922  LR: 0.000276  \n",
      "Epoch: [28][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2982(0.2958) Grad: 74883.1797  LR: 0.000276  \n",
      "Epoch: [28][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2717(0.2955) Grad: 55516.5430  LR: 0.000276  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7796(0.7796) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.2955  avg_val_loss: 0.7256  time: 64s\n",
      "Epoch 28 - avg_train_Score: 0.2955 avgScore: 0.7256\n",
      "Epoch 28 - Save Best Score: 0.7256 Model\n",
      "Epoch 28 - Save Best Loss: 0.7256 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7149(0.7256) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 10m 27s) Loss: 0.3149(0.3149) Grad: 81028.1875  LR: 0.000242  \n",
      "Epoch: [29][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2663(0.2830) Grad: 73100.1250  LR: 0.000242  \n",
      "Epoch: [29][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3217(0.2826) Grad: 70225.6641  LR: 0.000242  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.7654(0.7654) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2826  avg_val_loss: 0.7225  time: 64s\n",
      "Epoch 29 - avg_train_Score: 0.2826 avgScore: 0.7225\n",
      "Epoch 29 - Save Best Score: 0.7225 Model\n",
      "Epoch 29 - Save Best Loss: 0.7225 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7177(0.7225) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 10m 37s) Loss: 0.2694(0.2694) Grad: 60461.4102  LR: 0.000209  \n",
      "Epoch: [30][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2466(0.2723) Grad: 63351.0781  LR: 0.000209  \n",
      "Epoch: [30][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2608(0.2722) Grad: 52840.9336  LR: 0.000209  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7807(0.7807) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7117(0.7240) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2722  avg_val_loss: 0.7240  time: 65s\n",
      "Epoch 30 - avg_train_Score: 0.2722 avgScore: 0.7240\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 10m 27s) Loss: 0.2563(0.2563) Grad: 68672.4219  LR: 0.000178  \n",
      "Epoch: [31][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2213(0.2646) Grad: 53651.4922  LR: 0.000178  \n",
      "Epoch: [31][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2472(0.2644) Grad: 61631.4141  LR: 0.000178  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7771(0.7771) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2644  avg_val_loss: 0.7234  time: 65s\n",
      "Epoch 31 - avg_train_Score: 0.2644 avgScore: 0.7234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7074(0.7234) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.2718(0.2718) Grad: 52901.9297  LR: 0.000149  \n",
      "Epoch: [32][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2655(0.2522) Grad: 84203.7656  LR: 0.000149  \n",
      "Epoch: [32][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2849(0.2522) Grad: 73708.1094  LR: 0.000149  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7758(0.7758) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2522  avg_val_loss: 0.7211  time: 63s\n",
      "Epoch 32 - avg_train_Score: 0.2522 avgScore: 0.7211\n",
      "Epoch 32 - Save Best Score: 0.7211 Model\n",
      "Epoch 32 - Save Best Loss: 0.7211 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6998(0.7211) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 10m 1s) Loss: 0.2353(0.2353) Grad: 55203.0430  LR: 0.000122  \n",
      "Epoch: [33][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2161(0.2431) Grad: 62319.0391  LR: 0.000122  \n",
      "Epoch: [33][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2314(0.2434) Grad: 54617.4492  LR: 0.000122  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7616(0.7616) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2434  avg_val_loss: 0.7219  time: 64s\n",
      "Epoch 33 - avg_train_Score: 0.2434 avgScore: 0.7219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7043(0.7219) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 10m 27s) Loss: 0.2050(0.2050) Grad: 53305.6953  LR: 0.000098  \n",
      "Epoch: [34][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2442(0.2364) Grad: 62123.6055  LR: 0.000098  \n",
      "Epoch: [34][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2332(0.2362) Grad: 66772.8672  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7697(0.7697) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2362  avg_val_loss: 0.7221  time: 64s\n",
      "Epoch 34 - avg_train_Score: 0.2362 avgScore: 0.7221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7171(0.7221) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 14s) Loss: 0.2271(0.2271) Grad: 62066.0078  LR: 0.000076  \n",
      "Epoch: [35][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2109(0.2305) Grad: 54858.0625  LR: 0.000076  \n",
      "Epoch: [35][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2351(0.2302) Grad: 63499.2773  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7724(0.7724) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2302  avg_val_loss: 0.7227  time: 65s\n",
      "Epoch 35 - avg_train_Score: 0.2302 avgScore: 0.7227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7105(0.7227) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 10m 1s) Loss: 0.2270(0.2270) Grad: 63510.8086  LR: 0.000057  \n",
      "Epoch: [36][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2323(0.2246) Grad: 63806.5664  LR: 0.000057  \n",
      "Epoch: [36][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2057(0.2245) Grad: 73609.6406  LR: 0.000057  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7693(0.7693) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2245  avg_val_loss: 0.7234  time: 64s\n",
      "Epoch 36 - avg_train_Score: 0.2245 avgScore: 0.7234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7198(0.7234) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.2452(0.2452) Grad: 56934.5781  LR: 0.000040  \n",
      "Epoch: [37][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2008(0.2189) Grad: 70275.4453  LR: 0.000040  \n",
      "Epoch: [37][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2119(0.2190) Grad: 50139.9180  LR: 0.000040  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7684(0.7684) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2190  avg_val_loss: 0.7238  time: 64s\n",
      "Epoch 37 - avg_train_Score: 0.2190 avgScore: 0.7238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7186(0.7238) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 0.2138(0.2138) Grad: 57636.3633  LR: 0.000027  \n",
      "Epoch: [38][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2164(0.2148) Grad: 55303.4922  LR: 0.000027  \n",
      "Epoch: [38][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2138(0.2150) Grad: 54415.7812  LR: 0.000027  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7681(0.7681) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2150  avg_val_loss: 0.7243  time: 63s\n",
      "Epoch 38 - avg_train_Score: 0.2150 avgScore: 0.7243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7140(0.7243) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 10m 1s) Loss: 0.2210(0.2210) Grad: 52077.1641  LR: 0.000016  \n",
      "Epoch: [39][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.1946(0.2126) Grad: 48432.7891  LR: 0.000016  \n",
      "Epoch: [39][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2140(0.2124) Grad: 54093.9453  LR: 0.000016  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7689(0.7689) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2124  avg_val_loss: 0.7239  time: 64s\n",
      "Epoch 39 - avg_train_Score: 0.2124 avgScore: 0.7239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7150(0.7239) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.2812(0.2812) Grad: 74117.5234  LR: 0.000008  \n",
      "Epoch: [40][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.1868(0.2097) Grad: 52196.3672  LR: 0.000008  \n",
      "Epoch: [40][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2076(0.2098) Grad: 51328.7656  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7723(0.7723) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2098  avg_val_loss: 0.7238  time: 65s\n",
      "Epoch 40 - avg_train_Score: 0.2098 avgScore: 0.7238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7170(0.7238) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 1 result ==========\n",
      "score: 0.7238\n",
      "========== fold: 2 training ==========\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 15s) Loss: 5.1211(5.1211) Grad: 40732.0156  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.3082(2.7154) Grad: 49021.5586  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.0508(2.6124) Grad: 52467.6758  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 1.8564(1.8564) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.6124  avg_val_loss: 1.9334  time: 64s\n",
      "Epoch 1 - avg_train_Score: 2.6124 avgScore: 1.9334\n",
      "Epoch 1 - Save Best Score: 1.9334 Model\n",
      "Epoch 1 - Save Best Loss: 1.9334 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.9827(1.9334) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 9m 56s) Loss: 1.2598(1.2598) Grad: 201268.1719  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.1500(1.2234) Grad: 110493.0000  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9719(1.2234) Grad: 96345.5938  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.9107(0.9107) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.2234  avg_val_loss: 1.0677  time: 64s\n",
      "Epoch 2 - avg_train_Score: 1.2234 avgScore: 1.0677\n",
      "Epoch 2 - Save Best Score: 1.0677 Model\n",
      "Epoch 2 - Save Best Loss: 1.0677 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.1646(1.0677) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 12s) Loss: 1.0561(1.0561) Grad: 203171.7031  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8848(1.2172) Grad: 119275.6875  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.0602(1.2107) Grad: 137137.4375  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.8172(0.8172) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2107  avg_val_loss: 0.9654  time: 65s\n",
      "Epoch 3 - avg_train_Score: 1.2107 avgScore: 0.9654\n",
      "Epoch 3 - Save Best Score: 0.9654 Model\n",
      "Epoch 3 - Save Best Loss: 0.9654 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0747(0.9654) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 1.0126(1.0126) Grad: 136773.3281  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.4023(1.0705) Grad: 70052.5859  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.3140(1.0752) Grad: 62754.5664  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7309(0.7309) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0752  avg_val_loss: 0.9153  time: 64s\n",
      "Epoch 4 - avg_train_Score: 1.0752 avgScore: 0.9153\n",
      "Epoch 4 - Save Best Score: 0.9153 Model\n",
      "Epoch 4 - Save Best Loss: 0.9153 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9798(0.9153) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 10m 17s) Loss: 1.3025(1.3025) Grad: 121029.9688  LR: 0.000989  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.2016(1.0438) Grad: 28276.9551  LR: 0.000989  \n",
      "Epoch: [5][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.1383(1.0466) Grad: 19161.8184  LR: 0.000989  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7435(0.7435) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 1.0466  avg_val_loss: 0.8961  time: 64s\n",
      "Epoch 5 - avg_train_Score: 1.0466 avgScore: 0.8961\n",
      "Epoch 5 - Save Best Score: 0.8961 Model\n",
      "Epoch 5 - Save Best Loss: 0.8961 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8850(0.8961) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 10m 45s) Loss: 1.3243(1.3243) Grad: 108892.8594  LR: 0.000979  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6742(0.8695) Grad: 85929.5859  LR: 0.000979  \n",
      "Epoch: [6][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7101(0.8641) Grad: 77786.6562  LR: 0.000979  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7103(0.7103) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8641  avg_val_loss: 0.8116  time: 65s\n",
      "Epoch 6 - avg_train_Score: 0.8641 avgScore: 0.8116\n",
      "Epoch 6 - Save Best Score: 0.8116 Model\n",
      "Epoch 6 - Save Best Loss: 0.8116 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8214(0.8116) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 9m 42s) Loss: 0.6707(0.6707) Grad: 85499.0234  LR: 0.000965  \n",
      "Epoch: [7][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 1.0250(0.7536) Grad: 108104.6562  LR: 0.000965  \n",
      "Epoch: [7][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.8571(0.7559) Grad: 87775.7734  LR: 0.000965  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6832(0.6832) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7559  avg_val_loss: 0.7803  time: 65s\n",
      "Epoch 7 - avg_train_Score: 0.7559 avgScore: 0.7803\n",
      "Epoch 7 - Save Best Score: 0.7803 Model\n",
      "Epoch 7 - Save Best Loss: 0.7803 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8024(0.7803) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 10m 15s) Loss: 0.6165(0.6165) Grad: 91176.8906  LR: 0.000949  \n",
      "Epoch: [8][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6748(0.7069) Grad: nan  LR: 0.000949  \n",
      "Epoch: [8][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7373(0.7116) Grad: 53655.8945  LR: 0.000949  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6708(0.6708) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7116  avg_val_loss: 0.7700  time: 64s\n",
      "Epoch 8 - avg_train_Score: 0.7116 avgScore: 0.7700\n",
      "Epoch 8 - Save Best Score: 0.7700 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7786(0.7700) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Save Best Loss: 0.7700 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 9m 51s) Loss: 0.6383(0.6383) Grad: 94108.8516  LR: 0.000929  \n",
      "Epoch: [9][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.7074(0.6830) Grad: 83968.0156  LR: 0.000929  \n",
      "Epoch: [9][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.7465(0.6838) Grad: 95466.6328  LR: 0.000929  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6677(0.6677) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6838  avg_val_loss: 0.7538  time: 65s\n",
      "Epoch 9 - avg_train_Score: 0.6838 avgScore: 0.7538\n",
      "Epoch 9 - Save Best Score: 0.7538 Model\n",
      "Epoch 9 - Save Best Loss: 0.7538 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7711(0.7538) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 10m 7s) Loss: 0.5548(0.5548) Grad: 83962.8047  LR: 0.000908  \n",
      "Epoch: [10][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7064(0.6359) Grad: 92899.1016  LR: 0.000908  \n",
      "Epoch: [10][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8205(0.6373) Grad: 97704.2656  LR: 0.000908  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6781(0.6781) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6373  avg_val_loss: 0.7529  time: 64s\n",
      "Epoch 10 - avg_train_Score: 0.6373 avgScore: 0.7529\n",
      "Epoch 10 - Save Best Score: 0.7529 Model\n",
      "Epoch 10 - Save Best Loss: 0.7529 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7745(0.7529) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 10m 39s) Loss: 0.8311(0.8311) Grad: 117213.2578  LR: 0.000883  \n",
      "Epoch: [11][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6090(0.6143) Grad: 95732.4688  LR: 0.000883  \n",
      "Epoch: [11][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5796(0.6131) Grad: 93199.4453  LR: 0.000883  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6943(0.6943) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.6131  avg_val_loss: 0.7426  time: 64s\n",
      "Epoch 11 - avg_train_Score: 0.6131 avgScore: 0.7426\n",
      "Epoch 11 - Save Best Score: 0.7426 Model\n",
      "Epoch 11 - Save Best Loss: 0.7426 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7545(0.7426) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 10m 34s) Loss: 0.6808(0.6808) Grad: 96867.6328  LR: 0.000857  \n",
      "Epoch: [12][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5852(0.5866) Grad: 82797.1484  LR: 0.000857  \n",
      "Epoch: [12][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7008(0.5853) Grad: 83716.2500  LR: 0.000857  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6764(0.6764) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.5853  avg_val_loss: 0.7362  time: 65s\n",
      "Epoch 12 - avg_train_Score: 0.5853 avgScore: 0.7362\n",
      "Epoch 12 - Save Best Score: 0.7362 Model\n",
      "Epoch 12 - Save Best Loss: 0.7362 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7411(0.7362) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 10m 22s) Loss: 0.5892(0.5892) Grad: 96691.5234  LR: 0.000828  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4702(0.5576) Grad: 79558.5156  LR: 0.000828  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5024(0.5586) Grad: 79771.5625  LR: 0.000828  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.6970(0.6970) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7167(0.7347) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.5586  avg_val_loss: 0.7347  time: 64s\n",
      "Epoch 13 - avg_train_Score: 0.5586 avgScore: 0.7347\n",
      "Epoch 13 - Save Best Score: 0.7347 Model\n",
      "Epoch 13 - Save Best Loss: 0.7347 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 9m 47s) Loss: 0.5450(0.5450) Grad: 80902.5547  LR: 0.000797  \n",
      "Epoch: [14][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.5147(0.5371) Grad: 69107.4375  LR: 0.000797  \n",
      "Epoch: [14][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.5102(0.5368) Grad: 74851.9922  LR: 0.000797  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6950(0.6950) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5368  avg_val_loss: 0.7250  time: 65s\n",
      "Epoch 14 - avg_train_Score: 0.5368 avgScore: 0.7250\n",
      "Epoch 14 - Save Best Score: 0.7250 Model\n",
      "Epoch 14 - Save Best Loss: 0.7250 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7482(0.7250) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 26s) Loss: 0.5234(0.5234) Grad: 79404.7188  LR: 0.000764  \n",
      "Epoch: [15][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5144(0.5067) Grad: 70517.1406  LR: 0.000764  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5756(0.5086) Grad: 87638.2031  LR: 0.000764  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6856(0.6856) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.5086  avg_val_loss: 0.7250  time: 64s\n",
      "Epoch 15 - avg_train_Score: 0.5086 avgScore: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7037(0.7250) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 11m 29s) Loss: 0.5030(0.5030) Grad: 80074.8047  LR: 0.000730  \n",
      "Epoch: [16][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.5189(0.4942) Grad: 42769.3750  LR: 0.000730  \n",
      "Epoch: [16][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.4961(0.5013) Grad: 41229.5430  LR: 0.000730  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6873(0.6873) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.5013  avg_val_loss: 0.7241  time: 65s\n",
      "Epoch 16 - avg_train_Score: 0.5013 avgScore: 0.7241\n",
      "Epoch 16 - Save Best Score: 0.7241 Model\n",
      "Epoch 16 - Save Best Loss: 0.7241 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7185(0.7241) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.5659(0.5659) Grad: 95762.5938  LR: 0.000694  \n",
      "Epoch: [17][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4485(0.4827) Grad: 61603.1406  LR: 0.000694  \n",
      "Epoch: [17][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4945(0.4831) Grad: 76072.7031  LR: 0.000694  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6431(0.6431) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4831  avg_val_loss: 0.7143  time: 64s\n",
      "Epoch 17 - avg_train_Score: 0.4831 avgScore: 0.7143\n",
      "Epoch 17 - Save Best Score: 0.7143 Model\n",
      "Epoch 17 - Save Best Loss: 0.7143 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7383(0.7143) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 54s) Loss: 0.3865(0.3865) Grad: 84940.0391  LR: 0.000657  \n",
      "Epoch: [18][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4469(0.4408) Grad: 76349.3516  LR: 0.000657  \n",
      "Epoch: [18][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5154(0.4411) Grad: 80821.6328  LR: 0.000657  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6600(0.6600) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4411  avg_val_loss: 0.7082  time: 64s\n",
      "Epoch 18 - avg_train_Score: 0.4411 avgScore: 0.7082\n",
      "Epoch 18 - Save Best Score: 0.7082 Model\n",
      "Epoch 18 - Save Best Loss: 0.7082 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7181(0.7082) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 10m 17s) Loss: 0.4142(0.4142) Grad: 67807.7344  LR: 0.000620  \n",
      "Epoch: [19][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4231(0.4219) Grad: 77388.8984  LR: 0.000620  \n",
      "Epoch: [19][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4406(0.4220) Grad: 87554.2344  LR: 0.000620  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6425(0.6425) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7190(0.7055) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4220  avg_val_loss: 0.7055  time: 64s\n",
      "Epoch 19 - avg_train_Score: 0.4220 avgScore: 0.7055\n",
      "Epoch 19 - Save Best Score: 0.7055 Model\n",
      "Epoch 19 - Save Best Loss: 0.7055 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 10m 5s) Loss: 0.3561(0.3561) Grad: 67488.4375  LR: 0.000581  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3594(0.4031) Grad: 62984.0312  LR: 0.000581  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3626(0.4023) Grad: 63277.2344  LR: 0.000581  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6448(0.6448) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.4023  avg_val_loss: 0.7015  time: 64s\n",
      "Epoch 20 - avg_train_Score: 0.4023 avgScore: 0.7015\n",
      "Epoch 20 - Save Best Score: 0.7015 Model\n",
      "Epoch 20 - Save Best Loss: 0.7015 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.6904(0.7015) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 9m 48s) Loss: 0.3968(0.3968) Grad: 60244.2422  LR: 0.000542  \n",
      "Epoch: [21][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.5330(0.4394) Grad: 48465.3789  LR: 0.000542  \n",
      "Epoch: [21][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.4391(0.4388) Grad: 38438.5000  LR: 0.000542  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6469(0.6469) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.4388  avg_val_loss: 0.7059  time: 65s\n",
      "Epoch 21 - avg_train_Score: 0.4388 avgScore: 0.7059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7331(0.7059) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.4246(0.4246) Grad: 74741.4766  LR: 0.000503  \n",
      "Epoch: [22][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4362(0.3937) Grad: 69451.8438  LR: 0.000503  \n",
      "Epoch: [22][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3849(0.3924) Grad: 63436.3125  LR: 0.000503  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6326(0.6326) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3924  avg_val_loss: 0.6990  time: 65s\n",
      "Epoch 22 - avg_train_Score: 0.3924 avgScore: 0.6990\n",
      "Epoch 22 - Save Best Score: 0.6990 Model\n",
      "Epoch 22 - Save Best Loss: 0.6990 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7324(0.6990) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 9m 55s) Loss: 0.3876(0.3876) Grad: 60525.0312  LR: 0.000464  \n",
      "Epoch: [23][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3607(0.3675) Grad: 34723.0195  LR: 0.000464  \n",
      "Epoch: [23][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3716(0.3682) Grad: 47211.8125  LR: 0.000464  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6260(0.6260) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3682  avg_val_loss: 0.6970  time: 65s\n",
      "Epoch 23 - avg_train_Score: 0.3682 avgScore: 0.6970\n",
      "Epoch 23 - Save Best Score: 0.6970 Model\n",
      "Epoch 23 - Save Best Loss: 0.6970 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7074(0.6970) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 48s) Loss: 0.3892(0.3892) Grad: 75241.5547  LR: 0.000425  \n",
      "Epoch: [24][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3550(0.3509) Grad: 75452.7422  LR: 0.000425  \n",
      "Epoch: [24][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3454(0.3503) Grad: 52520.0391  LR: 0.000425  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6268(0.6268) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3503  avg_val_loss: 0.6957  time: 64s\n",
      "Epoch 24 - avg_train_Score: 0.3503 avgScore: 0.6957\n",
      "Epoch 24 - Save Best Score: 0.6957 Model\n",
      "Epoch 24 - Save Best Loss: 0.6957 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7349(0.6957) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 10m 17s) Loss: 0.3405(0.3405) Grad: 61588.9805  LR: 0.000386  \n",
      "Epoch: [25][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.3497(0.3188) Grad: 66493.7500  LR: 0.000386  \n",
      "Epoch: [25][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.3114(0.3187) Grad: 70432.4531  LR: 0.000386  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6381(0.6381) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3187  avg_val_loss: 0.6953  time: 65s\n",
      "Epoch 25 - avg_train_Score: 0.3187 avgScore: 0.6953\n",
      "Epoch 25 - Save Best Score: 0.6953 Model\n",
      "Epoch 25 - Save Best Loss: 0.6953 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7464(0.6953) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 9m 51s) Loss: 0.3519(0.3519) Grad: 62266.1758  LR: 0.000348  \n",
      "Epoch: [26][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3461(0.3079) Grad: 63262.6914  LR: 0.000348  \n",
      "Epoch: [26][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3255(0.3077) Grad: 84031.3906  LR: 0.000348  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6261(0.6261) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3077  avg_val_loss: 0.6946  time: 64s\n",
      "Epoch 26 - avg_train_Score: 0.3077 avgScore: 0.6946\n",
      "Epoch 26 - Save Best Score: 0.6946 Model\n",
      "Epoch 26 - Save Best Loss: 0.6946 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7578(0.6946) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 31s) Loss: 0.2801(0.2801) Grad: 66059.4609  LR: 0.000311  \n",
      "Epoch: [27][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3033(0.2972) Grad: 63493.9141  LR: 0.000311  \n",
      "Epoch: [27][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2729(0.2971) Grad: 57468.1836  LR: 0.000311  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6203(0.6203) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7339(0.6949) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.2971  avg_val_loss: 0.6949  time: 64s\n",
      "Epoch 27 - avg_train_Score: 0.2971 avgScore: 0.6949\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 10m 0s) Loss: 0.2938(0.2938) Grad: 66324.3672  LR: 0.000276  \n",
      "Epoch: [28][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3001(0.2894) Grad: 57526.0820  LR: 0.000276  \n",
      "Epoch: [28][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3186(0.2893) Grad: 70376.5312  LR: 0.000276  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6261(0.6261) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.2893  avg_val_loss: 0.6929  time: 64s\n",
      "Epoch 28 - avg_train_Score: 0.2893 avgScore: 0.6929\n",
      "Epoch 28 - Save Best Score: 0.6929 Model\n",
      "Epoch 28 - Save Best Loss: 0.6929 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7251(0.6929) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 9m 54s) Loss: 0.2702(0.2702) Grad: 56033.2734  LR: 0.000242  \n",
      "Epoch: [29][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2991(0.2788) Grad: 72744.6641  LR: 0.000242  \n",
      "Epoch: [29][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2866(0.2787) Grad: 62560.3203  LR: 0.000242  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6202(0.6202) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2787  avg_val_loss: 0.6925  time: 64s\n",
      "Epoch 29 - avg_train_Score: 0.2787 avgScore: 0.6925\n",
      "Epoch 29 - Save Best Score: 0.6925 Model\n",
      "Epoch 29 - Save Best Loss: 0.6925 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7453(0.6925) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 10m 8s) Loss: 0.2587(0.2587) Grad: 86838.2266  LR: 0.000209  \n",
      "Epoch: [30][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2717(0.2711) Grad: 53671.5742  LR: 0.000209  \n",
      "Epoch: [30][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2583(0.2713) Grad: 58253.8008  LR: 0.000209  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6212(0.6212) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2713  avg_val_loss: 0.6930  time: 63s\n",
      "Epoch 30 - avg_train_Score: 0.2713 avgScore: 0.6930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7228(0.6930) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 10m 34s) Loss: 0.2802(0.2802) Grad: 59971.2500  LR: 0.000178  \n",
      "Epoch: [31][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2405(0.2617) Grad: 56169.3750  LR: 0.000178  \n",
      "Epoch: [31][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2291(0.2618) Grad: 57269.1094  LR: 0.000178  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6198(0.6198) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2618  avg_val_loss: 0.6894  time: 63s\n",
      "Epoch 31 - avg_train_Score: 0.2618 avgScore: 0.6894\n",
      "Epoch 31 - Save Best Score: 0.6894 Model\n",
      "Epoch 31 - Save Best Loss: 0.6894 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7306(0.6894) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.2379(0.2379) Grad: 50595.1133  LR: 0.000149  \n",
      "Epoch: [32][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2763(0.2515) Grad: 77592.2500  LR: 0.000149  \n",
      "Epoch: [32][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2641(0.2514) Grad: 60009.1367  LR: 0.000149  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6300(0.6300) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2514  avg_val_loss: 0.6907  time: 64s\n",
      "Epoch 32 - avg_train_Score: 0.2514 avgScore: 0.6907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7260(0.6907) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 10m 50s) Loss: 0.2294(0.2294) Grad: 57260.7617  LR: 0.000122  \n",
      "Epoch: [33][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2665(0.2439) Grad: 61833.4844  LR: 0.000122  \n",
      "Epoch: [33][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2593(0.2440) Grad: 65186.7773  LR: 0.000122  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6267(0.6267) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2440  avg_val_loss: 0.6903  time: 65s\n",
      "Epoch 33 - avg_train_Score: 0.2440 avgScore: 0.6903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7349(0.6903) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 10m 38s) Loss: 0.2655(0.2655) Grad: 61097.4023  LR: 0.000098  \n",
      "Epoch: [34][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2455(0.2380) Grad: 50652.5469  LR: 0.000098  \n",
      "Epoch: [34][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2285(0.2378) Grad: 54463.4219  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6165(0.6165) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2378  avg_val_loss: 0.6892  time: 63s\n",
      "Epoch 34 - avg_train_Score: 0.2378 avgScore: 0.6892\n",
      "Epoch 34 - Save Best Score: 0.6892 Model\n",
      "Epoch 34 - Save Best Loss: 0.6892 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7341(0.6892) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 9m 55s) Loss: 0.2327(0.2327) Grad: 51155.7227  LR: 0.000076  \n",
      "Epoch: [35][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2464(0.2302) Grad: 74988.8750  LR: 0.000076  \n",
      "Epoch: [35][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2421(0.2301) Grad: 67858.5625  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6179(0.6179) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2301  avg_val_loss: 0.6898  time: 65s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7330(0.6898) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 - avg_train_Score: 0.2301 avgScore: 0.6898\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 10m 6s) Loss: 0.2507(0.2507) Grad: 58014.4961  LR: 0.000057  \n",
      "Epoch: [36][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2439(0.2271) Grad: 55495.3086  LR: 0.000057  \n",
      "Epoch: [36][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2171(0.2269) Grad: 62333.8086  LR: 0.000057  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6244(0.6244) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7284(0.6903) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2269  avg_val_loss: 0.6903  time: 64s\n",
      "Epoch 36 - avg_train_Score: 0.2269 avgScore: 0.6903\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 3s) Loss: 0.2406(0.2406) Grad: 74668.2656  LR: 0.000040  \n",
      "Epoch: [37][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2247(0.2207) Grad: 52816.1914  LR: 0.000040  \n",
      "Epoch: [37][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2377(0.2207) Grad: 70027.4844  LR: 0.000040  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6184(0.6184) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2207  avg_val_loss: 0.6912  time: 63s\n",
      "Epoch 37 - avg_train_Score: 0.2207 avgScore: 0.6912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7298(0.6912) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 10m 11s) Loss: 0.2265(0.2265) Grad: 62199.8438  LR: 0.000027  \n",
      "Epoch: [38][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2326(0.2172) Grad: 53671.8164  LR: 0.000027  \n",
      "Epoch: [38][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2284(0.2168) Grad: 59787.9062  LR: 0.000027  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6190(0.6190) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2168  avg_val_loss: 0.6906  time: 65s\n",
      "Epoch 38 - avg_train_Score: 0.2168 avgScore: 0.6906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7233(0.6906) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 10m 37s) Loss: 0.2326(0.2326) Grad: 56305.6836  LR: 0.000016  \n",
      "Epoch: [39][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2136(0.2137) Grad: 47135.1641  LR: 0.000016  \n",
      "Epoch: [39][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2173(0.2134) Grad: 50612.8945  LR: 0.000016  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6160(0.6160) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2134  avg_val_loss: 0.6910  time: 65s\n",
      "Epoch 39 - avg_train_Score: 0.2134 avgScore: 0.6910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7203(0.6910) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 10m 0s) Loss: 0.1948(0.1948) Grad: 61935.9492  LR: 0.000008  \n",
      "Epoch: [40][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2120(0.2117) Grad: 56649.9531  LR: 0.000008  \n",
      "Epoch: [40][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.1911(0.2118) Grad: 48262.3125  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6158(0.6158) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2118  avg_val_loss: 0.6910  time: 64s\n",
      "Epoch 40 - avg_train_Score: 0.2118 avgScore: 0.6910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7183(0.6910) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 2 result ==========\n",
      "score: 0.6910\n",
      "========== fold: 3 training ==========\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 7s) Loss: 4.4728(4.4728) Grad: 39764.5508  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.3478(2.7091) Grad: 66130.9375  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.1502(2.6053) Grad: 49928.9258  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 2.1456(2.1456) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.6053  avg_val_loss: 2.0629  time: 65s\n",
      "Epoch 1 - avg_train_Score: 2.6053 avgScore: 2.0629\n",
      "Epoch 1 - Save Best Score: 2.0629 Model\n",
      "Epoch 1 - Save Best Loss: 2.0629 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.7249(2.0629) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 28s) Loss: 1.2403(1.2403) Grad: 205969.5625  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.1239(1.2374) Grad: 97838.6719  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.2232(1.2353) Grad: 95704.1641  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 1.0525(1.0525) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.2353  avg_val_loss: 1.1504  time: 64s\n",
      "Epoch 2 - avg_train_Score: 1.2353 avgScore: 1.1504\n",
      "Epoch 2 - Save Best Score: 1.1504 Model\n",
      "Epoch 2 - Save Best Loss: 1.1504 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0633(1.1504) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 1.0644(1.0644) Grad: 202479.0312  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.2689(1.2523) Grad: 66776.0234  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.0832(1.2413) Grad: 52127.1953  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.8932(0.8932) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2413  avg_val_loss: 1.0163  time: 64s\n",
      "Epoch 3 - avg_train_Score: 1.2413 avgScore: 1.0163\n",
      "Epoch 3 - Save Best Score: 1.0163 Model\n",
      "Epoch 3 - Save Best Loss: 1.0163 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0938(1.0163) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 10m 7s) Loss: 1.0079(1.0079) Grad: 119005.2656  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8000(0.9932) Grad: 118223.4062  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7867(0.9863) Grad: 118832.2422  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.8000(0.8000) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 0.9863  avg_val_loss: 0.9020  time: 64s\n",
      "Epoch 4 - avg_train_Score: 0.9863 avgScore: 0.9020\n",
      "Epoch 4 - Save Best Score: 0.9020 Model\n",
      "Epoch 4 - Save Best Loss: 0.9020 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.0260(0.9020) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 10m 31s) Loss: 1.0932(1.0932) Grad: 126581.3984  LR: 0.000989  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8397(0.8963) Grad: 123857.1094  LR: 0.000989  \n",
      "Epoch: [5][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9983(0.8954) Grad: 118612.8438  LR: 0.000989  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7711(0.7711) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.8954  avg_val_loss: 0.8576  time: 64s\n",
      "Epoch 5 - avg_train_Score: 0.8954 avgScore: 0.8576\n",
      "Epoch 5 - Save Best Score: 0.8576 Model\n",
      "Epoch 5 - Save Best Loss: 0.8576 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9556(0.8576) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 11m 29s) Loss: 0.6789(0.6789) Grad: 105177.6406  LR: 0.000979  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.3665(0.8810) Grad: 64030.7852  LR: 0.000979  \n",
      "Epoch: [6][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9691(0.8844) Grad: 57110.5781  LR: 0.000979  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7862(0.7862) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8844  avg_val_loss: 0.8578  time: 64s\n",
      "Epoch 6 - avg_train_Score: 0.8844 avgScore: 0.8578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9712(0.8578) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 10m 14s) Loss: 0.7115(0.7115) Grad: 118633.6328  LR: 0.000965  \n",
      "Epoch: [7][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7815(0.7862) Grad: 98173.8906  LR: 0.000965  \n",
      "Epoch: [7][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8746(0.7854) Grad: 110033.4531  LR: 0.000965  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7509(0.7509) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8927(0.8209) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7854  avg_val_loss: 0.8209  time: 64s\n",
      "Epoch 7 - avg_train_Score: 0.7854 avgScore: 0.8209\n",
      "Epoch 7 - Save Best Score: 0.8209 Model\n",
      "Epoch 7 - Save Best Loss: 0.8209 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 9m 48s) Loss: 0.6974(0.6974) Grad: 84827.6328  LR: 0.000949  \n",
      "Epoch: [8][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7592(0.7171) Grad: 94849.4766  LR: 0.000949  \n",
      "Epoch: [8][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7817(0.7183) Grad: 90727.3281  LR: 0.000949  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7224(0.7224) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7183  avg_val_loss: 0.8069  time: 64s\n",
      "Epoch 8 - avg_train_Score: 0.7183 avgScore: 0.8069\n",
      "Epoch 8 - Save Best Score: 0.8069 Model\n",
      "Epoch 8 - Save Best Loss: 0.8069 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8668(0.8069) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 9m 56s) Loss: 0.7888(0.7888) Grad: 105041.1094  LR: 0.000929  \n",
      "Epoch: [9][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6257(0.6798) Grad: 90610.1953  LR: 0.000929  \n",
      "Epoch: [9][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7110(0.6790) Grad: 101300.6562  LR: 0.000929  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7150(0.7150) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6790  avg_val_loss: 0.7940  time: 65s\n",
      "Epoch 9 - avg_train_Score: 0.6790 avgScore: 0.7940\n",
      "Epoch 9 - Save Best Score: 0.7940 Model\n",
      "Epoch 9 - Save Best Loss: 0.7940 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8905(0.7940) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 9m 56s) Loss: 0.6009(0.6009) Grad: 97152.6875  LR: 0.000908  \n",
      "Epoch: [10][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6271(0.6461) Grad: 99501.6797  LR: 0.000908  \n",
      "Epoch: [10][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.6130(0.6483) Grad: 84737.0469  LR: 0.000908  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6785(0.6785) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6483  avg_val_loss: 0.7881  time: 63s\n",
      "Epoch 10 - avg_train_Score: 0.6483 avgScore: 0.7881\n",
      "Epoch 10 - Save Best Score: 0.7881 Model\n",
      "Epoch 10 - Save Best Loss: 0.7881 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8576(0.7881) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 9m 49s) Loss: 0.6130(0.6130) Grad: 87763.8984  LR: 0.000883  \n",
      "Epoch: [11][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6282(0.6196) Grad: 82649.2109  LR: 0.000883  \n",
      "Epoch: [11][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5618(0.6205) Grad: 95653.8672  LR: 0.000883  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6940(0.6940) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.6205  avg_val_loss: 0.7790  time: 63s\n",
      "Epoch 11 - avg_train_Score: 0.6205 avgScore: 0.7790\n",
      "Epoch 11 - Save Best Score: 0.7790 Model\n",
      "Epoch 11 - Save Best Loss: 0.7790 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8544(0.7790) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 10m 3s) Loss: 0.5599(0.5599) Grad: 84526.4688  LR: 0.000857  \n",
      "Epoch: [12][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4803(0.5853) Grad: 79823.9766  LR: 0.000857  \n",
      "Epoch: [12][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7322(0.5860) Grad: 92921.1328  LR: 0.000857  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6958(0.6958) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8348(0.7746) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.5860  avg_val_loss: 0.7746  time: 64s\n",
      "Epoch 12 - avg_train_Score: 0.5860 avgScore: 0.7746\n",
      "Epoch 12 - Save Best Score: 0.7746 Model\n",
      "Epoch 12 - Save Best Loss: 0.7746 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 10m 14s) Loss: 0.5997(0.5997) Grad: 80075.5391  LR: 0.000828  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6474(0.5590) Grad: 104454.3594  LR: 0.000828  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6405(0.5581) Grad: 83549.2422  LR: 0.000828  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6553(0.6553) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.5581  avg_val_loss: 0.7655  time: 65s\n",
      "Epoch 13 - avg_train_Score: 0.5581 avgScore: 0.7655\n",
      "Epoch 13 - Save Best Score: 0.7655 Model\n",
      "Epoch 13 - Save Best Loss: 0.7655 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8393(0.7655) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 9m 51s) Loss: 0.4939(0.4939) Grad: 82221.4297  LR: 0.000797  \n",
      "Epoch: [14][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.5120(0.5366) Grad: 82594.8047  LR: 0.000797  \n",
      "Epoch: [14][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5592(0.5375) Grad: 82037.2266  LR: 0.000797  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6768(0.6768) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5375  avg_val_loss: 0.7641  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.5375 avgScore: 0.7641\n",
      "Epoch 14 - Save Best Score: 0.7641 Model\n",
      "Epoch 14 - Save Best Loss: 0.7641 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8302(0.7641) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 27s) Loss: 0.6326(0.6326) Grad: 89178.4375  LR: 0.000764  \n",
      "Epoch: [15][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5366(0.5140) Grad: 85213.0859  LR: 0.000764  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5936(0.5135) Grad: 91321.5000  LR: 0.000764  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6757(0.6757) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.5135  avg_val_loss: 0.7635  time: 65s\n",
      "Epoch 15 - avg_train_Score: 0.5135 avgScore: 0.7635\n",
      "Epoch 15 - Save Best Score: 0.7635 Model\n",
      "Epoch 15 - Save Best Loss: 0.7635 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8516(0.7635) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 9m 59s) Loss: 0.5123(0.5123) Grad: 76750.7188  LR: 0.000730  \n",
      "Epoch: [16][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4375(0.4910) Grad: 66069.5469  LR: 0.000730  \n",
      "Epoch: [16][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4111(0.4905) Grad: 83546.4844  LR: 0.000730  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6789(0.6789) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4905  avg_val_loss: 0.7584  time: 64s\n",
      "Epoch 16 - avg_train_Score: 0.4905 avgScore: 0.7584\n",
      "Epoch 16 - Save Best Score: 0.7584 Model\n",
      "Epoch 16 - Save Best Loss: 0.7584 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8617(0.7584) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.4163(0.4163) Grad: 90895.0078  LR: 0.000694  \n",
      "Epoch: [17][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.5430(0.4615) Grad: 74852.5781  LR: 0.000694  \n",
      "Epoch: [17][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.3374(0.4617) Grad: 68139.3750  LR: 0.000694  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6807(0.6807) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4617  avg_val_loss: 0.7575  time: 65s\n",
      "Epoch 17 - avg_train_Score: 0.4617 avgScore: 0.7575\n",
      "Epoch 17 - Save Best Score: 0.7575 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8628(0.7575) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Save Best Loss: 0.7575 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 10m 7s) Loss: 0.5422(0.5422) Grad: 89292.3906  LR: 0.000657  \n",
      "Epoch: [18][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.4997(0.4448) Grad: 97276.4219  LR: 0.000657  \n",
      "Epoch: [18][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.4333(0.4466) Grad: 87855.2266  LR: 0.000657  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6761(0.6761) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4466  avg_val_loss: 0.7566  time: 63s\n",
      "Epoch 18 - avg_train_Score: 0.4466 avgScore: 0.7566\n",
      "Epoch 18 - Save Best Score: 0.7566 Model\n",
      "Epoch 18 - Save Best Loss: 0.7566 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8894(0.7566) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 12m 21s) Loss: 0.3879(0.3879) Grad: 68881.2344  LR: 0.000620  \n",
      "Epoch: [19][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3629(0.4283) Grad: 71465.8516  LR: 0.000620  \n",
      "Epoch: [19][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4774(0.4291) Grad: 101478.4766  LR: 0.000620  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6593(0.6593) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4291  avg_val_loss: 0.7481  time: 65s\n",
      "Epoch 19 - avg_train_Score: 0.4291 avgScore: 0.7481\n",
      "Epoch 19 - Save Best Score: 0.7481 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8468(0.7481) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Save Best Loss: 0.7481 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 0.3733(0.3733) Grad: 72877.7656  LR: 0.000581  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4574(0.4084) Grad: 75866.5703  LR: 0.000581  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4428(0.4084) Grad: 79021.3125  LR: 0.000581  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6640(0.6640) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.4084  avg_val_loss: 0.7483  time: 64s\n",
      "Epoch 20 - avg_train_Score: 0.4084 avgScore: 0.7483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8495(0.7483) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 10m 12s) Loss: 0.3082(0.3082) Grad: 67277.8125  LR: 0.000542  \n",
      "Epoch: [21][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3475(0.3912) Grad: 77411.3281  LR: 0.000542  \n",
      "Epoch: [21][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3904(0.3912) Grad: 65832.6250  LR: 0.000542  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6810(0.6810) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8544(0.7458) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3912  avg_val_loss: 0.7458  time: 64s\n",
      "Epoch 21 - avg_train_Score: 0.3912 avgScore: 0.7458\n",
      "Epoch 21 - Save Best Score: 0.7458 Model\n",
      "Epoch 21 - Save Best Loss: 0.7458 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.3806(0.3806) Grad: 84335.5547  LR: 0.000503  \n",
      "Epoch: [22][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4439(0.3764) Grad: 75754.2656  LR: 0.000503  \n",
      "Epoch: [22][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3122(0.3764) Grad: 65740.5859  LR: 0.000503  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.6579(0.6579) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8591(0.7447) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3764  avg_val_loss: 0.7447  time: 64s\n",
      "Epoch 22 - avg_train_Score: 0.3764 avgScore: 0.7447\n",
      "Epoch 22 - Save Best Score: 0.7447 Model\n",
      "Epoch 22 - Save Best Loss: 0.7447 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.3101(0.3101) Grad: 64465.4023  LR: 0.000464  \n",
      "Epoch: [23][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3492(0.3592) Grad: 70883.4297  LR: 0.000464  \n",
      "Epoch: [23][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3257(0.3581) Grad: 63316.8164  LR: 0.000464  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6654(0.6654) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3581  avg_val_loss: 0.7431  time: 64s\n",
      "Epoch 23 - avg_train_Score: 0.3581 avgScore: 0.7431\n",
      "Epoch 23 - Save Best Score: 0.7431 Model\n",
      "Epoch 23 - Save Best Loss: 0.7431 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8424(0.7431) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 18s) Loss: 0.3902(0.3902) Grad: 76442.6875  LR: 0.000425  \n",
      "Epoch: [24][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.3285(0.3430) Grad: 62334.7773  LR: 0.000425  \n",
      "Epoch: [24][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.3885(0.3432) Grad: 66813.5391  LR: 0.000425  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6865(0.6865) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8389(0.7422) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3432  avg_val_loss: 0.7422  time: 65s\n",
      "Epoch 24 - avg_train_Score: 0.3432 avgScore: 0.7422\n",
      "Epoch 24 - Save Best Score: 0.7422 Model\n",
      "Epoch 24 - Save Best Loss: 0.7422 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 12m 9s) Loss: 0.2978(0.2978) Grad: 67945.3750  LR: 0.000386  \n",
      "Epoch: [25][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3179(0.3331) Grad: 71937.0000  LR: 0.000386  \n",
      "Epoch: [25][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3342(0.3325) Grad: 62307.4219  LR: 0.000386  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.6621(0.6621) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3325  avg_val_loss: 0.7415  time: 65s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8509(0.7415) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 - avg_train_Score: 0.3325 avgScore: 0.7415\n",
      "Epoch 25 - Save Best Score: 0.7415 Model\n",
      "Epoch 25 - Save Best Loss: 0.7415 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 10m 17s) Loss: 0.3477(0.3477) Grad: 64561.1875  LR: 0.000348  \n",
      "Epoch: [26][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3742(0.3175) Grad: 84956.9531  LR: 0.000348  \n",
      "Epoch: [26][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2750(0.3171) Grad: 61746.1445  LR: 0.000348  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6617(0.6617) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8619(0.7386) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3171  avg_val_loss: 0.7386  time: 64s\n",
      "Epoch 26 - avg_train_Score: 0.3171 avgScore: 0.7386\n",
      "Epoch 26 - Save Best Score: 0.7386 Model\n",
      "Epoch 26 - Save Best Loss: 0.7386 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.3017(0.3017) Grad: 64798.5781  LR: 0.000311  \n",
      "Epoch: [27][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2813(0.3060) Grad: 54365.8398  LR: 0.000311  \n",
      "Epoch: [27][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2735(0.3050) Grad: 73952.2656  LR: 0.000311  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6955(0.6955) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8390(0.7388) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.3050  avg_val_loss: 0.7388  time: 64s\n",
      "Epoch 27 - avg_train_Score: 0.3050 avgScore: 0.7388\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 11m 10s) Loss: 0.3245(0.3245) Grad: 72981.4844  LR: 0.000276  \n",
      "Epoch: [28][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2849(0.2901) Grad: 79437.8750  LR: 0.000276  \n",
      "Epoch: [28][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2543(0.2907) Grad: 78207.7969  LR: 0.000276  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6683(0.6683) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.2907  avg_val_loss: 0.7392  time: 64s\n",
      "Epoch 28 - avg_train_Score: 0.2907 avgScore: 0.7392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8412(0.7392) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 10m 21s) Loss: 0.2693(0.2693) Grad: 65964.5234  LR: 0.000242  \n",
      "Epoch: [29][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2924(0.2779) Grad: 73912.3516  LR: 0.000242  \n",
      "Epoch: [29][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3128(0.2776) Grad: 78840.4219  LR: 0.000242  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6602(0.6602) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2776  avg_val_loss: 0.7394  time: 64s\n",
      "Epoch 29 - avg_train_Score: 0.2776 avgScore: 0.7394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8379(0.7394) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 10m 21s) Loss: 0.2806(0.2806) Grad: 73439.0312  LR: 0.000209  \n",
      "Epoch: [30][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2508(0.2690) Grad: 69112.8984  LR: 0.000209  \n",
      "Epoch: [30][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2941(0.2692) Grad: 59914.1914  LR: 0.000209  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6420(0.6420) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2692  avg_val_loss: 0.7388  time: 64s\n",
      "Epoch 30 - avg_train_Score: 0.2692 avgScore: 0.7388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8255(0.7388) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 0.3033(0.3033) Grad: 73729.7812  LR: 0.000178  \n",
      "Epoch: [31][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3290(0.2604) Grad: 74418.8984  LR: 0.000178  \n",
      "Epoch: [31][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2459(0.2599) Grad: 60314.8516  LR: 0.000178  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6431(0.6431) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2599  avg_val_loss: 0.7374  time: 64s\n",
      "Epoch 31 - avg_train_Score: 0.2599 avgScore: 0.7374\n",
      "Epoch 31 - Save Best Score: 0.7374 Model\n",
      "Epoch 31 - Save Best Loss: 0.7374 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8291(0.7374) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 10m 8s) Loss: 0.2180(0.2180) Grad: 53623.1211  LR: 0.000149  \n",
      "Epoch: [32][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2001(0.2500) Grad: 53150.3438  LR: 0.000149  \n",
      "Epoch: [32][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.1999(0.2496) Grad: 59186.4453  LR: 0.000149  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6404(0.6404) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2496  avg_val_loss: 0.7397  time: 65s\n",
      "Epoch 32 - avg_train_Score: 0.2496 avgScore: 0.7397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8312(0.7397) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 10m 12s) Loss: 0.2079(0.2079) Grad: 53572.2070  LR: 0.000122  \n",
      "Epoch: [33][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2301(0.2430) Grad: 56076.0430  LR: 0.000122  \n",
      "Epoch: [33][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2495(0.2430) Grad: 56289.0703  LR: 0.000122  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6533(0.6533) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2430  avg_val_loss: 0.7390  time: 64s\n",
      "Epoch 33 - avg_train_Score: 0.2430 avgScore: 0.7390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8402(0.7390) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 10m 5s) Loss: 0.2430(0.2430) Grad: 52571.6172  LR: 0.000098  \n",
      "Epoch: [34][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2771(0.2372) Grad: 61527.5547  LR: 0.000098  \n",
      "Epoch: [34][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2346(0.2368) Grad: 65332.6211  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6427(0.6427) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8326(0.7369) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2368  avg_val_loss: 0.7369  time: 64s\n",
      "Epoch 34 - avg_train_Score: 0.2368 avgScore: 0.7369\n",
      "Epoch 34 - Save Best Score: 0.7369 Model\n",
      "Epoch 34 - Save Best Loss: 0.7369 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 24s) Loss: 0.2192(0.2192) Grad: 69402.7188  LR: 0.000076  \n",
      "Epoch: [35][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2420(0.2285) Grad: 58950.2812  LR: 0.000076  \n",
      "Epoch: [35][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2497(0.2284) Grad: 63740.1797  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.6471(0.6471) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2284  avg_val_loss: 0.7384  time: 65s\n",
      "Epoch 35 - avg_train_Score: 0.2284 avgScore: 0.7384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8250(0.7384) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 10m 22s) Loss: 0.1814(0.1814) Grad: 47390.7773  LR: 0.000057  \n",
      "Epoch: [36][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.1814(0.2231) Grad: 51094.3867  LR: 0.000057  \n",
      "Epoch: [36][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2109(0.2233) Grad: 56801.6523  LR: 0.000057  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6459(0.6459) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2233  avg_val_loss: 0.7386  time: 65s\n",
      "Epoch 36 - avg_train_Score: 0.2233 avgScore: 0.7386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8304(0.7386) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 0.2055(0.2055) Grad: 66282.1406  LR: 0.000040  \n",
      "Epoch: [37][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2181(0.2185) Grad: 54341.3320  LR: 0.000040  \n",
      "Epoch: [37][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2172(0.2185) Grad: 62758.8203  LR: 0.000040  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6418(0.6418) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2185  avg_val_loss: 0.7384  time: 65s\n",
      "Epoch 37 - avg_train_Score: 0.2185 avgScore: 0.7384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8344(0.7384) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 11m 57s) Loss: 0.1761(0.1761) Grad: 54795.5000  LR: 0.000027  \n",
      "Epoch: [38][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2057(0.2134) Grad: 53338.6758  LR: 0.000027  \n",
      "Epoch: [38][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2403(0.2141) Grad: 71291.4766  LR: 0.000027  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.6389(0.6389) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2141  avg_val_loss: 0.7377  time: 63s\n",
      "Epoch 38 - avg_train_Score: 0.2141 avgScore: 0.7377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8332(0.7377) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 10m 9s) Loss: 0.2141(0.2141) Grad: 51481.2070  LR: 0.000016  \n",
      "Epoch: [39][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.1910(0.2109) Grad: 53878.6680  LR: 0.000016  \n",
      "Epoch: [39][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2188(0.2111) Grad: 67243.4297  LR: 0.000016  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.6396(0.6396) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2111  avg_val_loss: 0.7374  time: 65s\n",
      "Epoch 39 - avg_train_Score: 0.2111 avgScore: 0.7374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8374(0.7374) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 10m 23s) Loss: 0.2001(0.2001) Grad: 56199.4258  LR: 0.000008  \n",
      "Epoch: [40][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2071(0.2088) Grad: 42838.4414  LR: 0.000008  \n",
      "Epoch: [40][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.1775(0.2086) Grad: 60225.6914  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6387(0.6387) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2086  avg_val_loss: 0.7381  time: 64s\n",
      "Epoch 40 - avg_train_Score: 0.2086 avgScore: 0.7381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8387(0.7381) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 3 result ==========\n",
      "score: 0.7381\n",
      "========== fold: 4 training ==========\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 14m 49s) Loss: 5.7340(5.7340) Grad: 39885.9062  LR: 0.000100  \n",
      "Epoch: [1][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 1.1744(2.5989) Grad: 31841.3828  LR: 0.000100  \n",
      "Epoch: [1][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 1.5558(2.5054) Grad: 24178.1660  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 1.7053(1.7053) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.5054  avg_val_loss: 1.8703  time: 63s\n",
      "Epoch 1 - avg_train_Score: 2.5054 avgScore: 1.8703\n",
      "Epoch 1 - Save Best Score: 1.8703 Model\n",
      "Epoch 1 - Save Best Loss: 1.8703 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 2.1444(1.8703) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 14s) Loss: 1.0110(1.0110) Grad: 180570.2812  LR: 0.000100  \n",
      "Epoch: [2][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 1.0517(1.2121) Grad: 102692.9688  LR: 0.000100  \n",
      "Epoch: [2][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.0960(1.2099) Grad: 114188.0625  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 1.2103(1.2103) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.2099  avg_val_loss: 1.1204  time: 64s\n",
      "Epoch 2 - avg_train_Score: 1.2099 avgScore: 1.1204\n",
      "Epoch 2 - Save Best Score: 1.1204 Model\n",
      "Epoch 2 - Save Best Loss: 1.1204 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.2712(1.1204) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 16s) Loss: 1.1614(1.1614) Grad: 164570.7812  LR: 0.001000  \n",
      "Epoch: [3][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 1.1235(1.2399) Grad: 63020.0938  LR: 0.001000  \n",
      "Epoch: [3][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 1.2291(1.2300) Grad: 67729.1875  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 1.0745(1.0745) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.2300  avg_val_loss: 1.0094  time: 66s\n",
      "Epoch 3 - avg_train_Score: 1.2300 avgScore: 1.0094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 1.1237(1.0094) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Save Best Score: 1.0094 Model\n",
      "Epoch 3 - Save Best Loss: 1.0094 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 10m 33s) Loss: 0.9360(0.9360) Grad: 123984.6250  LR: 0.001000  \n",
      "Epoch: [4][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8421(1.0033) Grad: 132333.2812  LR: 0.001000  \n",
      "Epoch: [4][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9544(0.9992) Grad: 123699.2031  LR: 0.001000  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.9793(0.9793) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 0.9992  avg_val_loss: 0.8958  time: 64s\n",
      "Epoch 4 - avg_train_Score: 0.9992 avgScore: 0.8958\n",
      "Epoch 4 - Save Best Score: 0.8958 Model\n",
      "Epoch 4 - Save Best Loss: 0.8958 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9583(0.8958) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 11m 15s) Loss: 1.0360(1.0360) Grad: 126536.3672  LR: 0.000989  \n",
      "Epoch: [5][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9929(0.8943) Grad: 124099.1016  LR: 0.000989  \n",
      "Epoch: [5][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.9303(0.8936) Grad: 138648.3438  LR: 0.000989  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.8877(0.8877) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9491(0.8610) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.8936  avg_val_loss: 0.8610  time: 64s\n",
      "Epoch 5 - avg_train_Score: 0.8936 avgScore: 0.8610\n",
      "Epoch 5 - Save Best Score: 0.8610 Model\n",
      "Epoch 5 - Save Best Loss: 0.8610 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 11m 4s) Loss: 0.7057(0.7057) Grad: 127689.6562  LR: 0.000979  \n",
      "Epoch: [6][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.7338(0.8379) Grad: 109527.2969  LR: 0.000979  \n",
      "Epoch: [6][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 1.2033(0.8399) Grad: 176656.8906  LR: 0.000979  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.8720(0.8720) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8399  avg_val_loss: 0.8375  time: 65s\n",
      "Epoch 6 - avg_train_Score: 0.8399 avgScore: 0.8375\n",
      "Epoch 6 - Save Best Score: 0.8375 Model\n",
      "Epoch 6 - Save Best Loss: 0.8375 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8900(0.8375) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 10m 23s) Loss: 0.7709(0.7709) Grad: 168832.4219  LR: 0.000965  \n",
      "Epoch: [7][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.9426(0.8036) Grad: 64890.3008  LR: 0.000965  \n",
      "Epoch: [7][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.8458(0.8171) Grad: 52981.3242  LR: 0.000965  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.8755(0.8755) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8171  avg_val_loss: 0.8407  time: 65s\n",
      "Epoch 7 - avg_train_Score: 0.8171 avgScore: 0.8407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9407(0.8407) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 10m 12s) Loss: 0.9429(0.9429) Grad: 135987.0000  LR: 0.000949  \n",
      "Epoch: [8][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6986(0.7766) Grad: 124242.5391  LR: 0.000949  \n",
      "Epoch: [8][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7586(0.7737) Grad: 110489.6250  LR: 0.000949  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.8675(0.8675) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7737  avg_val_loss: 0.8138  time: 64s\n",
      "Epoch 8 - avg_train_Score: 0.7737 avgScore: 0.8138\n",
      "Epoch 8 - Save Best Score: 0.8138 Model\n",
      "Epoch 8 - Save Best Loss: 0.8138 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8796(0.8138) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 14s) Loss: 0.8642(0.8642) Grad: 134715.7500  LR: 0.000929  \n",
      "Epoch: [9][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.6264(0.7209) Grad: 120176.3203  LR: 0.000929  \n",
      "Epoch: [9][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.6150(0.7219) Grad: 114944.6719  LR: 0.000929  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.8160(0.8160) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.7219  avg_val_loss: 0.7987  time: 65s\n",
      "Epoch 9 - avg_train_Score: 0.7219 avgScore: 0.7987\n",
      "Epoch 9 - Save Best Score: 0.7987 Model\n",
      "Epoch 9 - Save Best Loss: 0.7987 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9086(0.7987) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 10m 2s) Loss: 0.7598(0.7598) Grad: 103440.1094  LR: 0.000908  \n",
      "Epoch: [10][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.6043(0.6796) Grad: 107635.3438  LR: 0.000908  \n",
      "Epoch: [10][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7467(0.6829) Grad: 128102.4062  LR: 0.000908  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.8094(0.8094) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6829  avg_val_loss: 0.7859  time: 65s\n",
      "Epoch 10 - avg_train_Score: 0.6829 avgScore: 0.7859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.9025(0.7859) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Save Best Score: 0.7859 Model\n",
      "Epoch 10 - Save Best Loss: 0.7859 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 10m 8s) Loss: 0.5560(0.5560) Grad: 95684.2734  LR: 0.000883  \n",
      "Epoch: [11][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.8128(0.6592) Grad: 114042.4766  LR: 0.000883  \n",
      "Epoch: [11][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5263(0.6579) Grad: 105338.9219  LR: 0.000883  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.8568(0.8568) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.6579  avg_val_loss: 0.7799  time: 64s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8825(0.7799) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 - avg_train_Score: 0.6579 avgScore: 0.7799\n",
      "Epoch 11 - Save Best Score: 0.7799 Model\n",
      "Epoch 11 - Save Best Loss: 0.7799 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 10m 11s) Loss: 0.5681(0.5681) Grad: 88367.1406  LR: 0.000857  \n",
      "Epoch: [12][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6324(0.6181) Grad: 108015.6562  LR: 0.000857  \n",
      "Epoch: [12][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.7560(0.6197) Grad: 97665.2812  LR: 0.000857  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7921(0.7921) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.6197  avg_val_loss: 0.7731  time: 64s\n",
      "Epoch 12 - avg_train_Score: 0.6197 avgScore: 0.7731\n",
      "Epoch 12 - Save Best Score: 0.7731 Model\n",
      "Epoch 12 - Save Best Loss: 0.7731 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8638(0.7731) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 10m 41s) Loss: 0.6388(0.6388) Grad: 88678.7969  LR: 0.000828  \n",
      "Epoch: [13][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.6672(0.5954) Grad: 140535.5469  LR: 0.000828  \n",
      "Epoch: [13][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6178(0.5964) Grad: 104245.1719  LR: 0.000828  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.8055(0.8055) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.5964  avg_val_loss: 0.7670  time: 64s\n",
      "Epoch 13 - avg_train_Score: 0.5964 avgScore: 0.7670\n",
      "Epoch 13 - Save Best Score: 0.7670 Model\n",
      "Epoch 13 - Save Best Loss: 0.7670 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8769(0.7670) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 11m 27s) Loss: 0.6388(0.6388) Grad: 101410.6641  LR: 0.000797  \n",
      "Epoch: [14][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5879(0.5656) Grad: 86345.0938  LR: 0.000797  \n",
      "Epoch: [14][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.5416(0.5666) Grad: 115064.7031  LR: 0.000797  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.8412(0.8412) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5666  avg_val_loss: 0.7646  time: 64s\n",
      "Epoch 14 - avg_train_Score: 0.5666 avgScore: 0.7646\n",
      "Epoch 14 - Save Best Score: 0.7646 Model\n",
      "Epoch 14 - Save Best Loss: 0.7646 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8240(0.7646) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 9m 49s) Loss: 0.4972(0.4972) Grad: 95734.8359  LR: 0.000764  \n",
      "Epoch: [15][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.5700(0.5351) Grad: 106128.8672  LR: 0.000764  \n",
      "Epoch: [15][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.6240(0.5360) Grad: 83980.0234  LR: 0.000764  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.8382(0.8382) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.5360  avg_val_loss: 0.7587  time: 64s\n",
      "Epoch 15 - avg_train_Score: 0.5360 avgScore: 0.7587\n",
      "Epoch 15 - Save Best Score: 0.7587 Model\n",
      "Epoch 15 - Save Best Loss: 0.7587 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8700(0.7587) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 9m 56s) Loss: 0.5595(0.5595) Grad: 112621.6719  LR: 0.000730  \n",
      "Epoch: [16][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.6461(0.5101) Grad: 126842.1328  LR: 0.000730  \n",
      "Epoch: [16][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.4386(0.5098) Grad: 72844.6328  LR: 0.000730  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7839(0.7839) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.5098  avg_val_loss: 0.7525  time: 63s\n",
      "Epoch 16 - avg_train_Score: 0.5098 avgScore: 0.7525\n",
      "Epoch 16 - Save Best Score: 0.7525 Model\n",
      "Epoch 16 - Save Best Loss: 0.7525 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8663(0.7525) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 38s) Loss: 0.5471(0.5471) Grad: 97721.2266  LR: 0.000694  \n",
      "Epoch: [17][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.4255(0.4854) Grad: 72989.3281  LR: 0.000694  \n",
      "Epoch: [17][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.5648(0.4848) Grad: 110222.4531  LR: 0.000694  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7916(0.7916) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4848  avg_val_loss: 0.7495  time: 65s\n",
      "Epoch 17 - avg_train_Score: 0.4848 avgScore: 0.7495\n",
      "Epoch 17 - Save Best Score: 0.7495 Model\n",
      "Epoch 17 - Save Best Loss: 0.7495 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7859(0.7495) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 9m 55s) Loss: 0.4756(0.4756) Grad: 93997.4688  LR: 0.000657  \n",
      "Epoch: [18][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4353(0.4650) Grad: 68319.9766  LR: 0.000657  \n",
      "Epoch: [18][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4939(0.4666) Grad: 84295.2891  LR: 0.000657  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7899(0.7899) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4666  avg_val_loss: 0.7459  time: 65s\n",
      "Epoch 18 - avg_train_Score: 0.4666 avgScore: 0.7459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7826(0.7459) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Save Best Score: 0.7459 Model\n",
      "Epoch 18 - Save Best Loss: 0.7459 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 10m 23s) Loss: 0.5134(0.5134) Grad: 73328.2266  LR: 0.000620  \n",
      "Epoch: [19][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4257(0.4398) Grad: 80208.8203  LR: 0.000620  \n",
      "Epoch: [19][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4020(0.4399) Grad: 72308.8281  LR: 0.000620  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7769(0.7769) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4399  avg_val_loss: 0.7438  time: 64s\n",
      "Epoch 19 - avg_train_Score: 0.4399 avgScore: 0.7438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7680(0.7438) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Save Best Score: 0.7438 Model\n",
      "Epoch 19 - Save Best Loss: 0.7438 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 0.4038(0.4038) Grad: 75590.5000  LR: 0.000581  \n",
      "Epoch: [20][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.4336(0.4208) Grad: 71095.5312  LR: 0.000581  \n",
      "Epoch: [20][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4151(0.4213) Grad: 68653.7656  LR: 0.000581  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7608(0.7608) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.4213  avg_val_loss: 0.7437  time: 64s\n",
      "Epoch 20 - avg_train_Score: 0.4213 avgScore: 0.7437\n",
      "Epoch 20 - Save Best Score: 0.7437 Model\n",
      "Epoch 20 - Save Best Loss: 0.7437 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7922(0.7437) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 10m 9s) Loss: 0.4207(0.4207) Grad: 67978.6797  LR: 0.000542  \n",
      "Epoch: [21][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3463(0.4038) Grad: 69608.6406  LR: 0.000542  \n",
      "Epoch: [21][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.5163(0.4038) Grad: 89637.3438  LR: 0.000542  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7720(0.7720) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.4038  avg_val_loss: 0.7406  time: 63s\n",
      "Epoch 21 - avg_train_Score: 0.4038 avgScore: 0.7406\n",
      "Epoch 21 - Save Best Score: 0.7406 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7880(0.7406) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Save Best Loss: 0.7406 Model\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 11m 28s) Loss: 0.4373(0.4373) Grad: 79418.2344  LR: 0.000503  \n",
      "Epoch: [22][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3443(0.3886) Grad: 73149.3516  LR: 0.000503  \n",
      "Epoch: [22][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3743(0.3882) Grad: 84461.9219  LR: 0.000503  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7576(0.7576) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3882  avg_val_loss: 0.7425  time: 64s\n",
      "Epoch 22 - avg_train_Score: 0.3882 avgScore: 0.7425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7999(0.7425) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 10m 9s) Loss: 0.3900(0.3900) Grad: 62723.6562  LR: 0.000464  \n",
      "Epoch: [23][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.3575(0.3694) Grad: 80998.3438  LR: 0.000464  \n",
      "Epoch: [23][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.3038(0.3694) Grad: 67349.4844  LR: 0.000464  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.7719(0.7719) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3694  avg_val_loss: 0.7376  time: 63s\n",
      "Epoch 23 - avg_train_Score: 0.3694 avgScore: 0.7376\n",
      "Epoch 23 - Save Best Score: 0.7376 Model\n",
      "Epoch 23 - Save Best Loss: 0.7376 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8008(0.7376) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 21s) Loss: 0.3715(0.3715) Grad: 74587.1328  LR: 0.000425  \n",
      "Epoch: [24][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.3129(0.3536) Grad: 69777.8438  LR: 0.000425  \n",
      "Epoch: [24][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.3373(0.3525) Grad: 73721.4297  LR: 0.000425  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7542(0.7542) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3525  avg_val_loss: 0.7378  time: 65s\n",
      "Epoch 24 - avg_train_Score: 0.3525 avgScore: 0.7378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8146(0.7378) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 10m 7s) Loss: 0.3940(0.3940) Grad: 82131.5391  LR: 0.000386  \n",
      "Epoch: [25][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3456(0.3384) Grad: 53972.6562  LR: 0.000386  \n",
      "Epoch: [25][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.4149(0.3386) Grad: 73614.9766  LR: 0.000386  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7537(0.7537) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3386  avg_val_loss: 0.7372  time: 64s\n",
      "Epoch 25 - avg_train_Score: 0.3386 avgScore: 0.7372\n",
      "Epoch 25 - Save Best Score: 0.7372 Model\n",
      "Epoch 25 - Save Best Loss: 0.7372 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8154(0.7372) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 10m 33s) Loss: 0.3203(0.3203) Grad: 68749.8984  LR: 0.000348  \n",
      "Epoch: [26][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3226(0.3223) Grad: 56602.3008  LR: 0.000348  \n",
      "Epoch: [26][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3174(0.3229) Grad: 72507.1172  LR: 0.000348  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7536(0.7536) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3229  avg_val_loss: 0.7369  time: 65s\n",
      "Epoch 26 - avg_train_Score: 0.3229 avgScore: 0.7369\n",
      "Epoch 26 - Save Best Score: 0.7369 Model\n",
      "Epoch 26 - Save Best Loss: 0.7369 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.8024(0.7369) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 0.3273(0.3273) Grad: 72550.6953  LR: 0.000311  \n",
      "Epoch: [27][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3034(0.3103) Grad: 67202.3203  LR: 0.000311  \n",
      "Epoch: [27][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2854(0.3101) Grad: 67272.3438  LR: 0.000311  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7640(0.7640) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.3101  avg_val_loss: 0.7351  time: 64s\n",
      "Epoch 27 - avg_train_Score: 0.3101 avgScore: 0.7351\n",
      "Epoch 27 - Save Best Score: 0.7351 Model\n",
      "Epoch 27 - Save Best Loss: 0.7351 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7634(0.7351) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.3044(0.3044) Grad: 71966.8984  LR: 0.000276  \n",
      "Epoch: [28][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2849(0.2955) Grad: 78107.3594  LR: 0.000276  \n",
      "Epoch: [28][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3277(0.2961) Grad: 75799.9453  LR: 0.000276  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.7634(0.7634) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.2961  avg_val_loss: 0.7338  time: 64s\n",
      "Epoch 28 - avg_train_Score: 0.2961 avgScore: 0.7338\n",
      "Epoch 28 - Save Best Score: 0.7338 Model\n",
      "Epoch 28 - Save Best Loss: 0.7338 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7644(0.7338) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 10m 29s) Loss: 0.3165(0.3165) Grad: 63990.2070  LR: 0.000242  \n",
      "Epoch: [29][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3405(0.2839) Grad: 82071.5312  LR: 0.000242  \n",
      "Epoch: [29][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.3352(0.2844) Grad: 85544.4141  LR: 0.000242  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7368(0.7368) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2844  avg_val_loss: 0.7346  time: 64s\n",
      "Epoch 29 - avg_train_Score: 0.2844 avgScore: 0.7346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7761(0.7346) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 10m 32s) Loss: 0.2504(0.2504) Grad: 67995.6484  LR: 0.000209  \n",
      "Epoch: [30][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3104(0.2741) Grad: 75080.6797  LR: 0.000209  \n",
      "Epoch: [30][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2644(0.2738) Grad: 69717.7266  LR: 0.000209  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7537(0.7537) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2738  avg_val_loss: 0.7331  time: 64s\n",
      "Epoch 30 - avg_train_Score: 0.2738 avgScore: 0.7331\n",
      "Epoch 30 - Save Best Score: 0.7331 Model\n",
      "Epoch 30 - Save Best Loss: 0.7331 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7684(0.7331) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.2776(0.2776) Grad: 55730.7773  LR: 0.000178  \n",
      "Epoch: [31][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2477(0.2645) Grad: 51217.8359  LR: 0.000178  \n",
      "Epoch: [31][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2655(0.2645) Grad: 65375.0859  LR: 0.000178  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7709(0.7709) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2645  avg_val_loss: 0.7339  time: 64s\n",
      "Epoch 31 - avg_train_Score: 0.2645 avgScore: 0.7339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7584(0.7339) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 0.2380(0.2380) Grad: 87687.5000  LR: 0.000149  \n",
      "Epoch: [32][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.3157(0.2546) Grad: 74553.2188  LR: 0.000149  \n",
      "Epoch: [32][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2383(0.2546) Grad: 60991.6680  LR: 0.000149  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7573(0.7573) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7548(0.7352) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2546  avg_val_loss: 0.7352  time: 64s\n",
      "Epoch 32 - avg_train_Score: 0.2546 avgScore: 0.7352\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 10m 3s) Loss: 0.2403(0.2403) Grad: 63770.2930  LR: 0.000122  \n",
      "Epoch: [33][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2713(0.2450) Grad: 58639.3086  LR: 0.000122  \n",
      "Epoch: [33][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2377(0.2446) Grad: 69795.1484  LR: 0.000122  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7488(0.7488) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7570(0.7336) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2446  avg_val_loss: 0.7336  time: 64s\n",
      "Epoch 33 - avg_train_Score: 0.2446 avgScore: 0.7336\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 10m 33s) Loss: 0.2392(0.2392) Grad: 69775.2188  LR: 0.000098  \n",
      "Epoch: [34][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2365(0.2366) Grad: 68258.7266  LR: 0.000098  \n",
      "Epoch: [34][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2610(0.2365) Grad: 73387.4844  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7473(0.7473) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2365  avg_val_loss: 0.7339  time: 63s\n",
      "Epoch 34 - avg_train_Score: 0.2365 avgScore: 0.7339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7607(0.7339) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 40s) Loss: 0.2305(0.2305) Grad: 68304.8828  LR: 0.000076  \n",
      "Epoch: [35][500/542] Elapsed 0m 51s (remain 0m 4s) Loss: 0.2221(0.2297) Grad: 54625.9102  LR: 0.000076  \n",
      "Epoch: [35][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2092(0.2295) Grad: 60031.2578  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7518(0.7518) \n",
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7659(0.7337) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2295  avg_val_loss: 0.7337  time: 65s\n",
      "Epoch 35 - avg_train_Score: 0.2295 avgScore: 0.7337\n",
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 10m 46s) Loss: 0.2287(0.2287) Grad: 59879.5781  LR: 0.000057  \n",
      "Epoch: [36][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2074(0.2239) Grad: 50316.9258  LR: 0.000057  \n",
      "Epoch: [36][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.2086(0.2233) Grad: 54107.8711  LR: 0.000057  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7482(0.7482) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2233  avg_val_loss: 0.7329  time: 64s\n",
      "Epoch 36 - avg_train_Score: 0.2233 avgScore: 0.7329\n",
      "Epoch 36 - Save Best Score: 0.7329 Model\n",
      "Epoch 36 - Save Best Loss: 0.7329 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7721(0.7329) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.1722(0.1722) Grad: 49229.5000  LR: 0.000040  \n",
      "Epoch: [37][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2549(0.2178) Grad: 56235.4805  LR: 0.000040  \n",
      "Epoch: [37][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.1948(0.2174) Grad: 50989.4961  LR: 0.000040  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.7525(0.7525) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2174  avg_val_loss: 0.7331  time: 64s\n",
      "Epoch 37 - avg_train_Score: 0.2174 avgScore: 0.7331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7735(0.7331) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 10m 10s) Loss: 0.2151(0.2151) Grad: 59911.1484  LR: 0.000027  \n",
      "Epoch: [38][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2176(0.2130) Grad: 51844.7109  LR: 0.000027  \n",
      "Epoch: [38][541/542] Elapsed 0m 55s (remain 0m 0s) Loss: 0.2158(0.2131) Grad: 58784.3047  LR: 0.000027  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 18s) Loss: 0.7519(0.7519) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2131  avg_val_loss: 0.7337  time: 64s\n",
      "Epoch 38 - avg_train_Score: 0.2131 avgScore: 0.7337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7804(0.7337) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 10m 32s) Loss: 0.2196(0.2196) Grad: 55009.2500  LR: 0.000016  \n",
      "Epoch: [39][500/542] Elapsed 0m 49s (remain 0m 4s) Loss: 0.2086(0.2101) Grad: 51840.5273  LR: 0.000016  \n",
      "Epoch: [39][541/542] Elapsed 0m 53s (remain 0m 0s) Loss: 0.2432(0.2098) Grad: 48489.0234  LR: 0.000016  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7536(0.7536) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2098  avg_val_loss: 0.7335  time: 63s\n",
      "Epoch 39 - avg_train_Score: 0.2098 avgScore: 0.7335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7778(0.7335) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_33553/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.2117(0.2117) Grad: 61171.4805  LR: 0.000008  \n",
      "Epoch: [40][500/542] Elapsed 0m 50s (remain 0m 4s) Loss: 0.2207(0.2088) Grad: 59509.2656  LR: 0.000008  \n",
      "Epoch: [40][541/542] Elapsed 0m 54s (remain 0m 0s) Loss: 0.1990(0.2087) Grad: 59850.0547  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7559(0.7559) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2087  avg_val_loss: 0.7334  time: 64s\n",
      "Epoch 40 - avg_train_Score: 0.2087 avgScore: 0.7334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7774(0.7334) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33553/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 4 result ==========\n",
      "score: 0.7334\n",
      "========== CV ==========\n",
      "score: 0.7204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "            if CFG.objective_cv == 'binary':\n",
    "                outputs.append(torch.sigmoid(model(x)).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'multiclass':\n",
    "                outputs.append(torch.softmax(\n",
    "                    model(x), axis=1).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'regression':\n",
    "                outputs.append(model(x).to('cpu').numpy())\n",
    "\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "\n",
    "def test_fn(valid_loader, model, device):\n",
    "    preds = []\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        preds.append(y_preds)\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def inference():\n",
    "    test = pd.read_csv(CFG.comp_dataset_path +\n",
    "                       'test_features.csv')\n",
    "\n",
    "    test['base_path'] = CFG.comp_dataset_path + 'images/' + test['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in test['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    print(test.head(5))\n",
    "\n",
    "    valid_dataset = CustomDataset(\n",
    "        test, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = EnsembleModel()\n",
    "    folds = [0] if CFG.use_holdout else list(range(CFG.n_fold))\n",
    "    for fold in folds:\n",
    "        _model = CustomModel(CFG, pretrained=False)\n",
    "        _model.to(device)\n",
    "\n",
    "        model_path = CFG.model_dir + \\\n",
    "            f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth'\n",
    "        print('load', model_path)\n",
    "        state = torch.load(model_path)['model']\n",
    "        _model.load_state_dict(state)\n",
    "        _model.eval()\n",
    "\n",
    "        # _model = tta.ClassificationTTAWrapper(\n",
    "        #     _model, tta.aliases.five_crop_transform(256, 256))\n",
    "\n",
    "        model.add_model(_model)\n",
    "\n",
    "    preds = test_fn(valid_loader, model, device)\n",
    "\n",
    "    test[CFG.target_col] = preds\n",
    "    test.to_csv(CFG.submission_dir +\n",
    "                'submission_oof.csv', index=False)\n",
    "    test[CFG.target_col].to_csv(\n",
    "        CFG.submission_dir + f'submission_{CFG.exp_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-0.5.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159afceaf33140cb841f4cf3974132bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     ID      vEgo      aEgo  steeringAngleDeg  \\\n",
      "0  012baccc145d400c896cb82065a93d42_120  3.374273 -0.019360        -34.008415   \n",
      "1  012baccc145d400c896cb82065a93d42_220  2.441048 -0.022754        307.860077   \n",
      "2  012baccc145d400c896cb82065a93d42_320  3.604152 -0.286239         10.774388   \n",
      "3  012baccc145d400c896cb82065a93d42_420  2.048902 -0.537628         61.045235   \n",
      "4  01d738e799d260a10f6324f78023b38f_120  2.201528 -1.898600          5.740093   \n",
      "\n",
      "   steeringTorque  brake  brakePressed  gas  gasPressed gearShifter  \\\n",
      "0            17.0    0.0         False  0.0       False       drive   \n",
      "1           295.0    0.0          True  0.0       False       drive   \n",
      "2          -110.0    0.0          True  0.0       False       drive   \n",
      "3           189.0    0.0          True  0.0       False       drive   \n",
      "4           -41.0    0.0          True  0.0       False       drive   \n",
      "\n",
      "   leftBlinker  rightBlinker  \\\n",
      "0        False         False   \n",
      "1        False         False   \n",
      "2        False         False   \n",
      "3         True         False   \n",
      "4        False         False   \n",
      "\n",
      "                                           base_path  \n",
      "0  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "1  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "2  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "3  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "4  ../raw/atmacup_18_dataset/images/01d738e799d26...  \n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_effnetb0_2/atmacup_18-models/tf_efficientnet_b0_ns_fold0_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_effnetb0_2/atmacup_18-models/tf_efficientnet_b0_ns_fold1_last.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n",
      "/tmp/ipykernel_33553/610043316.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path)['model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_effnetb0_2/atmacup_18-models/tf_efficientnet_b0_ns_fold2_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_effnetb0_2/atmacup_18-models/tf_efficientnet_b0_ns_fold3_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_effnetb0_2/atmacup_18-models/tf_efficientnet_b0_ns_fold4_last.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d847d85c277746adb1769c1598397a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
