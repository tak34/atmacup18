{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from timm.utils.model_ema import ModelEmaV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set dataset path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # ============== comp exp name =============\n",
    "    comp_name = 'atmacup_18'  # comp名\n",
    "\n",
    "    comp_dataset_path = '../raw/atmacup_18_dataset/'\n",
    "\n",
    "    exp_name = 'atmacup_18_cnn_swin_tiny'\n",
    "\n",
    "    is_debug = False\n",
    "    use_gray_scale = False\n",
    "\n",
    "    model_in_chans = 9  # モデルの入力チャンネル数\n",
    "\n",
    "    # ============== file path =============\n",
    "    train_fold_dir = \"../proc/baseline/folds\"\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = \"swin_tiny_patch4_window7_224\"\n",
    "\n",
    "    num_frames = 3  # model_in_chansの倍数\n",
    "    norm_in_chans = 1 if use_gray_scale else 3\n",
    "\n",
    "    use_torch_compile = False\n",
    "    use_ema = True\n",
    "    ema_decay = 0.995\n",
    "    # ============== training cfg =============\n",
    "    size = 224  # 224\n",
    "\n",
    "    batch_size = 64  # 32\n",
    "\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'\n",
    "    # scheduler = 'CosineAnnealingLR'\n",
    "    epochs = 40\n",
    "    if is_debug:\n",
    "        epochs = 2\n",
    "\n",
    "    # adamW warmupあり\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-4\n",
    "    if scheduler == 'GradualWarmupSchedulerV2':\n",
    "        lr /= warmup_factor\n",
    "\n",
    "    # ============== fold =============\n",
    "    n_fold = 5\n",
    "    use_holdout = False\n",
    "    use_alldata = False\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    skf_col = 'class'\n",
    "    group_col = 'scene'\n",
    "    fold_type = 'gkf'\n",
    "\n",
    "    objective_cv = 'regression'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'minimize'  # 'maximize', 'minimize'\n",
    "    metrics = 'calc_mae_atmacup'\n",
    "\n",
    "    # ============== pred target =============\n",
    "    target_size = 18\n",
    "    target_col = ['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1', 'x_2', 'y_2',\n",
    "                  'z_2', 'x_3', 'y_3', 'z_3', 'x_4', 'y_4', 'z_4', 'x_5', 'y_5', 'z_5']\n",
    "\n",
    "\n",
    "    # ============== ほぼ固定 =============\n",
    "    pretrained = True\n",
    "    inf_weight = 'last'  # 'best'\n",
    "\n",
    "    min_lr = 1e-8\n",
    "    weight_decay = 1e-5\n",
    "    max_grad_norm = 1000\n",
    "\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    # ============== set dataset path =============\n",
    "    if exp_name is not None:\n",
    "        print('set dataset path')\n",
    "\n",
    "        outputs_path = f'../proc/baseline/outputs/{exp_name}/'\n",
    "\n",
    "        submission_dir = outputs_path + 'submissions/'\n",
    "        submission_path = submission_dir + f'submission_{exp_name}.csv'\n",
    "\n",
    "        model_dir = outputs_path + \\\n",
    "            f'{comp_name}-models/'\n",
    "\n",
    "        figures_dir = outputs_path + 'figures/'\n",
    "\n",
    "        log_dir = outputs_path + 'logs/'\n",
    "        log_path = log_dir + f'{exp_name}.txt'\n",
    "\n",
    "    # ============== augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.ShiftScaleRotate(p=0.5),\n",
    "        A.OneOf([\n",
    "                A.GaussNoise(var_limit=[10, 50]),\n",
    "                A.GaussianBlur(),\n",
    "                A.MotionBlur(),\n",
    "                ], p=0.4),\n",
    "        # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        # A.CoarseDropout(max_holes=1, max_height=int(\n",
    "        #     size * 0.3), max_width=int(size * 0.3), p=0.5),\n",
    "\n",
    "        A.Normalize(\n",
    "            mean=[0] * norm_in_chans*num_frames,\n",
    "            std=[1] * norm_in_chans*num_frames, \n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * norm_in_chans*num_frames,\n",
    "            std=[1] * norm_in_chans*num_frames,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA が利用可能か: True\n",
      "利用可能な CUDA デバイス数: 1\n",
      "現在の CUDA デバイス: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA が利用可能か:\", torch.cuda.is_available())\n",
    "print(\"利用可能な CUDA デバイス数:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"現在の CUDA デバイス:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "def get_fold(train, cfg):\n",
    "    if cfg.fold_type == 'kf':\n",
    "        Fold = KFold(n_splits=cfg.n_fold,\n",
    "                     shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.target_col])\n",
    "    elif cfg.fold_type == 'skf':\n",
    "        Fold = StratifiedKFold(n_splits=cfg.n_fold,\n",
    "                               shuffle=True, random_state=cfg.seed)\n",
    "        kf = Fold.split(train, train[cfg.skf_col])\n",
    "    elif cfg.fold_type == 'gkf':\n",
    "        Fold = GroupKFold(n_splits=cfg.n_fold)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.group_col], groups)\n",
    "    elif cfg.fold_type == 'sgkf':\n",
    "        Fold = StratifiedGroupKFold(n_splits=cfg.n_fold,\n",
    "                                    shuffle=True, random_state=cfg.seed)\n",
    "        groups = train[cfg.group_col].values\n",
    "        kf = Fold.split(train, train[cfg.skf_col], groups)\n",
    "    # elif fold_type == 'mskf':\n",
    "    #     Fold = MultilabelStratifiedKFold(\n",
    "    #         n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    #     kf = Fold.split(train, train[cfg.skf_col])\n",
    "\n",
    "    for n, (train_index, val_index) in enumerate(kf):\n",
    "        train.loc[val_index, 'fold'] = int(n)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "    print(train.groupby('fold').size())\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_folds():\n",
    "    train_df = pd.read_csv(CFG.comp_dataset_path + 'train_features.csv')\n",
    "\n",
    "    train_df['scene'] = train_df['ID'].str.split('_').str[0]\n",
    "\n",
    "    print('group', CFG.group_col)\n",
    "    print(f'train len: {len(train_df)}')\n",
    "\n",
    "    train_df = get_fold(train_df, CFG)\n",
    "\n",
    "    # print(train_df.groupby(['fold', CFG.target_col]).size())\n",
    "    print(train_df['fold'].value_counts())\n",
    "\n",
    "    os.makedirs(CFG.train_fold_dir, exist_ok=True)\n",
    "\n",
    "    train_df.to_csv(CFG.train_fold_dir +\n",
    "                    'train_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group scene\n",
      "train len: 43371\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "dtype: int64\n",
      "fold\n",
      "0    8675\n",
      "1    8674\n",
      "2    8674\n",
      "3    8674\n",
      "4    8674\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "make_train_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数固定\n",
    "def set_seed(seed=None, cudnn_deterministic=True):\n",
    "    if seed is None:\n",
    "        seed = 42\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = cudnn_deterministic\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def make_dirs(cfg):\n",
    "    for dir in [cfg.model_dir, cfg.figures_dir, cfg.submission_dir, cfg.log_dir]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def cfg_init(cfg, mode='train'):\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    if mode == 'train':\n",
    "        make_dirs(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_init(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common_utils.logger import init_logger, wandb_init, AverageMeter, timeSince\n",
    "# from common_utils.settings import cfg_init\n",
    "\n",
    "def init_logger(log_file):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------- exp_info -----------------\n",
      "2024年11月20日 22:37:04\n"
     ]
    }
   ],
   "source": [
    "Logger = init_logger(log_file=CFG.log_path)\n",
    "\n",
    "Logger.info('\\n\\n-------- exp_info -----------------')\n",
    "Logger.info(datetime.datetime.now().strftime('%Y年%m月%d日 %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    # return roc_auc_score(y_true, y_pred)\n",
    "    eval_func = eval(CFG.metrics)\n",
    "    return eval_func(y_true, y_pred)\n",
    "\n",
    "\n",
    "def calc_mae_atmacup(y_true, y_pred):\n",
    "    abs_diff = np.abs(y_true - y_pred)  # 各予測の差分の絶対値を計算して\n",
    "    mae = np.mean(abs_diff.reshape(-1, ))  # 予測の差分の絶対値の平均を計算\n",
    "\n",
    "    return mae\n",
    "\n",
    "def get_result(result_df):\n",
    "\n",
    "    # preds = result_df['preds'].values\n",
    "\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "    preds = result_df[pred_cols].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    Logger.info(f'score: {score:<.4f}')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_traffic_light(image, id):\n",
    "    path = f'./datasets/atmacup_18/traffic_lights/{id}.json'\n",
    "    traffic_lights = json.load(open(path))\n",
    "\n",
    "    traffic_class = ['green',\n",
    "                     'straight', 'left', 'right', 'empty', 'other', 'yellow', 'red']\n",
    "    class_to_idx = {\n",
    "        cls: idx for idx, cls in enumerate(traffic_class)\n",
    "    }\n",
    "\n",
    "    for traffic_light in traffic_lights:\n",
    "        bbox = traffic_light['bbox']\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        # int\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        point1 = (x1, y1)\n",
    "        point2 = (x2, y2)\n",
    "\n",
    "        idx = class_to_idx[traffic_light['class']]\n",
    "        color = 255 - int(255*(idx/len(traffic_class)))\n",
    "\n",
    "        cv2.rectangle(image, point1, point2, color=color, thickness=1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def read_image_for_cache(path):\n",
    "    if CFG.use_gray_scale:\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # image = cv2.resize(image, (CFG.size, CFG.size))\n",
    "\n",
    "    # 効かない\n",
    "    # image = draw_traffic_light(image, path.split('/')[-2])\n",
    "    return (path, image)\n",
    "\n",
    "\n",
    "def make_video_cache(paths):\n",
    "    debug = []\n",
    "    for idx in range(9):\n",
    "        color = 255 - int(255*(idx/9))\n",
    "        debug.append(color)\n",
    "    print(debug)\n",
    "\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        res = pool.imap_unordered(read_image_for_cache, paths)\n",
    "        res = tqdm(res)\n",
    "        res = list(res)\n",
    "\n",
    "    return dict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import ReplayCompose\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    if data == 'train':\n",
    "        # aug = A.Compose(cfg.train_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.train_aug_list)\n",
    "    elif data == 'valid':\n",
    "        # aug = A.Compose(cfg.valid_aug_list)\n",
    "        aug = A.ReplayCompose(cfg.valid_aug_list)\n",
    "\n",
    "    # print(aug)\n",
    "    return aug\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, cfg, labels=None, transform=None):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.base_paths = df['base_path'].values\n",
    "        # self.labels = df[self.cfg.target_col].values\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def read_image_multiframe(self, idx):\n",
    "        base_path = self.base_paths[idx]\n",
    "\n",
    "        images = []\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "\n",
    "            image = self.cfg.video_cache[path]\n",
    "\n",
    "            images.append(image)\n",
    "        return images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.read_image_multiframe(idx)\n",
    "\n",
    "        if self.transform:\n",
    "            # image = self.transform(image=image)['image']\n",
    "            replay = None\n",
    "            images = []\n",
    "            for img in image:\n",
    "                if replay is None:\n",
    "                    sample = self.transform(image=img)\n",
    "                    replay = sample['replay']\n",
    "                else:\n",
    "                    sample = ReplayCompose.replay(replay, image=img)\n",
    "                images.append(sample['image'])\n",
    "\n",
    "            image = torch.concat(images, dim=0)\n",
    "\n",
    "        if self.labels is None:\n",
    "            return image\n",
    "\n",
    "        if self.cfg.objective_cv == 'multiclass':\n",
    "            label = torch.tensor(self.labels[idx]).long()\n",
    "        else:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aug_video(train, cfg, plot_count=1):\n",
    "    transform = CFG.train_aug_list\n",
    "    transform = A.ReplayCompose(transform)\n",
    "\n",
    "    dataset = CustomDataset(\n",
    "        train, CFG, transform=transform)\n",
    "\n",
    "    for i in range(plot_count):\n",
    "        image = dataset.read_image_multiframe(i)\n",
    "\n",
    "        if cfg.use_gray_scale:\n",
    "            image = np.stack(image, axis=2)\n",
    "        else:\n",
    "            image = np.concatenate(image, axis=2)\n",
    "\n",
    "        aug_image = dataset[i]\n",
    "        # torch to numpy\n",
    "        aug_image = aug_image.permute(1, 2, 0).numpy()*255\n",
    "\n",
    "        for frame in range(image.shape[-1]):\n",
    "            if frame % 3 != 0:\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            if cfg.use_gray_scale:\n",
    "                axes[0].imshow(image[..., frame], cmap=\"gray\")\n",
    "                axes[1].imshow(aug_image[..., frame], cmap=\"gray\")\n",
    "            else:\n",
    "                axes[0].imshow(image[..., frame:frame+3].astype(int))\n",
    "                axes[1].imshow(aug_image[..., frame:frame+3].astype(int))\n",
    "            plt.savefig(cfg.figures_dir +\n",
    "                        f'aug_{i}_frame{frame}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "        # self.cfg = cfg\n",
    "\n",
    "        if model_name is None:\n",
    "            model_name = cfg.model_name\n",
    "\n",
    "        print(f'pretrained: {pretrained}')\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=0,\n",
    "            in_chans=cfg.model_in_chans)\n",
    "\n",
    "        # モデルの出力サイズを取得\n",
    "        if hasattr(self.model, 'num_features'):\n",
    "            self.n_features = self.model.num_features  # num_featuresで取得するモデルが多い\n",
    "        elif hasattr(self.model, 'classifier') and hasattr(self.model.classifier, 'in_features'):\n",
    "            self.n_features = self.model.classifier.in_features  # classifierが存在する場合\n",
    "        elif hasattr(self.model, 'fc') and hasattr(self.model.fc, 'in_features'):\n",
    "            self.n_features = self.model.fc.in_features  # fcが存在する場合\n",
    "        else:\n",
    "            raise AttributeError(\"Could not find the output feature size.\")\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "\n",
    "        # nn.Dropout(0.5),\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.target_size)\n",
    "        )\n",
    "\n",
    "    def feature(self, image):\n",
    "\n",
    "        feature = self.model(image)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.feature(image)\n",
    "        output = self.final_fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "\n",
    "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        super(GradualWarmupSchedulerV2, self).__init__(\n",
    "            optimizer, multiplier, total_epoch, after_scheduler)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [\n",
    "                        base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
    "\n",
    "def get_scheduler(cfg, optimizer):\n",
    "    if cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=cfg.factor, patience=cfg.patience, verbose=True, eps=cfg.eps)\n",
    "    elif cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer, T_max=cfg.epochs, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=cfg.T_0, T_mult=1, eta_min=cfg.min_lr, last_epoch=-1)\n",
    "    elif cfg.scheduler == 'GradualWarmupSchedulerV2':\n",
    "        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, cfg.epochs, eta_min=1e-7)\n",
    "        scheduler = GradualWarmupSchedulerV2(\n",
    "            optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def scheduler_step(scheduler, avg_val_loss, epoch):\n",
    "    if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        scheduler.step(avg_val_loss)\n",
    "    elif isinstance(scheduler, CosineAnnealingLR):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "        scheduler.step()\n",
    "    elif isinstance(scheduler, GradualWarmupSchedulerV2):\n",
    "        scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device,\n",
    "             model_ema=None):\n",
    "    \"\"\" 1epoch毎のtrain \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.use_amp)\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    preds_labels = []\n",
    "    start = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with autocast(CFG.use_amp):\n",
    "            y_preds = model(images)\n",
    "\n",
    "            if y_preds.size(1) == 1:\n",
    "                y_preds = y_preds.view(-1)\n",
    "\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).detach().to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.detach().to('cpu').numpy())\n",
    "\n",
    "        preds_labels.append(labels.detach().to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(\n",
    "                              step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(preds_labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        if y_preds.size(1) == 1:\n",
    "            y_preds = y_preds.view(-1)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # binary\n",
    "        if CFG.objective_cv == 'binary':\n",
    "            preds.append(torch.sigmoid(y_preds).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'multiclass':\n",
    "            preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
    "        elif CFG.objective_cv == 'regression':\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(folds, fold):\n",
    "\n",
    "    Logger.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    if CFG.use_alldata:\n",
    "        train_folds = folds.copy().reset_index(drop=True)\n",
    "    else:\n",
    "        train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # train_folds = train_downsampling(train_folds)\n",
    "\n",
    "    train_labels = train_folds[CFG.target_col].values\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "\n",
    "    train_dataset = CustomDataset(\n",
    "        train_folds, CFG, labels=train_labels, transform=get_transforms(data='train', cfg=CFG))\n",
    "    valid_dataset = CustomDataset(\n",
    "        valid_folds, CFG, labels=valid_labels, transform=get_transforms(data='valid', cfg=CFG))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                              )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "\n",
    "    model = CustomModel(CFG, pretrained=CFG.pretrained)\n",
    "    model.to(device)\n",
    "\n",
    "    if CFG.use_ema:\n",
    "        model_ema = ModelEmaV2(model, decay=CFG.ema_decay)\n",
    "    else:\n",
    "        model_ema = None\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr)\n",
    "    scheduler = get_scheduler(CFG, optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    if CFG.objective_cv == 'binary':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif CFG.objective_cv == 'multiclass':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif CFG.objective_cv == 'regression':\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "    if CFG.metric_direction == 'minimize':\n",
    "        best_score = np.inf\n",
    "    elif CFG.metric_direction == 'maximize':\n",
    "        best_score = -1\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    df_score = pd.DataFrame(columns=[\"train_loss\", 'train_score', 'val_loss', 'val_score'])\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss, train_preds, train_labels_epoch = train_fn(fold, train_loader, model,\n",
    "                                                             criterion, optimizer, epoch, scheduler, device, model_ema)\n",
    "        train_score = get_score(train_labels_epoch, train_preds)\n",
    "\n",
    "        # eval\n",
    "        if model_ema is not None:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model_ema.module, criterion, device)\n",
    "        else:\n",
    "            avg_val_loss, valid_preds = valid_fn(\n",
    "                valid_loader, model, criterion, device)\n",
    "\n",
    "        scheduler_step(scheduler, avg_val_loss, epoch)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, valid_preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        # Logger.info(f'Epoch {epoch+1} - avgScore: {avg_score:.4f}')\n",
    "        Logger.info(\n",
    "            f'Epoch {epoch+1} - avg_train_Score: {train_score:.4f} avgScore: {score:.4f}')\n",
    "        \n",
    "        df_score.loc[epoch] = [avg_loss, train_score, avg_val_loss, score]\n",
    "\n",
    "        if CFG.metric_direction == 'minimize':\n",
    "            update_best = score < best_score\n",
    "        elif CFG.metric_direction == 'maximize':\n",
    "            update_best = score > best_score\n",
    "\n",
    "        if update_best:\n",
    "            best_loss = avg_val_loss\n",
    "            best_score = score\n",
    "\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            Logger.info(\n",
    "                f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "\n",
    "            if model_ema is not None:\n",
    "                torch.save({'model': model_ema.module.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "            else:\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'preds': valid_preds},\n",
    "                           CFG.model_dir + f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save({'model': model.state_dict(),\n",
    "                'preds': valid_preds},\n",
    "               CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    \"\"\"\n",
    "    if model_ema is not None:\n",
    "        torch.save({'model': model_ema.module.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "    else:\n",
    "        torch.save({'model': model.state_dict(),\n",
    "                    'preds': valid_preds},\n",
    "                   CFG.model_dir + f'{CFG.model_name}_fold{fold}_last.pth')\n",
    "\n",
    "    check_point = torch.load(\n",
    "        CFG.model_dir + f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth', map_location=torch.device('cpu'))\n",
    "    pred_cols = [f'pred_{i}' for i in range(CFG.target_size)]\n",
    "\n",
    "    check_point_pred = check_point['preds']\n",
    "\n",
    "    # Columns must be same length as key 対策\n",
    "    if check_point_pred.ndim == 1:\n",
    "        check_point_pred = check_point_pred.reshape(-1, CFG.target_size)\n",
    "\n",
    "    print('check_point_pred shape', check_point_pred.shape)\n",
    "    valid_folds[pred_cols] = check_point_pred\n",
    "    return valid_folds, df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train = pd.read_csv(CFG.train_fold_dir + 'train_folds.csv')\n",
    "    train['ori_idx'] = train.index\n",
    "\n",
    "    train['scene'] = train['ID'].str.split('_').str[0]\n",
    "\n",
    "    \"\"\"\n",
    "    if CFG.is_debug:\n",
    "        use_ids = train['scene'].unique()[:100]\n",
    "        train = train[train['scene'].isin(use_ids)].reset_index(drop=True)\n",
    "    \"\"\"\n",
    "\n",
    "    train['base_path'] = CFG.comp_dataset_path + 'images/' + train['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in train['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    # plot_aug_video(train, CFG, plot_count=10)\n",
    "\n",
    "    # train\n",
    "    oof_df = pd.DataFrame()\n",
    "    list_df_score = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold not in CFG.train_folds:\n",
    "            print(f'fold {fold} is skipped')\n",
    "            continue\n",
    "\n",
    "        _oof_df, _df_score = train_fold(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        list_df_score.append(_df_score)\n",
    "        Logger.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "\n",
    "        if CFG.use_holdout or CFG.use_alldata:\n",
    "            break\n",
    "\n",
    "    oof_df = oof_df.sort_values('ori_idx').reset_index(drop=True)\n",
    "\n",
    "    # CV result\n",
    "    Logger.info(\"========== CV ==========\")\n",
    "    score = get_result(oof_df)\n",
    "\n",
    "    # 学習曲線を可視化する\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.grid(alpha=0.1)\n",
    "    ax2.grid(alpha=0.1)\n",
    "    for i, df_score in enumerate(list_df_score):\n",
    "        ax1.plot(df_score['train_score'], label=f'fold {i}')\n",
    "        ax2.plot(df_score['val_score'], label=f'fold {i}')\n",
    "    ax1.set_title('Train Score')\n",
    "    ax2.set_title('Val Score') \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Score')\n",
    "    ax2.set_ylabel('Val Score')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CFG.figures_dir + f'learning_curve_{CFG.exp_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # save result\n",
    "    oof_df.to_csv(CFG.submission_dir + 'oof_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t-0.5.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_320/image_t.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-1.0.png', '../raw/atmacup_18_dataset/images/00066be8e20318869c38c66be466631a_420/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12546e56cb964ddebc49112c2e249c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 2s (remain 19m 50s) Loss: 5.4738(5.4738) Grad: 152745.0781  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 2.4769(3.0294) Grad: 100406.2969  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 2.6597(2.9668) Grad: 105888.2969  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 2.0497(2.0497) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 2.5030(2.4345) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.9668  avg_val_loss: 2.4345  time: 82s\n",
      "Epoch 1 - avg_train_Score: 2.9668 avgScore: 2.4345\n",
      "Epoch 1 - Save Best Score: 2.4345 Model\n",
      "Epoch 1 - Save Best Loss: 2.4345 Model\n",
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 2.2208(2.2208) Grad: 1121492.7500  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.9393(1.9882) Grad: 131648.3906  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.3979(1.9783) Grad: 168941.1250  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 1.5640(1.5640) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.9783  avg_val_loss: 1.8766  time: 81s\n",
      "Epoch 2 - avg_train_Score: 1.9783 avgScore: 1.8766\n",
      "Epoch 2 - Save Best Score: 1.8766 Model\n",
      "Epoch 2 - Save Best Loss: 1.8766 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 2.0217(1.8766) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 8s) Loss: 1.7799(1.7799) Grad: 907624.0000  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.1624(1.5207) Grad: 36400.3672  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.2452(1.4936) Grad: 33310.5195  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 1.1632(1.1632) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.4936  avg_val_loss: 1.1390  time: 81s\n",
      "Epoch 3 - avg_train_Score: 1.4936 avgScore: 1.1390\n",
      "Epoch 3 - Save Best Score: 1.1390 Model\n",
      "Epoch 3 - Save Best Loss: 1.1390 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.1797(1.1390) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 9m 56s) Loss: 1.0712(1.0712) Grad: 398719.5625  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9874(0.9971) Grad: 239610.5312  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9757(0.9934) Grad: 219179.0000  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 1.0215(1.0215) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 0.9934  avg_val_loss: 0.9802  time: 81s\n",
      "Epoch 4 - avg_train_Score: 0.9934 avgScore: 0.9802\n",
      "Epoch 4 - Save Best Score: 0.9802 Model\n",
      "Epoch 4 - Save Best Loss: 0.9802 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.0194(0.9802) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 10m 12s) Loss: 1.0075(1.0075) Grad: 383345.9688  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.0157(0.8982) Grad: 233105.7812  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.8543(0.8976) Grad: 240374.0938  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.9468(0.9468) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.8976  avg_val_loss: 0.9194  time: 81s\n",
      "Epoch 5 - avg_train_Score: 0.8976 avgScore: 0.9194\n",
      "Epoch 5 - Save Best Score: 0.9194 Model\n",
      "Epoch 5 - Save Best Loss: 0.9194 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9414(0.9194) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 9m 26s) Loss: 0.9064(0.9064) Grad: 439721.6562  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9099(0.8392) Grad: 269958.0938  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9951(0.8391) Grad: 271163.5625  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.9266(0.9266) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8391  avg_val_loss: 0.8843  time: 81s\n",
      "Epoch 6 - avg_train_Score: 0.8391 avgScore: 0.8843\n",
      "Epoch 6 - Save Best Score: 0.8843 Model\n",
      "Epoch 6 - Save Best Loss: 0.8843 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8821(0.8843) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 10m 27s) Loss: 0.7544(0.7544) Grad: 353861.3750  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9328(0.7872) Grad: 295092.5000  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7071(0.7857) Grad: 277397.9375  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.8650(0.8650) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7857  avg_val_loss: 0.8585  time: 82s\n",
      "Epoch 7 - avg_train_Score: 0.7857 avgScore: 0.8585\n",
      "Epoch 7 - Save Best Score: 0.8585 Model\n",
      "Epoch 7 - Save Best Loss: 0.8585 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8545(0.8585) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 9m 43s) Loss: 0.7050(0.7050) Grad: 606675.0625  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7432(0.8018) Grad: 118222.0000  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.8562(0.8028) Grad: 178560.9688  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.9070(0.9070) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.8028  avg_val_loss: 0.8402  time: 81s\n",
      "Epoch 8 - avg_train_Score: 0.8028 avgScore: 0.8402\n",
      "Epoch 8 - Save Best Score: 0.8402 Model\n",
      "Epoch 8 - Save Best Loss: 0.8402 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8138(0.8402) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 9m 30s) Loss: 0.8722(0.8722) Grad: 789278.7500  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6374(0.7002) Grad: 194751.9375  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5480(0.6992) Grad: 245735.7812  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.8271(0.8271) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.6992  avg_val_loss: 0.8110  time: 81s\n",
      "Epoch 9 - avg_train_Score: 0.6992 avgScore: 0.8110\n",
      "Epoch 9 - Save Best Score: 0.8110 Model\n",
      "Epoch 9 - Save Best Loss: 0.8110 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7206(0.8110) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 9m 45s) Loss: 0.5632(0.5632) Grad: 522129.9688  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6335(0.6494) Grad: 206072.5312  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5876(0.6494) Grad: 186504.0156  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.8430(0.8430) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6494  avg_val_loss: 0.8020  time: 81s\n",
      "Epoch 10 - avg_train_Score: 0.6494 avgScore: 0.8020\n",
      "Epoch 10 - Save Best Score: 0.8020 Model\n",
      "Epoch 10 - Save Best Loss: 0.8020 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7278(0.8020) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.5618(0.5618) Grad: 436527.5000  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7562(0.6725) Grad: 99206.6250  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5966(0.6745) Grad: 122562.6328  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.8309(0.8309) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.6745  avg_val_loss: 0.7939  time: 81s\n",
      "Epoch 11 - avg_train_Score: 0.6745 avgScore: 0.7939\n",
      "Epoch 11 - Save Best Score: 0.7939 Model\n",
      "Epoch 11 - Save Best Loss: 0.7939 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7275(0.7939) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 9m 54s) Loss: 0.5262(0.5262) Grad: 357731.5312  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7220(0.5987) Grad: 208104.4375  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4660(0.5981) Grad: 125980.4531  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.8014(0.8014) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.5981  avg_val_loss: 0.7751  time: 82s\n",
      "Epoch 12 - avg_train_Score: 0.5981 avgScore: 0.7751\n",
      "Epoch 12 - Save Best Score: 0.7751 Model\n",
      "Epoch 12 - Save Best Loss: 0.7751 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6928(0.7751) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 9m 38s) Loss: 0.5070(0.5070) Grad: 308809.0000  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5305(0.5141) Grad: 325100.3750  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4586(0.5124) Grad: 348884.4375  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7920(0.7920) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.5124  avg_val_loss: 0.7657  time: 81s\n",
      "Epoch 13 - avg_train_Score: 0.5124 avgScore: 0.7657\n",
      "Epoch 13 - Save Best Score: 0.7657 Model\n",
      "Epoch 13 - Save Best Loss: 0.7657 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6798(0.7657) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 9m 49s) Loss: 0.4408(0.4408) Grad: 351566.5000  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4429(0.5300) Grad: 149664.0781  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5422(0.5306) Grad: 204388.2031  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.8053(0.8053) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5306  avg_val_loss: 0.7636  time: 81s\n",
      "Epoch 14 - avg_train_Score: 0.5306 avgScore: 0.7636\n",
      "Epoch 14 - Save Best Score: 0.7636 Model\n",
      "Epoch 14 - Save Best Loss: 0.7636 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6706(0.7636) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 9m 30s) Loss: 0.5978(0.5978) Grad: 338125.4375  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5040(0.5100) Grad: 190718.1250  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5867(0.5112) Grad: 203429.7188  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7954(0.7954) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.5112  avg_val_loss: 0.7622  time: 81s\n",
      "Epoch 15 - avg_train_Score: 0.5112 avgScore: 0.7622\n",
      "Epoch 15 - Save Best Score: 0.7622 Model\n",
      "Epoch 15 - Save Best Loss: 0.7622 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6517(0.7622) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 9m 32s) Loss: 0.5234(0.5234) Grad: 327931.3125  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4272(0.4590) Grad: 364204.5938  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4243(0.4587) Grad: 294684.9688  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.7821(0.7821) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4587  avg_val_loss: 0.7539  time: 81s\n",
      "Epoch 16 - avg_train_Score: 0.4587 avgScore: 0.7539\n",
      "Epoch 16 - Save Best Score: 0.7539 Model\n",
      "Epoch 16 - Save Best Loss: 0.7539 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6676(0.7539) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 9m 54s) Loss: 0.3665(0.3665) Grad: 309889.1250  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4921(0.4644) Grad: 147657.1250  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5681(0.4682) Grad: 247277.7500  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.8031(0.8031) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4682  avg_val_loss: 0.7491  time: 81s\n",
      "Epoch 17 - avg_train_Score: 0.4682 avgScore: 0.7491\n",
      "Epoch 17 - Save Best Score: 0.7491 Model\n",
      "Epoch 17 - Save Best Loss: 0.7491 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6646(0.7491) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 9m 30s) Loss: 0.4501(0.4501) Grad: 376642.0312  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4103(0.4340) Grad: 370884.9062  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4106(0.4346) Grad: 298567.7188  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7793(0.7793) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4346  avg_val_loss: 0.7405  time: 81s\n",
      "Epoch 18 - avg_train_Score: 0.4346 avgScore: 0.7405\n",
      "Epoch 18 - Save Best Score: 0.7405 Model\n",
      "Epoch 18 - Save Best Loss: 0.7405 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6610(0.7405) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 9m 28s) Loss: 0.4047(0.4047) Grad: 372197.5000  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4886(0.4016) Grad: 312669.7188  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4451(0.4028) Grad: 319593.2812  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7745(0.7745) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4028  avg_val_loss: 0.7367  time: 81s\n",
      "Epoch 19 - avg_train_Score: 0.4028 avgScore: 0.7367\n",
      "Epoch 19 - Save Best Score: 0.7367 Model\n",
      "Epoch 19 - Save Best Loss: 0.7367 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6497(0.7367) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 9m 30s) Loss: 0.3857(0.3857) Grad: 286446.3125  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4080(0.4156) Grad: 212747.8750  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4238(0.4188) Grad: 167489.8750  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7720(0.7720) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.4188  avg_val_loss: 0.7367  time: 81s\n",
      "Epoch 20 - avg_train_Score: 0.4188 avgScore: 0.7367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6570(0.7367) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 9m 43s) Loss: 0.5115(0.5115) Grad: 417837.2188  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3923(0.3942) Grad: 318658.0000  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3518(0.3944) Grad: 256307.5938  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 19s) Loss: 0.7557(0.7557) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3944  avg_val_loss: 0.7331  time: 81s\n",
      "Epoch 21 - avg_train_Score: 0.3944 avgScore: 0.7331\n",
      "Epoch 21 - Save Best Score: 0.7331 Model\n",
      "Epoch 21 - Save Best Loss: 0.7331 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6489(0.7331) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 9m 46s) Loss: 0.3547(0.3547) Grad: 263824.1250  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3538(0.3640) Grad: 239193.0938  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3867(0.3648) Grad: 265979.4375  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7594(0.7594) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3648  avg_val_loss: 0.7308  time: 81s\n",
      "Epoch 22 - avg_train_Score: 0.3648 avgScore: 0.7308\n",
      "Epoch 22 - Save Best Score: 0.7308 Model\n",
      "Epoch 22 - Save Best Loss: 0.7308 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6522(0.7308) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 0.3381(0.3381) Grad: 276878.0938  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3465(0.3519) Grad: 266588.5000  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3022(0.3515) Grad: 269257.6250  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7479(0.7479) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3515  avg_val_loss: 0.7293  time: 82s\n",
      "Epoch 23 - avg_train_Score: 0.3515 avgScore: 0.7293\n",
      "Epoch 23 - Save Best Score: 0.7293 Model\n",
      "Epoch 23 - Save Best Loss: 0.7293 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6419(0.7293) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 9m 36s) Loss: 0.3418(0.3418) Grad: 257677.0312  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3046(0.3424) Grad: 276205.9062  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2699(0.3421) Grad: 209160.1562  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7514(0.7514) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3421  avg_val_loss: 0.7269  time: 81s\n",
      "Epoch 24 - avg_train_Score: 0.3421 avgScore: 0.7269\n",
      "Epoch 24 - Save Best Score: 0.7269 Model\n",
      "Epoch 24 - Save Best Loss: 0.7269 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6490(0.7269) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 9m 32s) Loss: 0.3526(0.3526) Grad: 313732.8438  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3858(0.3314) Grad: 325756.4062  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2981(0.3311) Grad: 254395.3594  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.7455(0.7455) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3311  avg_val_loss: 0.7249  time: 81s\n",
      "Epoch 25 - avg_train_Score: 0.3311 avgScore: 0.7249\n",
      "Epoch 25 - Save Best Score: 0.7249 Model\n",
      "Epoch 25 - Save Best Loss: 0.7249 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6579(0.7249) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 9m 48s) Loss: 0.3342(0.3342) Grad: 266091.1562  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3400(0.3205) Grad: 338831.8750  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3305(0.3200) Grad: 285820.8125  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7536(0.7536) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3200  avg_val_loss: 0.7238  time: 81s\n",
      "Epoch 26 - avg_train_Score: 0.3200 avgScore: 0.7238\n",
      "Epoch 26 - Save Best Score: 0.7238 Model\n",
      "Epoch 26 - Save Best Loss: 0.7238 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6518(0.7238) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 9m 31s) Loss: 0.2610(0.2610) Grad: 259976.7031  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3334(0.3086) Grad: 257687.7344  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2754(0.3088) Grad: 249181.0156  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 46s) Loss: 0.7471(0.7471) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.3088  avg_val_loss: 0.7228  time: 81s\n",
      "Epoch 27 - avg_train_Score: 0.3088 avgScore: 0.7228\n",
      "Epoch 27 - Save Best Score: 0.7228 Model\n",
      "Epoch 27 - Save Best Loss: 0.7228 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6501(0.7228) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 13m 24s) Loss: 0.3911(0.3911) Grad: 388932.1875  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3137(0.2977) Grad: 281411.2812  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.3571(0.2972) Grad: 300091.2812  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7503(0.7503) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.2972  avg_val_loss: 0.7235  time: 82s\n",
      "Epoch 28 - avg_train_Score: 0.2972 avgScore: 0.7235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6549(0.7235) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 9m 38s) Loss: 0.2925(0.2925) Grad: 277053.1250  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3792(0.2881) Grad: 287576.3438  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2345(0.2880) Grad: 216317.5312  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7469(0.7469) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2880  avg_val_loss: 0.7209  time: 81s\n",
      "Epoch 29 - avg_train_Score: 0.2880 avgScore: 0.7209\n",
      "Epoch 29 - Save Best Score: 0.7209 Model\n",
      "Epoch 29 - Save Best Loss: 0.7209 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6508(0.7209) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 9m 50s) Loss: 0.2702(0.2702) Grad: 229552.6250  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3226(0.2795) Grad: 129668.4688  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2895(0.2807) Grad: 129717.6719  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7400(0.7400) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2807  avg_val_loss: 0.7203  time: 81s\n",
      "Epoch 30 - avg_train_Score: 0.2807 avgScore: 0.7203\n",
      "Epoch 30 - Save Best Score: 0.7203 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6453(0.7203) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Save Best Loss: 0.7203 Model\n",
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 9m 41s) Loss: 0.2948(0.2948) Grad: 243886.2500  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2747(0.2716) Grad: 270135.6875  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2662(0.2715) Grad: 244370.7344  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7363(0.7363) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6492(0.7197) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2715  avg_val_loss: 0.7197  time: 81s\n",
      "Epoch 31 - avg_train_Score: 0.2715 avgScore: 0.7197\n",
      "Epoch 31 - Save Best Score: 0.7197 Model\n",
      "Epoch 31 - Save Best Loss: 0.7197 Model\n",
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 10m 7s) Loss: 0.2836(0.2836) Grad: 239869.6562  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3086(0.2638) Grad: 230533.6562  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2373(0.2633) Grad: 222938.1406  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.7289(0.7289) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2633  avg_val_loss: 0.7185  time: 82s\n",
      "Epoch 32 - avg_train_Score: 0.2633 avgScore: 0.7185\n",
      "Epoch 32 - Save Best Score: 0.7185 Model\n",
      "Epoch 32 - Save Best Loss: 0.7185 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6527(0.7185) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 9m 35s) Loss: 0.2264(0.2264) Grad: 267935.2188  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2501(0.2544) Grad: 246156.7656  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2336(0.2546) Grad: 209510.9531  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7404(0.7404) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2546  avg_val_loss: 0.7180  time: 81s\n",
      "Epoch 33 - avg_train_Score: 0.2546 avgScore: 0.7180\n",
      "Epoch 33 - Save Best Score: 0.7180 Model\n",
      "Epoch 33 - Save Best Loss: 0.7180 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6498(0.7180) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 10m 1s) Loss: 0.2498(0.2498) Grad: 239957.7031  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2405(0.2484) Grad: 265631.5625  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2462(0.2488) Grad: 241825.9219  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7374(0.7374) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2488  avg_val_loss: 0.7169  time: 82s\n",
      "Epoch 34 - avg_train_Score: 0.2488 avgScore: 0.7169\n",
      "Epoch 34 - Save Best Score: 0.7169 Model\n",
      "Epoch 34 - Save Best Loss: 0.7169 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6509(0.7169) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 4s) Loss: 0.2710(0.2710) Grad: 250604.5781  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2411(0.2424) Grad: 259460.1875  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2371(0.2422) Grad: 208104.5625  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7390(0.7390) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6540(0.7169) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2422  avg_val_loss: 0.7169  time: 81s\n",
      "Epoch 35 - avg_train_Score: 0.2422 avgScore: 0.7169\n",
      "Epoch 35 - Save Best Score: 0.7169 Model\n",
      "Epoch 35 - Save Best Loss: 0.7169 Model\n",
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 9m 32s) Loss: 0.2833(0.2833) Grad: 299121.3438  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.1756(0.2376) Grad: 210929.7344  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2118(0.2373) Grad: 241502.7812  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 20s) Loss: 0.7419(0.7419) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2373  avg_val_loss: 0.7167  time: 81s\n",
      "Epoch 36 - avg_train_Score: 0.2373 avgScore: 0.7167\n",
      "Epoch 36 - Save Best Score: 0.7167 Model\n",
      "Epoch 36 - Save Best Loss: 0.7167 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6518(0.7167) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 9m 51s) Loss: 0.2270(0.2270) Grad: 204809.0000  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2963(0.2343) Grad: 224873.9375  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1962(0.2339) Grad: 291030.6562  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7423(0.7423) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2339  avg_val_loss: 0.7161  time: 82s\n",
      "Epoch 37 - avg_train_Score: 0.2339 avgScore: 0.7161\n",
      "Epoch 37 - Save Best Score: 0.7161 Model\n",
      "Epoch 37 - Save Best Loss: 0.7161 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6496(0.7161) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 10m 0s) Loss: 0.2949(0.2949) Grad: 256380.2031  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2401(0.2295) Grad: 276540.3438  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2209(0.2296) Grad: 221781.9531  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7409(0.7409) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2296  avg_val_loss: 0.7164  time: 81s\n",
      "Epoch 38 - avg_train_Score: 0.2296 avgScore: 0.7164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6509(0.7164) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 9m 57s) Loss: 0.2183(0.2183) Grad: 203898.9375  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2219(0.2276) Grad: 230589.9688  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2473(0.2272) Grad: 236551.0938  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7374(0.7374) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2272  avg_val_loss: 0.7166  time: 81s\n",
      "Epoch 39 - avg_train_Score: 0.2272 avgScore: 0.7166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6534(0.7166) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 9m 53s) Loss: 0.2368(0.2368) Grad: 230578.4375  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2875(0.2265) Grad: 121583.6797  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2419(0.2267) Grad: 99366.0000  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 21s) Loss: 0.7381(0.7381) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2267  avg_val_loss: 0.7160  time: 81s\n",
      "Epoch 40 - avg_train_Score: 0.2267 avgScore: 0.7160\n",
      "Epoch 40 - Save Best Score: 0.7160 Model\n",
      "Epoch 40 - Save Best Loss: 0.7160 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6513(0.7160) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 0 result ==========\n",
      "score: 0.7160\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8675, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 30s) Loss: 6.3866(6.3866) Grad: 195029.8906  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 2.6612(3.1521) Grad: 96414.2500  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 2.2993(3.0871) Grad: 135119.0156  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 2.6699(2.6699) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 3.0871  avg_val_loss: 2.4627  time: 81s\n",
      "Epoch 1 - avg_train_Score: 3.0871 avgScore: 2.4627\n",
      "Epoch 1 - Save Best Score: 2.4627 Model\n",
      "Epoch 1 - Save Best Loss: 2.4627 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.7218(2.4627) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 9m 56s) Loss: 1.9826(1.9826) Grad: inf  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 2.0495(2.0443) Grad: 116622.8906  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.9466(2.0244) Grad: 126414.2266  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 1.9835(1.9835) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 2.0244  avg_val_loss: 1.8426  time: 82s\n",
      "Epoch 2 - avg_train_Score: 2.0244 avgScore: 1.8426\n",
      "Epoch 2 - Save Best Score: 1.8426 Model\n",
      "Epoch 2 - Save Best Loss: 1.8426 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.3550(1.8426) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 10m 39s) Loss: 1.4869(1.4869) Grad: inf  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.3856(1.4833) Grad: 48428.8984  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.2859(1.4601) Grad: 49516.9336  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 1.2363(1.2363) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.4601  avg_val_loss: 1.0952  time: 81s\n",
      "Epoch 3 - avg_train_Score: 1.4601 avgScore: 1.0952\n",
      "Epoch 3 - Save Best Score: 1.0952 Model\n",
      "Epoch 3 - Save Best Loss: 1.0952 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.0243(1.0952) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 10m 8s) Loss: 1.3226(1.3226) Grad: 578431.0625  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.0703(1.0124) Grad: 141559.4688  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.1251(1.0087) Grad: 196694.3750  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.9980(0.9980) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0087  avg_val_loss: 0.9663  time: 81s\n",
      "Epoch 4 - avg_train_Score: 1.0087 avgScore: 0.9663\n",
      "Epoch 4 - Save Best Score: 0.9663 Model\n",
      "Epoch 4 - Save Best Loss: 0.9663 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8910(0.9663) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 10m 11s) Loss: 0.8895(0.8895) Grad: 457776.7812  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9424(0.8893) Grad: 305142.8438  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7182(0.8873) Grad: 198260.7969  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.9160(0.9160) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.8873  avg_val_loss: 0.9008  time: 81s\n",
      "Epoch 5 - avg_train_Score: 0.8873 avgScore: 0.9008\n",
      "Epoch 5 - Save Best Score: 0.9008 Model\n",
      "Epoch 5 - Save Best Loss: 0.9008 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8638(0.9008) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 10m 21s) Loss: 0.9290(0.9290) Grad: 443351.6875  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8672(0.8287) Grad: 273427.1250  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9172(0.8280) Grad: 243956.3750  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.8526(0.8526) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8280  avg_val_loss: 0.8677  time: 82s\n",
      "Epoch 6 - avg_train_Score: 0.8280 avgScore: 0.8677\n",
      "Epoch 6 - Save Best Score: 0.8677 Model\n",
      "Epoch 6 - Save Best Loss: 0.8677 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8396(0.8677) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 9m 52s) Loss: 0.8189(0.8189) Grad: inf  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8247(0.7763) Grad: 230847.5000  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7625(0.7770) Grad: 317058.5938  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.8303(0.8303) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.7770  avg_val_loss: 0.8444  time: 81s\n",
      "Epoch 7 - avg_train_Score: 0.7770 avgScore: 0.8444\n",
      "Epoch 7 - Save Best Score: 0.8444 Model\n",
      "Epoch 7 - Save Best Loss: 0.8444 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8113(0.8444) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 9m 59s) Loss: 0.8196(0.8196) Grad: 499205.0625  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8053(0.7366) Grad: 228076.9219  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7452(0.7378) Grad: 222806.9219  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.8092(0.8092) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7378  avg_val_loss: 0.8274  time: 81s\n",
      "Epoch 8 - avg_train_Score: 0.7378 avgScore: 0.8274\n",
      "Epoch 8 - Save Best Score: 0.8274 Model\n",
      "Epoch 8 - Save Best Loss: 0.8274 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8377(0.8274) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 9m 53s) Loss: 0.7756(0.7756) Grad: 472109.6250  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5114(0.7034) Grad: 213563.8281  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7562(0.7037) Grad: 272023.5938  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7811(0.7811) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.7037  avg_val_loss: 0.8129  time: 81s\n",
      "Epoch 9 - avg_train_Score: 0.7037 avgScore: 0.8129\n",
      "Epoch 9 - Save Best Score: 0.8129 Model\n",
      "Epoch 9 - Save Best Loss: 0.8129 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8051(0.8129) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 9m 47s) Loss: 0.6537(0.6537) Grad: 475110.6250  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5283(0.6692) Grad: 218245.0000  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7282(0.6709) Grad: 254910.7188  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.8024(0.8024) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6709  avg_val_loss: 0.8003  time: 81s\n",
      "Epoch 10 - avg_train_Score: 0.6709 avgScore: 0.8003\n",
      "Epoch 10 - Save Best Score: 0.8003 Model\n",
      "Epoch 10 - Save Best Loss: 0.8003 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7984(0.8003) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 9m 53s) Loss: 0.7796(0.7796) Grad: 592860.9375  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6557(0.6385) Grad: 245082.1250  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5748(0.6383) Grad: 284341.4375  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7656(0.7656) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.6383  avg_val_loss: 0.7919  time: 82s\n",
      "Epoch 11 - avg_train_Score: 0.6383 avgScore: 0.7919\n",
      "Epoch 11 - Save Best Score: 0.7919 Model\n",
      "Epoch 11 - Save Best Loss: 0.7919 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7822(0.7919) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 10m 8s) Loss: 0.6900(0.6900) Grad: 465840.4375  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7255(0.6082) Grad: 247022.9375  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5946(0.6095) Grad: 306546.3125  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7617(0.7617) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.6095  avg_val_loss: 0.7892  time: 81s\n",
      "Epoch 12 - avg_train_Score: 0.6095 avgScore: 0.7892\n",
      "Epoch 12 - Save Best Score: 0.7892 Model\n",
      "Epoch 12 - Save Best Loss: 0.7892 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7865(0.7892) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 10m 53s) Loss: 0.6490(0.6490) Grad: 480588.0625  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5933(0.5914) Grad: 248250.9219  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5046(0.5905) Grad: 211834.6562  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7655(0.7655) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.5905  avg_val_loss: 0.7813  time: 81s\n",
      "Epoch 13 - avg_train_Score: 0.5905 avgScore: 0.7813\n",
      "Epoch 13 - Save Best Score: 0.7813 Model\n",
      "Epoch 13 - Save Best Loss: 0.7813 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7835(0.7813) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 10m 14s) Loss: 0.5503(0.5503) Grad: 420237.9688  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7120(0.5630) Grad: 214846.5156  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5062(0.5618) Grad: 184944.5000  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7518(0.7518) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5618  avg_val_loss: 0.7706  time: 82s\n",
      "Epoch 14 - avg_train_Score: 0.5618 avgScore: 0.7706\n",
      "Epoch 14 - Save Best Score: 0.7706 Model\n",
      "Epoch 14 - Save Best Loss: 0.7706 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7470(0.7706) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 30s) Loss: 0.5430(0.5430) Grad: 540626.0000  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6576(0.5112) Grad: 267277.9375  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4971(0.5134) Grad: 224028.3750  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7664(0.7664) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7483(0.7602) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.5134  avg_val_loss: 0.7602  time: 81s\n",
      "Epoch 15 - avg_train_Score: 0.5134 avgScore: 0.7602\n",
      "Epoch 15 - Save Best Score: 0.7602 Model\n",
      "Epoch 15 - Save Best Loss: 0.7602 Model\n",
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.5817(0.5817) Grad: 322553.5625  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5355(0.4711) Grad: 302661.2500  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4516(0.4713) Grad: 364352.0625  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7510(0.7510) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4713  avg_val_loss: 0.7529  time: 82s\n",
      "Epoch 16 - avg_train_Score: 0.4713 avgScore: 0.7529\n",
      "Epoch 16 - Save Best Score: 0.7529 Model\n",
      "Epoch 16 - Save Best Loss: 0.7529 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7510(0.7529) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 15s) Loss: 0.4497(0.4497) Grad: 314423.2500  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4659(0.4795) Grad: 198546.6406  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4723(0.4815) Grad: 212961.2344  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7528(0.7528) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4815  avg_val_loss: 0.7529  time: 81s\n",
      "Epoch 17 - avg_train_Score: 0.4815 avgScore: 0.7529\n",
      "Epoch 17 - Save Best Score: 0.7529 Model\n",
      "Epoch 17 - Save Best Loss: 0.7529 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7350(0.7529) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 10m 2s) Loss: 0.4590(0.4590) Grad: 343676.8438  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4866(0.4663) Grad: 157722.7031  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4878(0.4666) Grad: 178766.0000  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7179(0.7179) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4666  avg_val_loss: 0.7522  time: 81s\n",
      "Epoch 18 - avg_train_Score: 0.4666 avgScore: 0.7522\n",
      "Epoch 18 - Save Best Score: 0.7522 Model\n",
      "Epoch 18 - Save Best Loss: 0.7522 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7224(0.7522) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 9m 57s) Loss: 0.4734(0.4734) Grad: 413656.2500  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4053(0.4188) Grad: 352211.5938  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4450(0.4199) Grad: 352936.5625  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7426(0.7426) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4199  avg_val_loss: 0.7462  time: 81s\n",
      "Epoch 19 - avg_train_Score: 0.4199 avgScore: 0.7462\n",
      "Epoch 19 - Save Best Score: 0.7462 Model\n",
      "Epoch 19 - Save Best Loss: 0.7462 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7363(0.7462) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 10m 10s) Loss: 0.3672(0.3672) Grad: 312384.7500  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3891(0.4254) Grad: 162884.6250  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4428(0.4261) Grad: 162898.0938  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7371(0.7371) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.4261  avg_val_loss: 0.7467  time: 82s\n",
      "Epoch 20 - avg_train_Score: 0.4261 avgScore: 0.7467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7320(0.7467) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 10m 29s) Loss: 0.4376(0.4376) Grad: 308773.2188  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3839(0.3964) Grad: 317393.1250  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3755(0.3955) Grad: 286461.7500  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7195(0.7195) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3955  avg_val_loss: 0.7412  time: 82s\n",
      "Epoch 21 - avg_train_Score: 0.3955 avgScore: 0.7412\n",
      "Epoch 21 - Save Best Score: 0.7412 Model\n",
      "Epoch 21 - Save Best Loss: 0.7412 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7302(0.7412) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 10m 50s) Loss: 0.4217(0.4217) Grad: 289625.1562  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3662(0.3653) Grad: 298917.4062  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4285(0.3658) Grad: 284237.5000  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7262(0.7262) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3658  avg_val_loss: 0.7372  time: 82s\n",
      "Epoch 22 - avg_train_Score: 0.3658 avgScore: 0.7372\n",
      "Epoch 22 - Save Best Score: 0.7372 Model\n",
      "Epoch 22 - Save Best Loss: 0.7372 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7131(0.7372) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 10m 25s) Loss: 0.4014(0.4014) Grad: 328850.7188  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2798(0.3559) Grad: 269823.4688  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3127(0.3558) Grad: 282343.5938  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.7276(0.7276) \n",
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7249(0.7348) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3558  avg_val_loss: 0.7348  time: 82s\n",
      "Epoch 23 - avg_train_Score: 0.3558 avgScore: 0.7348\n",
      "Epoch 23 - Save Best Score: 0.7348 Model\n",
      "Epoch 23 - Save Best Loss: 0.7348 Model\n",
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 7s) Loss: 0.3967(0.3967) Grad: 299639.9688  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.3697(0.3485) Grad: 309294.7812  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2826(0.3477) Grad: 268211.9062  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7270(0.7270) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3477  avg_val_loss: 0.7353  time: 82s\n",
      "Epoch 24 - avg_train_Score: 0.3477 avgScore: 0.7353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7173(0.7353) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 10m 5s) Loss: 0.3328(0.3328) Grad: 313079.5625  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3254(0.3349) Grad: 269441.3125  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2768(0.3346) Grad: 220525.4375  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7240(0.7240) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3346  avg_val_loss: 0.7340  time: 82s\n",
      "Epoch 25 - avg_train_Score: 0.3346 avgScore: 0.7340\n",
      "Epoch 25 - Save Best Score: 0.7340 Model\n",
      "Epoch 25 - Save Best Loss: 0.7340 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7244(0.7340) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 10m 1s) Loss: 0.3452(0.3452) Grad: 276294.8750  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4292(0.3252) Grad: 307968.6250  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3172(0.3255) Grad: 288939.7188  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7285(0.7285) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3255  avg_val_loss: 0.7306  time: 82s\n",
      "Epoch 26 - avg_train_Score: 0.3255 avgScore: 0.7306\n",
      "Epoch 26 - Save Best Score: 0.7306 Model\n",
      "Epoch 26 - Save Best Loss: 0.7306 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7264(0.7306) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 46s) Loss: 0.3800(0.3800) Grad: 268793.4375  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2916(0.3147) Grad: 257306.5625  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2755(0.3150) Grad: 241072.1875  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7246(0.7246) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.3150  avg_val_loss: 0.7278  time: 82s\n",
      "Epoch 27 - avg_train_Score: 0.3150 avgScore: 0.7278\n",
      "Epoch 27 - Save Best Score: 0.7278 Model\n",
      "Epoch 27 - Save Best Loss: 0.7278 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7372(0.7278) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 10m 5s) Loss: 0.3279(0.3279) Grad: 304059.4688  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3045(0.3023) Grad: 370106.8125  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3293(0.3026) Grad: 314822.7812  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7258(0.7258) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.3026  avg_val_loss: 0.7299  time: 82s\n",
      "Epoch 28 - avg_train_Score: 0.3026 avgScore: 0.7299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7457(0.7299) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 10m 4s) Loss: 0.3260(0.3260) Grad: 275254.2812  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2752(0.3087) Grad: 132701.8594  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3410(0.3083) Grad: 205727.9844  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7185(0.7185) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.3083  avg_val_loss: 0.7306  time: 82s\n",
      "Epoch 29 - avg_train_Score: 0.3083 avgScore: 0.7306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7162(0.7306) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 10m 29s) Loss: 0.3699(0.3699) Grad: 379636.6562  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2876(0.2882) Grad: 279338.8438  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2493(0.2883) Grad: 211166.6406  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7262(0.7262) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2883  avg_val_loss: 0.7285  time: 81s\n",
      "Epoch 30 - avg_train_Score: 0.2883 avgScore: 0.7285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7147(0.7285) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 10m 31s) Loss: 0.2827(0.2827) Grad: 286398.1562  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2562(0.2749) Grad: 231956.6875  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2365(0.2747) Grad: 259356.7500  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7321(0.7321) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2747  avg_val_loss: 0.7275  time: 82s\n",
      "Epoch 31 - avg_train_Score: 0.2747 avgScore: 0.7275\n",
      "Epoch 31 - Save Best Score: 0.7275 Model\n",
      "Epoch 31 - Save Best Loss: 0.7275 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7136(0.7275) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 9m 49s) Loss: 0.2700(0.2700) Grad: 241212.9219  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2801(0.2666) Grad: 305031.7188  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2710(0.2668) Grad: 328434.4062  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7334(0.7334) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2668  avg_val_loss: 0.7271  time: 81s\n",
      "Epoch 32 - avg_train_Score: 0.2668 avgScore: 0.7271\n",
      "Epoch 32 - Save Best Score: 0.7271 Model\n",
      "Epoch 32 - Save Best Loss: 0.7271 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7152(0.7271) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 10m 12s) Loss: 0.2345(0.2345) Grad: 237508.0625  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3071(0.2588) Grad: 242049.7500  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2283(0.2584) Grad: 263802.1875  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7320(0.7320) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2584  avg_val_loss: 0.7266  time: 81s\n",
      "Epoch 33 - avg_train_Score: 0.2584 avgScore: 0.7266\n",
      "Epoch 33 - Save Best Score: 0.7266 Model\n",
      "Epoch 33 - Save Best Loss: 0.7266 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7209(0.7266) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 10m 41s) Loss: 0.2585(0.2585) Grad: 223884.8906  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2888(0.2529) Grad: 311061.9062  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2472(0.2525) Grad: 254321.7031  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7303(0.7303) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2525  avg_val_loss: 0.7255  time: 81s\n",
      "Epoch 34 - avg_train_Score: 0.2525 avgScore: 0.7255\n",
      "Epoch 34 - Save Best Score: 0.7255 Model\n",
      "Epoch 34 - Save Best Loss: 0.7255 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7278(0.7255) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.2627(0.2627) Grad: 214573.2500  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.1854(0.2470) Grad: 205641.1094  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2284(0.2470) Grad: 246271.2656  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7298(0.7298) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2470  avg_val_loss: 0.7258  time: 81s\n",
      "Epoch 35 - avg_train_Score: 0.2470 avgScore: 0.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7241(0.7258) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 12m 3s) Loss: 0.2852(0.2852) Grad: 242010.5781  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.2189(0.2422) Grad: 192923.3906  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2608(0.2428) Grad: 288567.1562  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7272(0.7272) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2428  avg_val_loss: 0.7239  time: 82s\n",
      "Epoch 36 - avg_train_Score: 0.2428 avgScore: 0.7239\n",
      "Epoch 36 - Save Best Score: 0.7239 Model\n",
      "Epoch 36 - Save Best Loss: 0.7239 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7209(0.7239) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 2s) Loss: 0.1938(0.1938) Grad: 220874.2656  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2508(0.2386) Grad: 219733.2969  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2081(0.2384) Grad: 252550.9688  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7247(0.7247) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2384  avg_val_loss: 0.7240  time: 82s\n",
      "Epoch 37 - avg_train_Score: 0.2384 avgScore: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7172(0.7240) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 10m 2s) Loss: 0.2074(0.2074) Grad: 221265.9062  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2300(0.2360) Grad: 238681.9062  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2389(0.2359) Grad: 277874.1562  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.7248(0.7248) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2359  avg_val_loss: 0.7234  time: 81s\n",
      "Epoch 38 - avg_train_Score: 0.2359 avgScore: 0.7234\n",
      "Epoch 38 - Save Best Score: 0.7234 Model\n",
      "Epoch 38 - Save Best Loss: 0.7234 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7186(0.7234) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 10m 5s) Loss: 0.2427(0.2427) Grad: 329055.0625  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2302(0.2323) Grad: 204337.2344  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2303(0.2331) Grad: 218544.6250  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7247(0.7247) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2331  avg_val_loss: 0.7235  time: 81s\n",
      "Epoch 39 - avg_train_Score: 0.2331 avgScore: 0.7235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7201(0.7235) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 10m 7s) Loss: 0.2631(0.2631) Grad: 246328.9375  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2248(0.2313) Grad: 225908.1562  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2347(0.2316) Grad: 239958.3438  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7251(0.7251) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2316  avg_val_loss: 0.7237  time: 82s\n",
      "Epoch 40 - avg_train_Score: 0.2316 avgScore: 0.7237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7204(0.7237) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 1 result ==========\n",
      "score: 0.7237\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 4.5788(4.5788) Grad: 152290.8281  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 2.4102(3.0057) Grad: 81914.0703  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 2.5320(2.9495) Grad: 50214.2031  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 2.0493(2.0493) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.9495  avg_val_loss: 2.3169  time: 81s\n",
      "Epoch 1 - avg_train_Score: 2.9495 avgScore: 2.3169\n",
      "Epoch 1 - Save Best Score: 2.3169 Model\n",
      "Epoch 1 - Save Best Loss: 2.3169 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 2.2411(2.3169) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 16s) Loss: 2.2594(2.2594) Grad: inf  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.8148(1.9481) Grad: 41840.4297  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.7866(1.9284) Grad: 36173.2578  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 1.3949(1.3949) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.9284  avg_val_loss: 1.6988  time: 82s\n",
      "Epoch 2 - avg_train_Score: 1.9284 avgScore: 1.6988\n",
      "Epoch 2 - Save Best Score: 1.6988 Model\n",
      "Epoch 2 - Save Best Loss: 1.6988 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.6267(1.6988) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 9m 57s) Loss: 1.7837(1.7837) Grad: inf  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.3057(1.5345) Grad: 17554.6602  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.1482(1.5105) Grad: 23900.8965  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.9212(0.9212) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.5105  avg_val_loss: 1.1070  time: 81s\n",
      "Epoch 3 - avg_train_Score: 1.5105 avgScore: 1.1070\n",
      "Epoch 3 - Save Best Score: 1.1070 Model\n",
      "Epoch 3 - Save Best Loss: 1.1070 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.1017(1.1070) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 10m 6s) Loss: 1.1899(1.1899) Grad: inf  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8682(1.0443) Grad: 107430.0469  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9985(1.0379) Grad: 180304.7812  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.8871(0.8871) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0379  avg_val_loss: 0.9944  time: 81s\n",
      "Epoch 4 - avg_train_Score: 1.0379 avgScore: 0.9944\n",
      "Epoch 4 - Save Best Score: 0.9944 Model\n",
      "Epoch 4 - Save Best Loss: 0.9944 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9142(0.9944) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 1.0285(1.0285) Grad: 656068.4375  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.0751(0.9399) Grad: 309700.4688  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7739(0.9403) Grad: 166301.4531  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.8621(0.8621) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9403  avg_val_loss: 0.9557  time: 82s\n",
      "Epoch 5 - avg_train_Score: 0.9403 avgScore: 0.9557\n",
      "Epoch 5 - Save Best Score: 0.9557 Model\n",
      "Epoch 5 - Save Best Loss: 0.9557 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8843(0.9557) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 10m 18s) Loss: 0.9005(0.9005) Grad: 507375.1250  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9737(0.9171) Grad: 129563.8984  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 1.0288(0.9177) Grad: 139296.3438  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.8308(0.8308) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.9177  avg_val_loss: 0.9255  time: 82s\n",
      "Epoch 6 - avg_train_Score: 0.9177 avgScore: 0.9255\n",
      "Epoch 6 - Save Best Score: 0.9255 Model\n",
      "Epoch 6 - Save Best Loss: 0.9255 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8588(0.9255) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 10m 4s) Loss: 0.8107(0.8107) Grad: 446502.3438  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.0304(0.8324) Grad: 131584.7188  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.8103(0.8341) Grad: 185459.2812  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.8091(0.8091) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8341  avg_val_loss: 0.8813  time: 81s\n",
      "Epoch 7 - avg_train_Score: 0.8341 avgScore: 0.8813\n",
      "Epoch 7 - Save Best Score: 0.8813 Model\n",
      "Epoch 7 - Save Best Loss: 0.8813 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8241(0.8813) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 10m 30s) Loss: 0.8255(0.8255) Grad: 553959.6875  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7055(0.7728) Grad: 279716.1562  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7158(0.7723) Grad: 293602.3438  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7611(0.7611) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7723  avg_val_loss: 0.8513  time: 82s\n",
      "Epoch 8 - avg_train_Score: 0.7723 avgScore: 0.8513\n",
      "Epoch 8 - Save Best Score: 0.8513 Model\n",
      "Epoch 8 - Save Best Loss: 0.8513 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8117(0.8513) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 38s) Loss: 0.5980(0.5980) Grad: inf  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.6695(0.7738) Grad: 127371.6406  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.8034(0.7770) Grad: 122035.7344  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7631(0.7631) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.7770  avg_val_loss: 0.8411  time: 82s\n",
      "Epoch 9 - avg_train_Score: 0.7770 avgScore: 0.8411\n",
      "Epoch 9 - Save Best Score: 0.8411 Model\n",
      "Epoch 9 - Save Best Loss: 0.8411 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7852(0.8411) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 11m 11s) Loss: 0.7274(0.7274) Grad: 561278.1250  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6295(0.6976) Grad: 143331.7500  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.9801(0.7012) Grad: 141410.9531  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7453(0.7453) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.7012  avg_val_loss: 0.8192  time: 82s\n",
      "Epoch 10 - avg_train_Score: 0.7012 avgScore: 0.8192\n",
      "Epoch 10 - Save Best Score: 0.8192 Model\n",
      "Epoch 10 - Save Best Loss: 0.8192 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7817(0.8192) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.6251(0.6251) Grad: 592494.1250  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8337(0.7108) Grad: 191000.2031  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6344(0.7108) Grad: 163067.2812  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7350(0.7350) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.7108  avg_val_loss: 0.8061  time: 81s\n",
      "Epoch 11 - avg_train_Score: 0.7108 avgScore: 0.8061\n",
      "Epoch 11 - Save Best Score: 0.8061 Model\n",
      "Epoch 11 - Save Best Loss: 0.8061 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7611(0.8061) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 10m 22s) Loss: 0.7162(0.7162) Grad: 671118.8750  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5841(0.6383) Grad: 266266.5625  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6380(0.6394) Grad: 236741.9375  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7087(0.7087) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.6394  avg_val_loss: 0.7881  time: 81s\n",
      "Epoch 12 - avg_train_Score: 0.6394 avgScore: 0.7881\n",
      "Epoch 12 - Save Best Score: 0.7881 Model\n",
      "Epoch 12 - Save Best Loss: 0.7881 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7518(0.7881) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 11m 3s) Loss: 0.5861(0.5861) Grad: 513968.5000  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6207(0.6060) Grad: 218815.3750  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6494(0.6070) Grad: 207240.9688  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7185(0.7185) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.6070  avg_val_loss: 0.7806  time: 82s\n",
      "Epoch 13 - avg_train_Score: 0.6070 avgScore: 0.7806\n",
      "Epoch 13 - Save Best Score: 0.7806 Model\n",
      "Epoch 13 - Save Best Loss: 0.7806 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7531(0.7806) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 10m 47s) Loss: 0.6272(0.6272) Grad: 457900.8750  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5908(0.5760) Grad: 237698.6719  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.4692(0.5796) Grad: 189545.7500  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.7148(0.7148) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5796  avg_val_loss: 0.7795  time: 82s\n",
      "Epoch 14 - avg_train_Score: 0.5796 avgScore: 0.7795\n",
      "Epoch 14 - Save Best Score: 0.7795 Model\n",
      "Epoch 14 - Save Best Loss: 0.7795 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7318(0.7795) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 11m 6s) Loss: 0.5865(0.5865) Grad: 548783.6875  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5747(0.5603) Grad: 268982.3125  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6001(0.5601) Grad: 211420.0625  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6870(0.6870) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.5601  avg_val_loss: 0.7722  time: 82s\n",
      "Epoch 15 - avg_train_Score: 0.5601 avgScore: 0.7722\n",
      "Epoch 15 - Save Best Score: 0.7722 Model\n",
      "Epoch 15 - Save Best Loss: 0.7722 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7308(0.7722) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 10m 22s) Loss: 0.6097(0.6097) Grad: 498777.5312  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5772(0.5407) Grad: 260668.6094  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6054(0.5422) Grad: 202402.0469  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6947(0.6947) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.5422  avg_val_loss: 0.7616  time: 81s\n",
      "Epoch 16 - avg_train_Score: 0.5422 avgScore: 0.7616\n",
      "Epoch 16 - Save Best Score: 0.7616 Model\n",
      "Epoch 16 - Save Best Loss: 0.7616 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7109(0.7616) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 36s) Loss: 0.4100(0.4100) Grad: 354339.4062  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5572(0.5276) Grad: 231976.7344  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.4241(0.5296) Grad: 247320.2344  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6822(0.6822) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.5296  avg_val_loss: 0.7619  time: 82s\n",
      "Epoch 17 - avg_train_Score: 0.5296 avgScore: 0.7619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7286(0.7619) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 10m 28s) Loss: 0.4423(0.4423) Grad: 433337.5625  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5639(0.4781) Grad: 237345.3594  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4707(0.4786) Grad: 194053.5469  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.6902(0.6902) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4786  avg_val_loss: 0.7558  time: 81s\n",
      "Epoch 18 - avg_train_Score: 0.4786 avgScore: 0.7558\n",
      "Epoch 18 - Save Best Score: 0.7558 Model\n",
      "Epoch 18 - Save Best Loss: 0.7558 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7256(0.7558) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 10m 7s) Loss: 0.5324(0.5324) Grad: 402310.2812  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5187(0.4882) Grad: 225700.2031  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6001(0.4889) Grad: 127607.5938  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.6779(0.6779) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4889  avg_val_loss: 0.7497  time: 82s\n",
      "Epoch 19 - avg_train_Score: 0.4889 avgScore: 0.7497\n",
      "Epoch 19 - Save Best Score: 0.7497 Model\n",
      "Epoch 19 - Save Best Loss: 0.7497 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7219(0.7497) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 10m 22s) Loss: 0.4757(0.4757) Grad: 454469.8438  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4098(0.4366) Grad: 358207.4375  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4007(0.4359) Grad: 367900.7500  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6749(0.6749) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.4359  avg_val_loss: 0.7452  time: 82s\n",
      "Epoch 20 - avg_train_Score: 0.4359 avgScore: 0.7452\n",
      "Epoch 20 - Save Best Score: 0.7452 Model\n",
      "Epoch 20 - Save Best Loss: 0.7452 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7178(0.7452) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 10m 31s) Loss: 0.4290(0.4290) Grad: 322938.2500  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4139(0.4066) Grad: 332570.3438  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.3788(0.4068) Grad: 288712.5312  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.6742(0.6742) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.4068  avg_val_loss: 0.7416  time: 82s\n",
      "Epoch 21 - avg_train_Score: 0.4068 avgScore: 0.7416\n",
      "Epoch 21 - Save Best Score: 0.7416 Model\n",
      "Epoch 21 - Save Best Loss: 0.7416 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7047(0.7416) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 10m 9s) Loss: 0.4060(0.4060) Grad: 324953.8750  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3720(0.3963) Grad: 333134.9062  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3500(0.3961) Grad: 340412.7812  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.6867(0.6867) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3961  avg_val_loss: 0.7419  time: 82s\n",
      "Epoch 22 - avg_train_Score: 0.3961 avgScore: 0.7419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7039(0.7419) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 10m 10s) Loss: 0.3668(0.3668) Grad: 332131.0312  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3645(0.3861) Grad: 307599.1562  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3631(0.3857) Grad: 386526.7812  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6845(0.6845) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3857  avg_val_loss: 0.7360  time: 81s\n",
      "Epoch 23 - avg_train_Score: 0.3857 avgScore: 0.7360\n",
      "Epoch 23 - Save Best Score: 0.7360 Model\n",
      "Epoch 23 - Save Best Loss: 0.7360 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6955(0.7360) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 41s) Loss: 0.3759(0.3759) Grad: 334901.5938  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4629(0.4092) Grad: 168582.2656  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3992(0.4089) Grad: 177269.3594  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6641(0.6641) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.4089  avg_val_loss: 0.7387  time: 82s\n",
      "Epoch 24 - avg_train_Score: 0.4089 avgScore: 0.7387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6874(0.7387) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 0.3421(0.3421) Grad: 330726.0938  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3561(0.3746) Grad: 292672.1250  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3541(0.3743) Grad: 296218.8125  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.6682(0.6682) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3743  avg_val_loss: 0.7341  time: 82s\n",
      "Epoch 25 - avg_train_Score: 0.3743 avgScore: 0.7341\n",
      "Epoch 25 - Save Best Score: 0.7341 Model\n",
      "Epoch 25 - Save Best Loss: 0.7341 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6932(0.7341) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 10m 34s) Loss: 0.3306(0.3306) Grad: 351242.1250  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3419(0.3482) Grad: 308995.3125  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3208(0.3483) Grad: 304602.9688  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.6553(0.6553) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3483  avg_val_loss: 0.7312  time: 81s\n",
      "Epoch 26 - avg_train_Score: 0.3483 avgScore: 0.7312\n",
      "Epoch 26 - Save Best Score: 0.7312 Model\n",
      "Epoch 26 - Save Best Loss: 0.7312 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6936(0.7312) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.3554(0.3554) Grad: 288496.5312  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3215(0.3354) Grad: 235680.3125  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3332(0.3354) Grad: 277659.7500  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.6550(0.6550) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.3354  avg_val_loss: 0.7285  time: 82s\n",
      "Epoch 27 - avg_train_Score: 0.3354 avgScore: 0.7285\n",
      "Epoch 27 - Save Best Score: 0.7285 Model\n",
      "Epoch 27 - Save Best Loss: 0.7285 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6931(0.7285) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 10m 23s) Loss: 0.2823(0.2823) Grad: 251143.9531  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2886(0.3240) Grad: 275659.5312  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3321(0.3238) Grad: 313469.5312  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6683(0.6683) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.3238  avg_val_loss: 0.7279  time: 82s\n",
      "Epoch 28 - avg_train_Score: 0.3238 avgScore: 0.7279\n",
      "Epoch 28 - Save Best Score: 0.7279 Model\n",
      "Epoch 28 - Save Best Loss: 0.7279 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7038(0.7279) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 9m 56s) Loss: 0.2982(0.2982) Grad: 280782.4062  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3045(0.3159) Grad: 273623.1250  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.3000(0.3154) Grad: 285477.7812  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.6506(0.6506) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.3154  avg_val_loss: 0.7273  time: 82s\n",
      "Epoch 29 - avg_train_Score: 0.3154 avgScore: 0.7273\n",
      "Epoch 29 - Save Best Score: 0.7273 Model\n",
      "Epoch 29 - Save Best Loss: 0.7273 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6989(0.7273) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 9m 59s) Loss: 0.3376(0.3376) Grad: 336760.2500  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4021(0.3037) Grad: 343949.3438  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2865(0.3045) Grad: 264461.5625  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6681(0.6681) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.3045  avg_val_loss: 0.7260  time: 82s\n",
      "Epoch 30 - avg_train_Score: 0.3045 avgScore: 0.7260\n",
      "Epoch 30 - Save Best Score: 0.7260 Model\n",
      "Epoch 30 - Save Best Loss: 0.7260 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6958(0.7260) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 10m 14s) Loss: 0.3087(0.3087) Grad: 310481.4688  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2630(0.2950) Grad: 300294.9688  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2697(0.2950) Grad: 250215.1875  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6625(0.6625) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2950  avg_val_loss: 0.7250  time: 82s\n",
      "Epoch 31 - avg_train_Score: 0.2950 avgScore: 0.7250\n",
      "Epoch 31 - Save Best Score: 0.7250 Model\n",
      "Epoch 31 - Save Best Loss: 0.7250 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6920(0.7250) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 10m 34s) Loss: 0.2683(0.2683) Grad: 326848.3438  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2761(0.2853) Grad: 267758.7812  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3769(0.2859) Grad: 305716.0000  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.6609(0.6609) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2859  avg_val_loss: 0.7240  time: 82s\n",
      "Epoch 32 - avg_train_Score: 0.2859 avgScore: 0.7240\n",
      "Epoch 32 - Save Best Score: 0.7240 Model\n",
      "Epoch 32 - Save Best Loss: 0.7240 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6925(0.7240) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 10m 34s) Loss: 0.2682(0.2682) Grad: 285331.2500  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3128(0.2777) Grad: 260287.9844  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3003(0.2779) Grad: 366490.0000  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.6601(0.6601) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2779  avg_val_loss: 0.7237  time: 82s\n",
      "Epoch 33 - avg_train_Score: 0.2779 avgScore: 0.7237\n",
      "Epoch 33 - Save Best Score: 0.7237 Model\n",
      "Epoch 33 - Save Best Loss: 0.7237 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6955(0.7237) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 10m 48s) Loss: 0.2896(0.2896) Grad: 260575.7500  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2807(0.2784) Grad: 150705.6094  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2743(0.2782) Grad: 176630.0781  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6561(0.6561) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2782  avg_val_loss: 0.7238  time: 82s\n",
      "Epoch 34 - avg_train_Score: 0.2782 avgScore: 0.7238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7021(0.7238) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.2316(0.2316) Grad: 254510.7031  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2530(0.2640) Grad: 238081.1250  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2140(0.2640) Grad: 248778.2969  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6579(0.6579) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2640  avg_val_loss: 0.7235  time: 81s\n",
      "Epoch 35 - avg_train_Score: 0.2640 avgScore: 0.7235\n",
      "Epoch 35 - Save Best Score: 0.7235 Model\n",
      "Epoch 35 - Save Best Loss: 0.7235 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6930(0.7235) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 10m 15s) Loss: 0.2306(0.2306) Grad: 274313.1562  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2722(0.2581) Grad: 270653.9688  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3064(0.2582) Grad: 355172.8750  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6583(0.6583) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2582  avg_val_loss: 0.7230  time: 82s\n",
      "Epoch 36 - avg_train_Score: 0.2582 avgScore: 0.7230\n",
      "Epoch 36 - Save Best Score: 0.7230 Model\n",
      "Epoch 36 - Save Best Loss: 0.7230 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6957(0.7230) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 32s) Loss: 0.2711(0.2711) Grad: 387978.3438  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2862(0.2530) Grad: 260333.7500  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2643(0.2536) Grad: 225596.7969  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.6550(0.6550) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2536  avg_val_loss: 0.7223  time: 81s\n",
      "Epoch 37 - avg_train_Score: 0.2536 avgScore: 0.7223\n",
      "Epoch 37 - Save Best Score: 0.7223 Model\n",
      "Epoch 37 - Save Best Loss: 0.7223 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6928(0.7223) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.2628(0.2628) Grad: 278798.7500  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2535(0.2509) Grad: 256056.2031  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2978(0.2512) Grad: 356102.8125  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.6549(0.6549) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2512  avg_val_loss: 0.7226  time: 81s\n",
      "Epoch 38 - avg_train_Score: 0.2512 avgScore: 0.7226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6911(0.7226) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 10m 49s) Loss: 0.2868(0.2868) Grad: 323568.0000  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2596(0.2466) Grad: 282318.4375  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2458(0.2472) Grad: 277836.5938  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6540(0.6540) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2472  avg_val_loss: 0.7224  time: 82s\n",
      "Epoch 39 - avg_train_Score: 0.2472 avgScore: 0.7224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6936(0.7224) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 10m 9s) Loss: 0.2595(0.2595) Grad: 281319.5312  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2826(0.2450) Grad: 275984.4375  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2350(0.2455) Grad: 235641.8594  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.6568(0.6568) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2455  avg_val_loss: 0.7223  time: 81s\n",
      "Epoch 40 - avg_train_Score: 0.2455 avgScore: 0.7223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.6939(0.7223) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 2 result ==========\n",
      "score: 0.7223\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 28s) Loss: 4.9267(4.9267) Grad: 160813.0000  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 2.3696(3.0272) Grad: 52665.4219  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 2.6342(2.9641) Grad: 248094.7500  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 2.1604(2.1604) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 2.9641  avg_val_loss: 2.3578  time: 82s\n",
      "Epoch 1 - avg_train_Score: 2.9641 avgScore: 2.3578\n",
      "Epoch 1 - Save Best Score: 2.3578 Model\n",
      "Epoch 1 - Save Best Loss: 2.3578 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 2.0385(2.3578) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 43s) Loss: 1.7219(1.7219) Grad: 989891.5000  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.8727(1.9548) Grad: 107015.3750  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.3687(1.9400) Grad: 61836.0820  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 1.6212(1.6212) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 1.9400  avg_val_loss: 1.8025  time: 81s\n",
      "Epoch 2 - avg_train_Score: 1.9400 avgScore: 1.8025\n",
      "Epoch 2 - Save Best Score: 1.8025 Model\n",
      "Epoch 2 - Save Best Loss: 1.8025 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.5555(1.8025) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 9m 57s) Loss: 1.5948(1.5948) Grad: 1334389.3750  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.1244(1.4581) Grad: 42115.8711  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.0719(1.4388) Grad: 34264.1992  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 1.0444(1.0444) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.4388  avg_val_loss: 1.1514  time: 82s\n",
      "Epoch 3 - avg_train_Score: 1.4388 avgScore: 1.1514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.0079(1.1514) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Save Best Score: 1.1514 Model\n",
      "Epoch 3 - Save Best Loss: 1.1514 Model\n",
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 10m 12s) Loss: 1.2384(1.2384) Grad: 645079.5000  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9213(1.0574) Grad: 146244.0625  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.0759(1.0508) Grad: 134467.5156  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.9475(0.9475) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0508  avg_val_loss: 1.0081  time: 82s\n",
      "Epoch 4 - avg_train_Score: 1.0508 avgScore: 1.0081\n",
      "Epoch 4 - Save Best Score: 1.0081 Model\n",
      "Epoch 4 - Save Best Loss: 1.0081 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.0061(1.0081) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 10m 16s) Loss: 0.9796(0.9796) Grad: 698576.9375  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8768(0.9231) Grad: 118961.2812  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6647(0.9234) Grad: 142477.1562  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.8748(0.8748) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9234  avg_val_loss: 0.9512  time: 81s\n",
      "Epoch 5 - avg_train_Score: 0.9234 avgScore: 0.9512\n",
      "Epoch 5 - Save Best Score: 0.9512 Model\n",
      "Epoch 5 - Save Best Loss: 0.9512 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.0089(0.9512) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 10m 23s) Loss: 0.9561(0.9561) Grad: 644455.1250  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7959(0.8384) Grad: 293157.0000  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.8479(0.8371) Grad: 332939.4375  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.8379(0.8379) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8371  avg_val_loss: 0.9194  time: 81s\n",
      "Epoch 6 - avg_train_Score: 0.8371 avgScore: 0.9194\n",
      "Epoch 6 - Save Best Score: 0.9194 Model\n",
      "Epoch 6 - Save Best Loss: 0.9194 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9837(0.9194) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.6982(0.6982) Grad: 526985.1250  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9398(0.8393) Grad: 179296.6562  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.8601(0.8408) Grad: 139416.7188  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.8392(0.8392) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8408  avg_val_loss: 0.9029  time: 82s\n",
      "Epoch 7 - avg_train_Score: 0.8408 avgScore: 0.9029\n",
      "Epoch 7 - Save Best Score: 0.9029 Model\n",
      "Epoch 7 - Save Best Loss: 0.9029 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9770(0.9029) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 10m 5s) Loss: 0.8669(0.8669) Grad: 459792.0312  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6741(0.7555) Grad: 304187.5625  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.7114(0.7547) Grad: 284258.5000  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.8205(0.8205) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7547  avg_val_loss: 0.8850  time: 82s\n",
      "Epoch 8 - avg_train_Score: 0.7547 avgScore: 0.8850\n",
      "Epoch 8 - Save Best Score: 0.8850 Model\n",
      "Epoch 8 - Save Best Loss: 0.8850 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9429(0.8850) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 11m 7s) Loss: 0.7448(0.7448) Grad: 601502.1250  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6697(0.7069) Grad: 260434.3438  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7879(0.7074) Grad: 291490.9375  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.8144(0.8144) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.7074  avg_val_loss: 0.8703  time: 81s\n",
      "Epoch 9 - avg_train_Score: 0.7074 avgScore: 0.8703\n",
      "Epoch 9 - Save Best Score: 0.8703 Model\n",
      "Epoch 9 - Save Best Loss: 0.8703 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9417(0.8703) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 10m 1s) Loss: 0.7314(0.7314) Grad: 489345.3125  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.8865(0.7338) Grad: 160505.1094  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6947(0.7368) Grad: 170423.2344  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.8440(0.8440) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.7368  avg_val_loss: 0.8672  time: 81s\n",
      "Epoch 10 - avg_train_Score: 0.7368 avgScore: 0.8672\n",
      "Epoch 10 - Save Best Score: 0.8672 Model\n",
      "Epoch 10 - Save Best Loss: 0.8672 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9342(0.8672) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 10m 9s) Loss: 0.7520(0.7520) Grad: 583937.8125  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5743(0.6638) Grad: 184154.7188  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5384(0.6624) Grad: 190746.5469  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.8299(0.8299) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.6624  avg_val_loss: 0.8484  time: 81s\n",
      "Epoch 11 - avg_train_Score: 0.6624 avgScore: 0.8484\n",
      "Epoch 11 - Save Best Score: 0.8484 Model\n",
      "Epoch 11 - Save Best Loss: 0.8484 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9298(0.8484) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 10m 35s) Loss: 0.5708(0.5708) Grad: 486153.0625  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5155(0.6157) Grad: 241713.1406  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5515(0.6157) Grad: 186918.0625  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.8252(0.8252) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.6157  avg_val_loss: 0.8388  time: 82s\n",
      "Epoch 12 - avg_train_Score: 0.6157 avgScore: 0.8388\n",
      "Epoch 12 - Save Best Score: 0.8388 Model\n",
      "Epoch 12 - Save Best Loss: 0.8388 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8912(0.8388) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 10m 0s) Loss: 0.7549(0.7549) Grad: inf  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5978(0.5888) Grad: 240778.7500  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.8217(0.5898) Grad: 243924.4844  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7972(0.7972) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.5898  avg_val_loss: 0.8335  time: 82s\n",
      "Epoch 13 - avg_train_Score: 0.5898 avgScore: 0.8335\n",
      "Epoch 13 - Save Best Score: 0.8335 Model\n",
      "Epoch 13 - Save Best Loss: 0.8335 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8952(0.8335) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 0.5510(0.5510) Grad: 354315.2500  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5491(0.5646) Grad: 189778.3125  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6722(0.5644) Grad: 204784.0000  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7970(0.7970) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5644  avg_val_loss: 0.8284  time: 82s\n",
      "Epoch 14 - avg_train_Score: 0.5644 avgScore: 0.8284\n",
      "Epoch 14 - Save Best Score: 0.8284 Model\n",
      "Epoch 14 - Save Best Loss: 0.8284 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9390(0.8284) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 1s) Loss: 0.5224(0.5224) Grad: 375161.0625  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5891(0.5428) Grad: 256682.3906  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4813(0.5437) Grad: 211817.9844  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7893(0.7893) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.5437  avg_val_loss: 0.8237  time: 81s\n",
      "Epoch 15 - avg_train_Score: 0.5437 avgScore: 0.8237\n",
      "Epoch 15 - Save Best Score: 0.8237 Model\n",
      "Epoch 15 - Save Best Loss: 0.8237 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9280(0.8237) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 10m 26s) Loss: 0.5344(0.5344) Grad: 489116.3750  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5242(0.5515) Grad: 98825.0859  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5279(0.5530) Grad: 96380.8906  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7483(0.7483) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.5530  avg_val_loss: 0.8202  time: 81s\n",
      "Epoch 16 - avg_train_Score: 0.5530 avgScore: 0.8202\n",
      "Epoch 16 - Save Best Score: 0.8202 Model\n",
      "Epoch 16 - Save Best Loss: 0.8202 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9226(0.8202) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 36s) Loss: 0.5260(0.5260) Grad: 396906.9062  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5236(0.5121) Grad: 190953.2500  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4996(0.5106) Grad: 182716.8438  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7603(0.7603) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.5106  avg_val_loss: 0.8141  time: 82s\n",
      "Epoch 17 - avg_train_Score: 0.5106 avgScore: 0.8141\n",
      "Epoch 17 - Save Best Score: 0.8141 Model\n",
      "Epoch 17 - Save Best Loss: 0.8141 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9362(0.8141) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 9m 51s) Loss: 0.4525(0.4525) Grad: 373188.8438  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4721(0.4564) Grad: 161376.5312  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4763(0.4583) Grad: 176036.3594  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7417(0.7417) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4583  avg_val_loss: 0.8137  time: 82s\n",
      "Epoch 18 - avg_train_Score: 0.4583 avgScore: 0.8137\n",
      "Epoch 18 - Save Best Score: 0.8137 Model\n",
      "Epoch 18 - Save Best Loss: 0.8137 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9076(0.8137) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.5096(0.5096) Grad: 391213.9062  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3741(0.4221) Grad: 384389.6562  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3889(0.4231) Grad: 316750.4688  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7415(0.7415) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4231  avg_val_loss: 0.8060  time: 82s\n",
      "Epoch 19 - avg_train_Score: 0.4231 avgScore: 0.8060\n",
      "Epoch 19 - Save Best Score: 0.8060 Model\n",
      "Epoch 19 - Save Best Loss: 0.8060 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8973(0.8060) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.4701(0.4701) Grad: 299741.7812  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3464(0.3991) Grad: 325430.0625  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3407(0.3999) Grad: 284376.8750  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.7503(0.7503) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.3999  avg_val_loss: 0.8029  time: 81s\n",
      "Epoch 20 - avg_train_Score: 0.3999 avgScore: 0.8029\n",
      "Epoch 20 - Save Best Score: 0.8029 Model\n",
      "Epoch 20 - Save Best Loss: 0.8029 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8966(0.8029) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 9m 45s) Loss: 0.3635(0.3635) Grad: 351447.5625  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4119(0.4180) Grad: 214123.2656  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4511(0.4196) Grad: 185950.4531  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7356(0.7356) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.4196  avg_val_loss: 0.8021  time: 81s\n",
      "Epoch 21 - avg_train_Score: 0.4196 avgScore: 0.8021\n",
      "Epoch 21 - Save Best Score: 0.8021 Model\n",
      "Epoch 21 - Save Best Loss: 0.8021 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8953(0.8021) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 10m 2s) Loss: 0.3831(0.3831) Grad: 370172.1875  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3708(0.3956) Grad: 290936.8125  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.3696(0.3950) Grad: 351445.1875  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7258(0.7258) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3950  avg_val_loss: 0.7991  time: 82s\n",
      "Epoch 22 - avg_train_Score: 0.3950 avgScore: 0.7991\n",
      "Epoch 22 - Save Best Score: 0.7991 Model\n",
      "Epoch 22 - Save Best Loss: 0.7991 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9111(0.7991) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 0.3310(0.3310) Grad: 291179.4062  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3695(0.3703) Grad: 303513.5625  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3315(0.3703) Grad: 319299.2500  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7120(0.7120) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3703  avg_val_loss: 0.7942  time: 81s\n",
      "Epoch 23 - avg_train_Score: 0.3703 avgScore: 0.7942\n",
      "Epoch 23 - Save Best Score: 0.7942 Model\n",
      "Epoch 23 - Save Best Loss: 0.7942 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9047(0.7942) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 9m 52s) Loss: 0.3542(0.3542) Grad: 337411.7188  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3706(0.3595) Grad: 318117.4688  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3320(0.3602) Grad: 280955.5625  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.7126(0.7126) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3602  avg_val_loss: 0.7957  time: 81s\n",
      "Epoch 24 - avg_train_Score: 0.3602 avgScore: 0.7957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9072(0.7957) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 9m 42s) Loss: 0.3257(0.3257) Grad: 276919.8125  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3822(0.3490) Grad: 276111.1875  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3432(0.3494) Grad: 240494.2500  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7184(0.7184) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3494  avg_val_loss: 0.7942  time: 81s\n",
      "Epoch 25 - avg_train_Score: 0.3494 avgScore: 0.7942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9130(0.7942) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 10m 6s) Loss: 0.3115(0.3115) Grad: 230189.3438  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3046(0.3373) Grad: 233059.6719  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3113(0.3382) Grad: 346081.3125  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7177(0.7177) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3382  avg_val_loss: 0.7922  time: 81s\n",
      "Epoch 26 - avg_train_Score: 0.3382 avgScore: 0.7922\n",
      "Epoch 26 - Save Best Score: 0.7922 Model\n",
      "Epoch 26 - Save Best Loss: 0.7922 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9023(0.7922) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 2s) Loss: 0.3240(0.3240) Grad: 249467.3906  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3411(0.3498) Grad: 153653.8906  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3302(0.3498) Grad: 131326.8594  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7230(0.7230) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.3498  avg_val_loss: 0.7916  time: 81s\n",
      "Epoch 27 - avg_train_Score: 0.3498 avgScore: 0.7916\n",
      "Epoch 27 - Save Best Score: 0.7916 Model\n",
      "Epoch 27 - Save Best Loss: 0.7916 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8962(0.7916) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 10m 17s) Loss: 0.3325(0.3325) Grad: 343640.0000  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3742(0.3357) Grad: 167233.9062  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4003(0.3363) Grad: 149161.5312  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7214(0.7214) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.3363  avg_val_loss: 0.7899  time: 81s\n",
      "Epoch 28 - avg_train_Score: 0.3363 avgScore: 0.7899\n",
      "Epoch 28 - Save Best Score: 0.7899 Model\n",
      "Epoch 28 - Save Best Loss: 0.7899 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8836(0.7899) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 10m 47s) Loss: 0.3382(0.3382) Grad: 308928.4062  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3567(0.3074) Grad: 286274.2812  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3238(0.3080) Grad: 230289.8594  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 41s) Loss: 0.7105(0.7105) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.3080  avg_val_loss: 0.7890  time: 82s\n",
      "Epoch 29 - avg_train_Score: 0.3080 avgScore: 0.7890\n",
      "Epoch 29 - Save Best Score: 0.7890 Model\n",
      "Epoch 29 - Save Best Loss: 0.7890 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8861(0.7890) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 10m 21s) Loss: 0.3193(0.3193) Grad: 277320.5312  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3047(0.2946) Grad: 292582.5938  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3173(0.2943) Grad: 252532.1875  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7052(0.7052) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2943  avg_val_loss: 0.7882  time: 81s\n",
      "Epoch 30 - avg_train_Score: 0.2943 avgScore: 0.7882\n",
      "Epoch 30 - Save Best Score: 0.7882 Model\n",
      "Epoch 30 - Save Best Loss: 0.7882 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8854(0.7882) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 9m 48s) Loss: 0.2870(0.2870) Grad: 228797.1719  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3058(0.2843) Grad: 341768.4375  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2671(0.2848) Grad: 280604.7812  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.7063(0.7063) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2848  avg_val_loss: 0.7885  time: 81s\n",
      "Epoch 31 - avg_train_Score: 0.2848 avgScore: 0.7885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8961(0.7885) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 10m 23s) Loss: 0.3038(0.3038) Grad: 262728.8125  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.3389(0.2780) Grad: 282351.2500  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.3185(0.2782) Grad: 278246.6562  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7034(0.7034) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2782  avg_val_loss: 0.7874  time: 82s\n",
      "Epoch 32 - avg_train_Score: 0.2782 avgScore: 0.7874\n",
      "Epoch 32 - Save Best Score: 0.7874 Model\n",
      "Epoch 32 - Save Best Loss: 0.7874 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8904(0.7874) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 10m 20s) Loss: 0.2885(0.2885) Grad: 328299.1875  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3018(0.2710) Grad: 285404.8125  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2574(0.2714) Grad: 238289.3125  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.7123(0.7123) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2714  avg_val_loss: 0.7872  time: 82s\n",
      "Epoch 33 - avg_train_Score: 0.2714 avgScore: 0.7872\n",
      "Epoch 33 - Save Best Score: 0.7872 Model\n",
      "Epoch 33 - Save Best Loss: 0.7872 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8850(0.7872) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 10m 46s) Loss: 0.2188(0.2188) Grad: 259591.6094  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3296(0.2675) Grad: 283808.9062  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2664(0.2678) Grad: 257247.0000  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7062(0.7062) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2678  avg_val_loss: 0.7871  time: 82s\n",
      "Epoch 34 - avg_train_Score: 0.2678 avgScore: 0.7871\n",
      "Epoch 34 - Save Best Score: 0.7871 Model\n",
      "Epoch 34 - Save Best Loss: 0.7871 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8809(0.7871) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.2433(0.2433) Grad: 260781.3438  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2828(0.2586) Grad: 314376.7188  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2906(0.2586) Grad: 299928.2188  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7052(0.7052) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2586  avg_val_loss: 0.7849  time: 81s\n",
      "Epoch 35 - avg_train_Score: 0.2586 avgScore: 0.7849\n",
      "Epoch 35 - Save Best Score: 0.7849 Model\n",
      "Epoch 35 - Save Best Loss: 0.7849 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8868(0.7849) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 9m 45s) Loss: 0.2214(0.2214) Grad: 225268.1562  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2570(0.2551) Grad: 131524.2969  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2445(0.2556) Grad: 130756.7500  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7062(0.7062) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2556  avg_val_loss: 0.7854  time: 81s\n",
      "Epoch 36 - avg_train_Score: 0.2556 avgScore: 0.7854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8874(0.7854) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 58s) Loss: 0.3088(0.3088) Grad: 239018.8125  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.2307(0.2516) Grad: 264282.2812  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2601(0.2518) Grad: 225477.5469  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7091(0.7091) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2518  avg_val_loss: 0.7848  time: 82s\n",
      "Epoch 37 - avg_train_Score: 0.2518 avgScore: 0.7848\n",
      "Epoch 37 - Save Best Score: 0.7848 Model\n",
      "Epoch 37 - Save Best Loss: 0.7848 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8802(0.7848) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 10m 10s) Loss: 0.2399(0.2399) Grad: 248024.5625  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2043(0.2465) Grad: 260956.4062  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2612(0.2467) Grad: 237192.7188  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7093(0.7093) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2467  avg_val_loss: 0.7845  time: 82s\n",
      "Epoch 38 - avg_train_Score: 0.2467 avgScore: 0.7845\n",
      "Epoch 38 - Save Best Score: 0.7845 Model\n",
      "Epoch 38 - Save Best Loss: 0.7845 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8830(0.7845) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 9m 58s) Loss: 0.2510(0.2510) Grad: 264537.2188  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2559(0.2453) Grad: 286712.4375  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2506(0.2453) Grad: 273763.0312  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7080(0.7080) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2453  avg_val_loss: 0.7841  time: 81s\n",
      "Epoch 39 - avg_train_Score: 0.2453 avgScore: 0.7841\n",
      "Epoch 39 - Save Best Score: 0.7841 Model\n",
      "Epoch 39 - Save Best Loss: 0.7841 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8845(0.7841) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 9m 54s) Loss: 0.2067(0.2067) Grad: 224953.7656  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2442(0.2424) Grad: 244367.6406  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2229(0.2424) Grad: 262013.3594  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7065(0.7065) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2424  avg_val_loss: 0.7839  time: 82s\n",
      "Epoch 40 - avg_train_Score: 0.2424 avgScore: 0.7839\n",
      "Epoch 40 - Save Best Score: 0.7839 Model\n",
      "Epoch 40 - Save Best Loss: 0.7839 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8863(0.7839) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 3 result ==========\n",
      "score: 0.7839\n",
      "========== fold: 4 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n",
      "pretrained: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/542] Elapsed 0m 1s (remain 10m 5s) Loss: 5.7302(5.7302) Grad: 182863.8750  LR: 0.000010  \n",
      "Epoch: [1][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 2.0393(3.0691) Grad: 57491.1680  LR: 0.000010  \n",
      "Epoch: [1][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 2.1316(3.0098) Grad: 118520.7109  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 2.1596(2.1596) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 1 - avg_train_loss: 3.0098  avg_val_loss: 2.4486  time: 81s\n",
      "Epoch 1 - avg_train_Score: 3.0098 avgScore: 2.4486\n",
      "Epoch 1 - Save Best Score: 2.4486 Model\n",
      "Epoch 1 - Save Best Loss: 2.4486 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 2.5543(2.4486) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/542] Elapsed 0m 1s (remain 10m 4s) Loss: 2.1851(2.1851) Grad: inf  LR: 0.000010  \n",
      "Epoch: [2][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.7587(2.0157) Grad: 153145.5625  LR: 0.000010  \n",
      "Epoch: [2][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.7708(2.0039) Grad: 219106.4844  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 1.6060(1.6060) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 2 - avg_train_loss: 2.0039  avg_val_loss: 1.8651  time: 81s\n",
      "Epoch 2 - avg_train_Score: 2.0039 avgScore: 1.8651\n",
      "Epoch 2 - Save Best Score: 1.8651 Model\n",
      "Epoch 2 - Save Best Loss: 1.8651 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.8810(1.8651) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/542] Elapsed 0m 1s (remain 9m 59s) Loss: 1.5407(1.5407) Grad: inf  LR: 0.000100  \n",
      "Epoch: [3][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.5582(1.5205) Grad: 101200.3984  LR: 0.000100  \n",
      "Epoch: [3][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.9860(1.4983) Grad: 23132.7461  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 1.0658(1.0658) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n",
      "Epoch 3 - avg_train_loss: 1.4983  avg_val_loss: 1.1239  time: 81s\n",
      "Epoch 3 - avg_train_Score: 1.4983 avgScore: 1.1239\n",
      "Epoch 3 - Save Best Score: 1.1239 Model\n",
      "Epoch 3 - Save Best Loss: 1.1239 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.3531(1.1239) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/542] Elapsed 0m 1s (remain 9m 59s) Loss: 0.9660(0.9660) Grad: 445891.2188  LR: 0.000100  \n",
      "Epoch: [4][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9603(1.0325) Grad: 263683.2188  LR: 0.000100  \n",
      "Epoch: [4][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 1.0368(1.0272) Grad: 319319.6562  LR: 0.000100  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 1.0067(1.0067) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 4 - avg_train_loss: 1.0272  avg_val_loss: 1.0016  time: 82s\n",
      "Epoch 4 - avg_train_Score: 1.0272 avgScore: 1.0016\n",
      "Epoch 4 - Save Best Score: 1.0016 Model\n",
      "Epoch 4 - Save Best Loss: 1.0016 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.1162(1.0016) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/542] Elapsed 0m 1s (remain 10m 30s) Loss: 0.9061(0.9061) Grad: 464372.2188  LR: 0.000099  \n",
      "Epoch: [5][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6878(0.9308) Grad: 252712.4219  LR: 0.000099  \n",
      "Epoch: [5][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6992(0.9256) Grad: 258596.6406  LR: 0.000099  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.9501(0.9501) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 5 - avg_train_loss: 0.9256  avg_val_loss: 0.9475  time: 82s\n",
      "Epoch 5 - avg_train_Score: 0.9256 avgScore: 0.9475\n",
      "Epoch 5 - Save Best Score: 0.9475 Model\n",
      "Epoch 5 - Save Best Loss: 0.9475 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 1.0279(0.9475) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/542] Elapsed 0m 1s (remain 10m 36s) Loss: 0.8152(0.8152) Grad: 461591.9688  LR: 0.000098  \n",
      "Epoch: [6][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.8547(0.8670) Grad: 245194.6719  LR: 0.000098  \n",
      "Epoch: [6][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.8470(0.8663) Grad: 230433.0781  LR: 0.000098  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.9001(0.9001) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 6 - avg_train_loss: 0.8663  avg_val_loss: 0.9088  time: 82s\n",
      "Epoch 6 - avg_train_Score: 0.8663 avgScore: 0.9088\n",
      "Epoch 6 - Save Best Score: 0.9088 Model\n",
      "Epoch 6 - Save Best Loss: 0.9088 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9895(0.9088) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/542] Elapsed 0m 1s (remain 10m 17s) Loss: 0.9742(0.9742) Grad: 522967.0938  LR: 0.000097  \n",
      "Epoch: [7][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 1.0037(0.8529) Grad: 169663.8750  LR: 0.000097  \n",
      "Epoch: [7][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.8402(0.8576) Grad: 123708.2500  LR: 0.000097  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 39s) Loss: 0.8723(0.8723) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 7 - avg_train_loss: 0.8576  avg_val_loss: 0.8876  time: 82s\n",
      "Epoch 7 - avg_train_Score: 0.8576 avgScore: 0.8876\n",
      "Epoch 7 - Save Best Score: 0.8876 Model\n",
      "Epoch 7 - Save Best Loss: 0.8876 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9350(0.8876) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/542] Elapsed 0m 1s (remain 12m 20s) Loss: 0.9019(0.9019) Grad: 528427.9375  LR: 0.000095  \n",
      "Epoch: [8][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.9167(0.7820) Grad: 123201.5234  LR: 0.000095  \n",
      "Epoch: [8][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6108(0.7835) Grad: 93892.1406  LR: 0.000095  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.8299(0.8299) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 8 - avg_train_loss: 0.7835  avg_val_loss: 0.8624  time: 82s\n",
      "Epoch 8 - avg_train_Score: 0.7835 avgScore: 0.8624\n",
      "Epoch 8 - Save Best Score: 0.8624 Model\n",
      "Epoch 8 - Save Best Loss: 0.8624 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8811(0.8624) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/542] Elapsed 0m 1s (remain 10m 30s) Loss: 0.7942(0.7942) Grad: 680204.8125  LR: 0.000093  \n",
      "Epoch: [9][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.7838(0.7259) Grad: 270209.4688  LR: 0.000093  \n",
      "Epoch: [9][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.8550(0.7249) Grad: 335866.7500  LR: 0.000093  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 22s) Loss: 0.8296(0.8296) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 9 - avg_train_loss: 0.7249  avg_val_loss: 0.8436  time: 82s\n",
      "Epoch 9 - avg_train_Score: 0.7249 avgScore: 0.8436\n",
      "Epoch 9 - Save Best Score: 0.8436 Model\n",
      "Epoch 9 - Save Best Loss: 0.8436 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.9321(0.8436) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/542] Elapsed 0m 1s (remain 9m 44s) Loss: 0.6407(0.6407) Grad: 409012.3750  LR: 0.000091  \n",
      "Epoch: [10][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6326(0.6710) Grad: 92528.3047  LR: 0.000091  \n",
      "Epoch: [10][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.7761(0.6781) Grad: 169489.8438  LR: 0.000091  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.8324(0.8324) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 10 - avg_train_loss: 0.6781  avg_val_loss: 0.8356  time: 81s\n",
      "Epoch 10 - avg_train_Score: 0.6781 avgScore: 0.8356\n",
      "Epoch 10 - Save Best Score: 0.8356 Model\n",
      "Epoch 10 - Save Best Loss: 0.8356 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8657(0.8356) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/542] Elapsed 0m 1s (remain 10m 41s) Loss: 0.7925(0.7925) Grad: 597816.8125  LR: 0.000088  \n",
      "Epoch: [11][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6037(0.6519) Grad: 283888.7812  LR: 0.000088  \n",
      "Epoch: [11][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.5752(0.6516) Grad: 256047.8438  LR: 0.000088  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.8119(0.8119) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 11 - avg_train_loss: 0.6516  avg_val_loss: 0.8254  time: 82s\n",
      "Epoch 11 - avg_train_Score: 0.6516 avgScore: 0.8254\n",
      "Epoch 11 - Save Best Score: 0.8254 Model\n",
      "Epoch 11 - Save Best Loss: 0.8254 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8827(0.8254) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/542] Elapsed 0m 1s (remain 10m 22s) Loss: 0.7132(0.7132) Grad: 499278.6562  LR: 0.000086  \n",
      "Epoch: [12][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6762(0.6121) Grad: 216462.0000  LR: 0.000086  \n",
      "Epoch: [12][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6293(0.6128) Grad: 243448.0312  LR: 0.000086  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 27s) Loss: 0.8025(0.8025) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 12 - avg_train_loss: 0.6128  avg_val_loss: 0.8159  time: 82s\n",
      "Epoch 12 - avg_train_Score: 0.6128 avgScore: 0.8159\n",
      "Epoch 12 - Save Best Score: 0.8159 Model\n",
      "Epoch 12 - Save Best Loss: 0.8159 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8421(0.8159) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/542] Elapsed 0m 1s (remain 9m 52s) Loss: 0.5633(0.5633) Grad: 515325.9375  LR: 0.000083  \n",
      "Epoch: [13][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.6274(0.5939) Grad: 129694.6328  LR: 0.000083  \n",
      "Epoch: [13][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.6869(0.6004) Grad: 119031.4219  LR: 0.000083  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7775(0.7775) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 13 - avg_train_loss: 0.6004  avg_val_loss: 0.8147  time: 81s\n",
      "Epoch 13 - avg_train_Score: 0.6004 avgScore: 0.8147\n",
      "Epoch 13 - Save Best Score: 0.8147 Model\n",
      "Epoch 13 - Save Best Loss: 0.8147 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8224(0.8147) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/542] Elapsed 0m 1s (remain 10m 22s) Loss: 0.6110(0.6110) Grad: 421277.3125  LR: 0.000080  \n",
      "Epoch: [14][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5331(0.5735) Grad: 210362.2344  LR: 0.000080  \n",
      "Epoch: [14][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5273(0.5722) Grad: 227675.0938  LR: 0.000080  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 35s) Loss: 0.7923(0.7923) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 14 - avg_train_loss: 0.5722  avg_val_loss: 0.8040  time: 82s\n",
      "Epoch 14 - avg_train_Score: 0.5722 avgScore: 0.8040\n",
      "Epoch 14 - Save Best Score: 0.8040 Model\n",
      "Epoch 14 - Save Best Loss: 0.8040 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8070(0.8040) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/542] Elapsed 0m 1s (remain 10m 27s) Loss: 0.5302(0.5302) Grad: 438566.2812  LR: 0.000076  \n",
      "Epoch: [15][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5166(0.5263) Grad: 142598.1250  LR: 0.000076  \n",
      "Epoch: [15][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5753(0.5279) Grad: 233370.7656  LR: 0.000076  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 31s) Loss: 0.8024(0.8024) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 15 - avg_train_loss: 0.5279  avg_val_loss: 0.8002  time: 82s\n",
      "Epoch 15 - avg_train_Score: 0.5279 avgScore: 0.8002\n",
      "Epoch 15 - Save Best Score: 0.8002 Model\n",
      "Epoch 15 - Save Best Loss: 0.8002 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8436(0.8002) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/542] Elapsed 0m 1s (remain 10m 34s) Loss: 0.4296(0.4296) Grad: 374195.5312  LR: 0.000073  \n",
      "Epoch: [16][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.5192(0.4925) Grad: 222429.2969  LR: 0.000073  \n",
      "Epoch: [16][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.5538(0.4933) Grad: 188887.9375  LR: 0.000073  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7983(0.7983) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 16 - avg_train_loss: 0.4933  avg_val_loss: 0.7942  time: 82s\n",
      "Epoch 16 - avg_train_Score: 0.4933 avgScore: 0.7942\n",
      "Epoch 16 - Save Best Score: 0.7942 Model\n",
      "Epoch 16 - Save Best Loss: 0.7942 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8333(0.7942) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/542] Elapsed 0m 1s (remain 10m 37s) Loss: 0.5674(0.5674) Grad: 503248.4062  LR: 0.000069  \n",
      "Epoch: [17][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4568(0.4493) Grad: 340548.4375  LR: 0.000069  \n",
      "Epoch: [17][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4830(0.4487) Grad: 351225.5000  LR: 0.000069  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.8042(0.8042) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 17 - avg_train_loss: 0.4487  avg_val_loss: 0.7894  time: 82s\n",
      "Epoch 17 - avg_train_Score: 0.4487 avgScore: 0.7894\n",
      "Epoch 17 - Save Best Score: 0.7894 Model\n",
      "Epoch 17 - Save Best Loss: 0.7894 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8416(0.7894) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/542] Elapsed 0m 1s (remain 11m 5s) Loss: 0.4375(0.4375) Grad: 381117.5312  LR: 0.000066  \n",
      "Epoch: [18][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3924(0.4244) Grad: 320351.5625  LR: 0.000066  \n",
      "Epoch: [18][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4075(0.4237) Grad: 411791.6250  LR: 0.000066  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7859(0.7859) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 18 - avg_train_loss: 0.4237  avg_val_loss: 0.7867  time: 82s\n",
      "Epoch 18 - avg_train_Score: 0.4237 avgScore: 0.7867\n",
      "Epoch 18 - Save Best Score: 0.7867 Model\n",
      "Epoch 18 - Save Best Loss: 0.7867 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8321(0.7867) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/542] Elapsed 0m 1s (remain 10m 33s) Loss: 0.4168(0.4168) Grad: 330698.9062  LR: 0.000062  \n",
      "Epoch: [19][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4584(0.4149) Grad: 340334.2500  LR: 0.000062  \n",
      "Epoch: [19][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3835(0.4143) Grad: 389696.0312  LR: 0.000062  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7869(0.7869) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 19 - avg_train_loss: 0.4143  avg_val_loss: 0.7821  time: 81s\n",
      "Epoch 19 - avg_train_Score: 0.4143 avgScore: 0.7821\n",
      "Epoch 19 - Save Best Score: 0.7821 Model\n",
      "Epoch 19 - Save Best Loss: 0.7821 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8125(0.7821) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/542] Elapsed 0m 1s (remain 10m 8s) Loss: 0.4561(0.4561) Grad: 373722.3750  LR: 0.000058  \n",
      "Epoch: [20][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3996(0.4067) Grad: 382495.5625  LR: 0.000058  \n",
      "Epoch: [20][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4519(0.4067) Grad: 328182.3438  LR: 0.000058  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 37s) Loss: 0.7813(0.7813) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 20 - avg_train_loss: 0.4067  avg_val_loss: 0.7824  time: 82s\n",
      "Epoch 20 - avg_train_Score: 0.4067 avgScore: 0.7824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8054(0.7824) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/542] Elapsed 0m 1s (remain 10m 7s) Loss: 0.3437(0.3437) Grad: 331443.4062  LR: 0.000054  \n",
      "Epoch: [21][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4427(0.3951) Grad: 348848.3438  LR: 0.000054  \n",
      "Epoch: [21][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4289(0.3952) Grad: 335258.8125  LR: 0.000054  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 29s) Loss: 0.7747(0.7747) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 21 - avg_train_loss: 0.3952  avg_val_loss: 0.7782  time: 82s\n",
      "Epoch 21 - avg_train_Score: 0.3952 avgScore: 0.7782\n",
      "Epoch 21 - Save Best Score: 0.7782 Model\n",
      "Epoch 21 - Save Best Loss: 0.7782 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8084(0.7782) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/542] Elapsed 0m 1s (remain 10m 22s) Loss: 0.4460(0.4460) Grad: 286469.0625  LR: 0.000050  \n",
      "Epoch: [22][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4730(0.3835) Grad: 333136.5000  LR: 0.000050  \n",
      "Epoch: [22][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3671(0.3838) Grad: 267437.0938  LR: 0.000050  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7745(0.7745) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 22 - avg_train_loss: 0.3838  avg_val_loss: 0.7741  time: 81s\n",
      "Epoch 22 - avg_train_Score: 0.3838 avgScore: 0.7741\n",
      "Epoch 22 - Save Best Score: 0.7741 Model\n",
      "Epoch 22 - Save Best Loss: 0.7741 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7899(0.7741) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/542] Elapsed 0m 1s (remain 10m 34s) Loss: 0.4204(0.4204) Grad: 320369.7188  LR: 0.000046  \n",
      "Epoch: [23][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.4583(0.3682) Grad: 343502.8750  LR: 0.000046  \n",
      "Epoch: [23][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3756(0.3682) Grad: 287741.8125  LR: 0.000046  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 25s) Loss: 0.7778(0.7778) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 23 - avg_train_loss: 0.3682  avg_val_loss: 0.7746  time: 81s\n",
      "Epoch 23 - avg_train_Score: 0.3682 avgScore: 0.7746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8017(0.7746) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/542] Elapsed 0m 1s (remain 10m 19s) Loss: 0.3479(0.3479) Grad: 259658.3438  LR: 0.000043  \n",
      "Epoch: [24][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3473(0.3577) Grad: 287719.2812  LR: 0.000043  \n",
      "Epoch: [24][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3594(0.3577) Grad: 382590.7500  LR: 0.000043  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7583(0.7583) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 24 - avg_train_loss: 0.3577  avg_val_loss: 0.7742  time: 82s\n",
      "Epoch 24 - avg_train_Score: 0.3577 avgScore: 0.7742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8045(0.7742) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.3360(0.3360) Grad: 289679.6562  LR: 0.000039  \n",
      "Epoch: [25][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3307(0.3461) Grad: 312676.0000  LR: 0.000039  \n",
      "Epoch: [25][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3481(0.3461) Grad: 321379.5625  LR: 0.000039  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 32s) Loss: 0.7719(0.7719) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 25 - avg_train_loss: 0.3461  avg_val_loss: 0.7710  time: 82s\n",
      "Epoch 25 - avg_train_Score: 0.3461 avgScore: 0.7710\n",
      "Epoch 25 - Save Best Score: 0.7710 Model\n",
      "Epoch 25 - Save Best Loss: 0.7710 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8030(0.7710) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/542] Elapsed 0m 1s (remain 10m 10s) Loss: 0.3807(0.3807) Grad: 318380.2188  LR: 0.000035  \n",
      "Epoch: [26][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3042(0.3308) Grad: 241036.7812  LR: 0.000035  \n",
      "Epoch: [26][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.4043(0.3310) Grad: 482103.3438  LR: 0.000035  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7872(0.7872) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 26 - avg_train_loss: 0.3310  avg_val_loss: 0.7690  time: 82s\n",
      "Epoch 26 - avg_train_Score: 0.3310 avgScore: 0.7690\n",
      "Epoch 26 - Save Best Score: 0.7690 Model\n",
      "Epoch 26 - Save Best Loss: 0.7690 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8049(0.7690) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/542] Elapsed 0m 1s (remain 10m 12s) Loss: 0.3383(0.3383) Grad: 390459.4062  LR: 0.000031  \n",
      "Epoch: [27][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3196(0.3212) Grad: 287906.2812  LR: 0.000031  \n",
      "Epoch: [27][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3387(0.3207) Grad: 313247.3438  LR: 0.000031  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 36s) Loss: 0.7868(0.7868) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 27 - avg_train_loss: 0.3207  avg_val_loss: 0.7671  time: 82s\n",
      "Epoch 27 - avg_train_Score: 0.3207 avgScore: 0.7671\n",
      "Epoch 27 - Save Best Score: 0.7671 Model\n",
      "Epoch 27 - Save Best Loss: 0.7671 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8144(0.7671) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.3297(0.3297) Grad: 242510.0156  LR: 0.000028  \n",
      "Epoch: [28][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2596(0.3087) Grad: 231243.0625  LR: 0.000028  \n",
      "Epoch: [28][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.3417(0.3080) Grad: 317738.5938  LR: 0.000028  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7801(0.7801) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 28 - avg_train_loss: 0.3080  avg_val_loss: 0.7639  time: 81s\n",
      "Epoch 28 - avg_train_Score: 0.3080 avgScore: 0.7639\n",
      "Epoch 28 - Save Best Score: 0.7639 Model\n",
      "Epoch 28 - Save Best Loss: 0.7639 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8068(0.7639) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/542] Elapsed 0m 1s (remain 10m 3s) Loss: 0.3169(0.3169) Grad: 263655.6875  LR: 0.000024  \n",
      "Epoch: [29][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3184(0.2966) Grad: 269310.7500  LR: 0.000024  \n",
      "Epoch: [29][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2690(0.2965) Grad: 217390.2031  LR: 0.000024  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 38s) Loss: 0.7731(0.7731) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 29 - avg_train_loss: 0.2965  avg_val_loss: 0.7640  time: 81s\n",
      "Epoch 29 - avg_train_Score: 0.2965 avgScore: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8117(0.7640) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/542] Elapsed 0m 1s (remain 10m 28s) Loss: 0.2755(0.2755) Grad: 252954.6094  LR: 0.000021  \n",
      "Epoch: [30][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2512(0.2881) Grad: 244990.1719  LR: 0.000021  \n",
      "Epoch: [30][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2465(0.2880) Grad: 270785.1250  LR: 0.000021  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7890(0.7890) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 30 - avg_train_loss: 0.2880  avg_val_loss: 0.7618  time: 81s\n",
      "Epoch 30 - avg_train_Score: 0.2880 avgScore: 0.7618\n",
      "Epoch 30 - Save Best Score: 0.7618 Model\n",
      "Epoch 30 - Save Best Loss: 0.7618 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8102(0.7618) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/542] Elapsed 0m 1s (remain 11m 9s) Loss: 0.2473(0.2473) Grad: 230280.3750  LR: 0.000018  \n",
      "Epoch: [31][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2580(0.2793) Grad: 257793.2344  LR: 0.000018  \n",
      "Epoch: [31][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2079(0.2790) Grad: 243596.7188  LR: 0.000018  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7842(0.7842) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 31 - avg_train_loss: 0.2790  avg_val_loss: 0.7612  time: 81s\n",
      "Epoch 31 - avg_train_Score: 0.2790 avgScore: 0.7612\n",
      "Epoch 31 - Save Best Score: 0.7612 Model\n",
      "Epoch 31 - Save Best Loss: 0.7612 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8074(0.7612) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/542] Elapsed 0m 1s (remain 10m 27s) Loss: 0.2690(0.2690) Grad: 266911.3125  LR: 0.000015  \n",
      "Epoch: [32][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.3062(0.2703) Grad: 290556.2812  LR: 0.000015  \n",
      "Epoch: [32][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2986(0.2702) Grad: 248349.3281  LR: 0.000015  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7659(0.7659) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 32 - avg_train_loss: 0.2702  avg_val_loss: 0.7620  time: 81s\n",
      "Epoch 32 - avg_train_Score: 0.2702 avgScore: 0.7620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7983(0.7620) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/542] Elapsed 0m 1s (remain 11m 39s) Loss: 0.2721(0.2721) Grad: 237242.2500  LR: 0.000012  \n",
      "Epoch: [33][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2448(0.2609) Grad: 249132.2656  LR: 0.000012  \n",
      "Epoch: [33][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2284(0.2609) Grad: 269153.4688  LR: 0.000012  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 24s) Loss: 0.7692(0.7692) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 33 - avg_train_loss: 0.2609  avg_val_loss: 0.7615  time: 82s\n",
      "Epoch 33 - avg_train_Score: 0.2609 avgScore: 0.7615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8039(0.7615) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/542] Elapsed 0m 1s (remain 10m 2s) Loss: 0.2179(0.2179) Grad: 255590.3281  LR: 0.000010  \n",
      "Epoch: [34][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2452(0.2549) Grad: 233952.4375  LR: 0.000010  \n",
      "Epoch: [34][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2993(0.2545) Grad: 236281.0312  LR: 0.000010  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 28s) Loss: 0.7644(0.7644) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 34 - avg_train_loss: 0.2545  avg_val_loss: 0.7601  time: 81s\n",
      "Epoch 34 - avg_train_Score: 0.2545 avgScore: 0.7601\n",
      "Epoch 34 - Save Best Score: 0.7601 Model\n",
      "Epoch 34 - Save Best Loss: 0.7601 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8043(0.7601) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/542] Elapsed 0m 1s (remain 10m 22s) Loss: 0.2591(0.2591) Grad: 245957.2812  LR: 0.000008  \n",
      "Epoch: [35][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2505(0.2504) Grad: 228086.0938  LR: 0.000008  \n",
      "Epoch: [35][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2396(0.2506) Grad: 247100.3750  LR: 0.000008  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 34s) Loss: 0.7646(0.7646) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 35 - avg_train_loss: 0.2506  avg_val_loss: 0.7595  time: 82s\n",
      "Epoch 35 - avg_train_Score: 0.2506 avgScore: 0.7595\n",
      "Epoch 35 - Save Best Score: 0.7595 Model\n",
      "Epoch 35 - Save Best Loss: 0.7595 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7923(0.7595) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/542] Elapsed 0m 1s (remain 10m 40s) Loss: 0.1999(0.1999) Grad: 220665.3125  LR: 0.000006  \n",
      "Epoch: [36][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2439(0.2463) Grad: 286041.5000  LR: 0.000006  \n",
      "Epoch: [36][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2078(0.2452) Grad: 216914.1250  LR: 0.000006  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 23s) Loss: 0.7671(0.7671) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 36 - avg_train_loss: 0.2452  avg_val_loss: 0.7593  time: 82s\n",
      "Epoch 36 - avg_train_Score: 0.2452 avgScore: 0.7593\n",
      "Epoch 36 - Save Best Score: 0.7593 Model\n",
      "Epoch 36 - Save Best Loss: 0.7593 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8031(0.7593) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.2413(0.2413) Grad: 262697.1875  LR: 0.000004  \n",
      "Epoch: [37][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2381(0.2411) Grad: 235928.7031  LR: 0.000004  \n",
      "Epoch: [37][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2441(0.2415) Grad: 257720.6094  LR: 0.000004  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 33s) Loss: 0.7665(0.7665) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 37 - avg_train_loss: 0.2415  avg_val_loss: 0.7590  time: 81s\n",
      "Epoch 37 - avg_train_Score: 0.2415 avgScore: 0.7590\n",
      "Epoch 37 - Save Best Score: 0.7590 Model\n",
      "Epoch 37 - Save Best Loss: 0.7590 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8003(0.7590) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/542] Elapsed 0m 1s (remain 11m 20s) Loss: 0.2509(0.2509) Grad: 237224.5000  LR: 0.000003  \n",
      "Epoch: [38][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2512(0.2367) Grad: 192826.5938  LR: 0.000003  \n",
      "Epoch: [38][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2127(0.2366) Grad: 265227.2188  LR: 0.000003  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 26s) Loss: 0.7650(0.7650) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 38 - avg_train_loss: 0.2366  avg_val_loss: 0.7585  time: 82s\n",
      "Epoch 38 - avg_train_Score: 0.2366 avgScore: 0.7585\n",
      "Epoch 38 - Save Best Score: 0.7585 Model\n",
      "Epoch 38 - Save Best Loss: 0.7585 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.8015(0.7585) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/542] Elapsed 0m 1s (remain 10m 10s) Loss: 0.2059(0.2059) Grad: 220018.0938  LR: 0.000002  \n",
      "Epoch: [39][500/542] Elapsed 1m 4s (remain 0m 5s) Loss: 0.2053(0.2347) Grad: 202288.1094  LR: 0.000002  \n",
      "Epoch: [39][541/542] Elapsed 1m 9s (remain 0m 0s) Loss: 0.2306(0.2341) Grad: 220191.5469  LR: 0.000002  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 30s) Loss: 0.7637(0.7637) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 39 - avg_train_loss: 0.2341  avg_val_loss: 0.7579  time: 82s\n",
      "Epoch 39 - avg_train_Score: 0.2341 avgScore: 0.7579\n",
      "Epoch 39 - Save Best Score: 0.7579 Model\n",
      "Epoch 39 - Save Best Loss: 0.7579 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7971(0.7579) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/2782480385.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=CFG.use_amp)\n",
      "/tmp/ipykernel_94354/2782480385.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(CFG.use_amp):\n",
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1087: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  _warn_get_lr_called_within_step(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/542] Elapsed 0m 1s (remain 10m 13s) Loss: 0.2510(0.2510) Grad: 287441.5000  LR: 0.000001  \n",
      "Epoch: [40][500/542] Elapsed 1m 3s (remain 0m 5s) Loss: 0.2095(0.2342) Grad: 277769.4375  LR: 0.000001  \n",
      "Epoch: [40][541/542] Elapsed 1m 8s (remain 0m 0s) Loss: 0.2513(0.2340) Grad: 255559.6406  LR: 0.000001  \n",
      "EVAL: [0/68] Elapsed 0m 1s (remain 1m 42s) Loss: 0.7660(0.7660) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/atmacup18/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Epoch 40 - avg_train_loss: 0.2340  avg_val_loss: 0.7579  time: 82s\n",
      "Epoch 40 - avg_train_Score: 0.2340 avgScore: 0.7579\n",
      "Epoch 40 - Save Best Score: 0.7579 Model\n",
      "Epoch 40 - Save Best Loss: 0.7579 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [67/68] Elapsed 0m 12s (remain 0m 0s) Loss: 0.7943(0.7579) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/1421667788.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load(\n",
      "========== fold: 4 result ==========\n",
      "score: 0.7579\n",
      "========== CV ==========\n",
      "score: 0.7408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_pred shape (8674, 18)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "            if CFG.objective_cv == 'binary':\n",
    "                outputs.append(torch.sigmoid(model(x)).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'multiclass':\n",
    "                outputs.append(torch.softmax(\n",
    "                    model(x), axis=1).to('cpu').numpy())\n",
    "            elif CFG.objective_cv == 'regression':\n",
    "                outputs.append(model(x).to('cpu').numpy())\n",
    "\n",
    "        avg_preds = np.mean(outputs, axis=0)\n",
    "        return avg_preds\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "\n",
    "def test_fn(valid_loader, model, device):\n",
    "    preds = []\n",
    "\n",
    "    for step, (images) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        preds.append(y_preds)\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def inference():\n",
    "    test = pd.read_csv(CFG.comp_dataset_path +\n",
    "                       'test_features.csv')\n",
    "\n",
    "    test['base_path'] = CFG.comp_dataset_path + 'images/' + test['ID'] + '/'\n",
    "\n",
    "    paths = []\n",
    "    for base_path in test['base_path'].values:\n",
    "        suffixs = ['image_t-1.0.png', 'image_t-0.5.png', 'image_t.png']\n",
    "        for suffix in suffixs:\n",
    "            path = base_path + suffix\n",
    "            paths.append(path)\n",
    "\n",
    "    print(paths[:5])\n",
    "\n",
    "    CFG.video_cache = make_video_cache(paths)\n",
    "\n",
    "    print(test.head(5))\n",
    "\n",
    "    valid_dataset = CustomDataset(\n",
    "        test, CFG, transform=get_transforms(data='valid', cfg=CFG))\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    model = EnsembleModel()\n",
    "    folds = [0] if CFG.use_holdout else list(range(CFG.n_fold))\n",
    "    for fold in folds:\n",
    "        _model = CustomModel(CFG, pretrained=False)\n",
    "        _model.to(device)\n",
    "\n",
    "        model_path = CFG.model_dir + \\\n",
    "            f'{CFG.model_name}_fold{fold}_{CFG.inf_weight}.pth'\n",
    "        print('load', model_path)\n",
    "        state = torch.load(model_path)['model']\n",
    "        _model.load_state_dict(state)\n",
    "        _model.eval()\n",
    "\n",
    "        # _model = tta.ClassificationTTAWrapper(\n",
    "        #     _model, tta.aliases.five_crop_transform(256, 256))\n",
    "\n",
    "        model.add_model(_model)\n",
    "\n",
    "    preds = test_fn(valid_loader, model, device)\n",
    "\n",
    "    test[CFG.target_col] = preds\n",
    "    test.to_csv(CFG.submission_dir +\n",
    "                'submission_oof.csv', index=False)\n",
    "    test[CFG.target_col].to_csv(\n",
    "        CFG.submission_dir + f'submission_{CFG.exp_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t-0.5.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_120/image_t.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-1.0.png', '../raw/atmacup_18_dataset/images/012baccc145d400c896cb82065a93d42_220/image_t-0.5.png']\n",
      "[255, 227, 199, 170, 142, 114, 85, 57, 29]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722eee4ee0944d7b9cbf7d6e6cb8baab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     ID      vEgo      aEgo  steeringAngleDeg  \\\n",
      "0  012baccc145d400c896cb82065a93d42_120  3.374273 -0.019360        -34.008415   \n",
      "1  012baccc145d400c896cb82065a93d42_220  2.441048 -0.022754        307.860077   \n",
      "2  012baccc145d400c896cb82065a93d42_320  3.604152 -0.286239         10.774388   \n",
      "3  012baccc145d400c896cb82065a93d42_420  2.048902 -0.537628         61.045235   \n",
      "4  01d738e799d260a10f6324f78023b38f_120  2.201528 -1.898600          5.740093   \n",
      "\n",
      "   steeringTorque  brake  brakePressed  gas  gasPressed gearShifter  \\\n",
      "0            17.0    0.0         False  0.0       False       drive   \n",
      "1           295.0    0.0          True  0.0       False       drive   \n",
      "2          -110.0    0.0          True  0.0       False       drive   \n",
      "3           189.0    0.0          True  0.0       False       drive   \n",
      "4           -41.0    0.0          True  0.0       False       drive   \n",
      "\n",
      "   leftBlinker  rightBlinker  \\\n",
      "0        False         False   \n",
      "1        False         False   \n",
      "2        False         False   \n",
      "3         True         False   \n",
      "4        False         False   \n",
      "\n",
      "                                           base_path  \n",
      "0  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "1  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "2  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "3  ../raw/atmacup_18_dataset/images/012baccc145d4...  \n",
      "4  ../raw/atmacup_18_dataset/images/01d738e799d26...  \n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_tiny/atmacup_18-models/swin_tiny_patch4_window7_224_fold0_last.pth\n",
      "pretrained: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94354/610043316.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_path)['model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_tiny/atmacup_18-models/swin_tiny_patch4_window7_224_fold1_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_tiny/atmacup_18-models/swin_tiny_patch4_window7_224_fold2_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_tiny/atmacup_18-models/swin_tiny_patch4_window7_224_fold3_last.pth\n",
      "pretrained: False\n",
      "load ../proc/baseline/outputs/atmacup_18_cnn_swin_tiny/atmacup_18-models/swin_tiny_patch4_window7_224_fold4_last.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34affa1d8a54d92b1a24a546ef9b4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
